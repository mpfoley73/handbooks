# Time Series

```{r include=FALSE}
library(tidyverse)
library(xts)
library(fpp3)
library(tsibble)
library(fable)
library(patchwork) # for arranging plots
library(glue) # for informative labels
```

These notes are based on the [Time Series with R](https://www.datacamp.com/tracks/time-series-with-r) skill track at DataCamp and **Forecasting: Principles and Practice** [@Hyndman2021].

Not all forecasting is quantitative - if you have no data you use qualitative judgment procedures. But quantitative forecasting isn't necessarily based on time series models - your model may be a cross-sectional analysis of relevant factors, perhaps even including time-related factors such as month of year or day of week. Time series forecasting is a specific type of forecasting: it is a quantitative forecast of future outcomes based, at least in part, on the assumption that future outcomes are functionally dependent upon prior outcomes. 

In most cases your forecasting objective is to project a time series. In these cases, the time series forecast is either an exercise of decomposing a time series into trend and seasonality components (exponential smoothing models) or describing the autocorrelation within the data (ARIMA models). There may also be cases where you include other predictor variables (dynamic models).

These notes are structured as four sections. The toolbox section is about using R to explore and wrangle time series data. The next three sections describe the three main modeling techniques: exponential smoothing, ARIMA, and dynamic models.

## Toolbox

This section deals with foundational concepts in time series analysis: benchmarking methods, prediction intervals, evaluations accuracy.

### R Structures

Use a `tsibble` object to work with time series data. `tsibble`, from the package of the same name, is a time-series `tibble`. Unlike `ts`, `zoo`, and `xts`, `tsibble` preserves the time index, making heterogeneous data structures possible. But `tsibble` is new, so you will encounter the other other frameworks and should at least be familiar with them.

I'll work with the `prison_population.csv` file accompanying Hyndman's text to get familiar with these packages. It contains quarterly prison population statistics grouped by several attributes. In essence, several time series within one file.

```{r message=FALSE}
prison <- readr::read_csv("https://OTexts.com/fpp3/extrafiles/prison_population.csv")
head(prison)
```

#### ts, xts, and zoo {-}

`ts` is the base R time series package. The `ts` object is essentially a matrix of observations indexed by a chronological identifier. Because it is a matrix, any descriptive attributes need to enter as numeric, perhaps by one-hot encoding, or pivoting the data (yuck). `ts` will not recognize irregularly spaced time series.

Define a `ts` object with `ts(x, start, frequency)` where `frequency` is the number of observations in the seasonal pattern: 7 for days in a week cycle; 5 for weekdays in a week cycle; 24 for hours in a day cycle, 24\*7 for hours in a week cycle, etc. In this case `prison` is quarterly starting with 2005 Q1. Had the series started at 2005 Q2, you'd specify `start = c(2005, 2)`. I'll pull out a single time series from the file, defined by the key `State`, `Gender`, `Legal` and `Indigenous`.

```{r}
prison_ts <- prison %>% 
  filter(State == "ACT" & Gender == "Male" & Legal == "Remanded" & Indigenous == "ATSI") %>%
  arrange(Date) %>%
  select(Count) %>%
  ts(start = 2005, frequency = 4)

str(prison_ts)
```

`zoo` (Zeileis's ordered observations) better supports ordered observations and provides methods similar to those for `ts`. `zoo` objects contain an array of data values and an index attribute to provide information about the data ordering. `zoo` was introduced in 2014.

`xts` (extensible time series) extends `zoo`. `xts` is more flexible than `ts` while imposing reasonable constraints to make it a true time-based class. `xts` objects are essentially a matrix of observations indexed by a time *object*. Create an `xts` object with `xts(x, order.by)` where `order.by` is a vector of dates/times to index the data. You can also add metadata to the `xts` object by declaring name-value pairs such as `born = as.POSIXct("1899-05-08").`

```{r message=FALSE}
x <- prison %>%
  filter(State == "ACT" & Gender == "Male" & Legal == "Remanded" & Indigenous == "ATSI") %>%
  arrange(Date)

prison_xts <- xts(x$Count, order.by = x$Date, 
                  State = "ACT", Gender = "Male", Legal = "Remanded", Indigenous = "ATSI")
str(prison_xts)
```

Create subsets of a time series (to create training and test data sets) with base R function `window(x, start, end)`.

```{r collapse=TRUE}
window(prison_ts, start = c(2005, 4), end = c(2007, 3))

window(prison_xts, start = as.Date("2005-12-01"), end = as.Date("2007-09-01"))
```

#### tsibble {-}

A `tsibble` object is a tibble uniquely defined by `key` columns plus a date `index` column. This structure accommodates multiple series, and descriptive attribute columns.

```{r}
prison_tsibble <- prison %>% 
  mutate(Date = yearquarter(Date)) %>%
  rename(Qtr = Date) %>%
  tsibble(key = c(State, Gender, Legal, Indigenous), index = Qtr)

head(prison_tsibble)
```

`tsibble`s behave just like `tibble`s, so the framework will feel natural - all the tidyverse verbs will work. The only thing that will trip you up is that `tsibble` objects are pre-grouped by the index, so if you used the `summarize()` verb on it, you'll get one row per index value.^[No, you cannot `ungroup()` first - the grouping is baked in somehow. If you want to ignore the index, cast to `tibble` first.]

```{r}
prison_tsibble %>% 
  group_by(State) %>% 
  summarize(sum_Count = sum(Count))
```

### Fitting Models

Consider whether you are fitting a model for explanatory purposes (which factors are important and what is there affect on the outcome) or for predictive purposes (expected future outcomes). If explanation is your goal, you will fit a model, check that assumptions related to inference are met, then summarize the model parameters. If prediction is your goal, you might compare multiple models, cross-validate the results against a hold-out data set, then make predictions.

Fit a model using `fabletools::model()`. You can even fit multiple models. Below, I fit a simple naive model (projection of last value) to the Google closing stock prices from 2015, then use it to predict values from Jan 2016.

Test data sets should be about 20% of the original dataset, or at least as long as the anticipated forecast length. One way to create a test data set is with filter. Another is `group_by()` + `slice()`.

```{r}
google_dat <- tsibbledata::gafa_stock %>%
  filter(Symbol == "GOOG", year(Date) >= 2015) %>%
  # re-index on trading day
  arrange(Date) %>%
  mutate(trading_day = row_number()) %>%
  update_tsibble(index = trading_day, regular = TRUE)

# Break into training and test
google_train <- google_dat %>% filter(year(Date) == 2015)
google_test <- google_dat %>% filter(yearmonth(Date) == yearmonth("2016 Jan"))

# Train model
google_mdl <- google_train %>%
  model(mdl_naive = NAIVE(Close))

# Generate predictions (forecast)
google_fc <- google_mdl %>%
  forecast(new_data = google_test)
google_fc %>%
  autoplot(color = "goldenrod") +
  autolayer(google_train, Close, color = "goldenrod") +
  theme_light() +
  theme(legend.position = "none") +
  labs(title = "20d naive forecast from model fit to CY-2015",
       caption = "Shaded area is 80%- and 90% confidence interval.",
       x = "Trading Day", y = "Closing Price")
```

### Evaluating Models

Evaluate the model fit with residuals diagnostics.^[Residuals and errors are *not* the same thing. The *residual* is the difference between the observed and fitted value in the *training* data set. The *error* is the difference between the observed and fitted value in the *test* data set.] `broom::augment()` adds three columns to the relevant model cols: `.fitted`, `.resid`, and `.innov`. `.innov` is the residual from the transformed data - it equals `.resid` if no transformation.

```{r}
google_mdl_aug <- google_mdl %>% broom::augment()
```

Innovation residuals should be independent random variables normally distributed with mean zero and constant variance. Happily, `feasts` has just what you need.

```{r warning=FALSE}
google_mdl %>% gg_tsresiduals() +
  labs(title = "Residuals Analysis")
```

The autocorrelation plot supports the independence assumption. The histogram is pretty normal, but the right tail is awfully long. The residuals plot has mean zero and constant variance. You can carry out a  *portmanteau* test test on the autocorrelation assumption. Two common tests are the Box-Pierce and the Ljung-Box. These tests check the likelihood of a combination of autocorrelations at once, without testing any one correlation - kind of like an ANOVA test. The Ljung-Box test statistic is a sum of squared $k$-lagged autocorrelations, $r_k^2$,

$$Q^* = T(T+2) \sum_{k=1}^l(T-k)^{-1}r_k^2.$$

The test statistic has a $\chi^2$ distribution with $l - K$ degrees of freedom (where $K$ is the number of parameters in the model). Use $l = 10$ for non-seasonal data and $l = 2m$ for seasonal data. If your model has no explanatory variables, $K = 0.$

```{r}
google_mdl_aug %>% features(.var = .innov, features = ljung_box, lag = 10, dof = 0)
```

The *p*-value is not under .05, so do *not* reject the assumption of autocorrelation - i.e., the assumption of white noise.

Use the `checkresiduals()` function from the `forecast` package to check these assumptions.  The Ljung-Box test should have a p-value >.05 and the histogram should be normal.  The code chunk below tests the naive model on the `goog200` dataset of daily Google stock prices.  The ACF of the residuals show no residuals outside the 95% acceptance region of no-autocorrelation.  The Ljung-Box fails to reject the null hypothesis of no-autocorrelation (*p* = 0.3551).  The histogram is centered on zero and normally distributed (maybe slightly skewed right).  The residuals plot indicates constant variance.

### Evaluating Accuracy

#### Benchmarks

Some forecasting methods are extremely simple and surprisingly effective. The **mean** method projects the historical average, $\hat{y}_{T+h|T} = \bar{y}.$ The **naive** method projects the last observation, $\hat{y}_{T+h|T} = y_T.$ The **seasonal naive** method projects the last seasonal observation, $\hat{y}_{T+h|T} = y_{T+h-m(k+1)}.$ The **drift** method projects the straight line from the first and last observation, $\hat{y}_{T+h|T} = y_T + h\left(\frac{y_T - y_1}{T-1}\right).$

```{r}
aus_production %>%
  # same thing as
  # filter(Quarter >= yearquarter("1970 Q1") & Quarter <= yearquarter("2004 Q4")) %>%
  filter_index("1995 Q1" ~ "2007 Q4") %>%
  model(Mean = MEAN(Beer),
        Naive = NAIVE(Beer),
        SNaive = SNAIVE(Beer),
        Drift = RW(Beer ~ drift())) %>%
  forecast(h = 8) %>%
  ggplot(aes(x = Quarter)) +
  geom_line(aes(y = .mean, color = .model), size = 1) +
  geom_line(data = aus_production %>% filter_index("1995 Q1" ~ "2009 Q4"),
         aes(y = Beer),
         color = "darkgrey", size = 1) +
  theme_light() +
  guides(color = guide_legend(title = "Forecast")) +
  labs(title = "Simple forecast methods are useful benchmarks.",
       x = NULL, y = NULL,
       caption = "Source: Quarterly beer production (ML) from tsibbledata::aus_production.")
```

Evaluate the forecast accuracy with the test data (aka, "hold-out set", and "out-of-sample data"). Forecast accuracy can only be determined by how well the model performs on new data. The **forecast error** is the difference between the observed and forecast value, $e_{T+h} = y_{T+h} - \hat{y}_{t+h|T}.$ Forecast errors differ from model residuals in the data (train vs test) and because forecast values are (usually) multi-step forecasts which include prior forecast values as inputs.

There are a few benchmark metrics to evaluate a fit based on the errors. 

* **MAE**.  Mean absolute error, $mean(|e_t|)$
* **RMSE**.  Root mean squared error, $\sqrt{mean(e_t^2)}$
* **MAPE**.  Mean absolute percentage error, $mean(|e_t / y_t|) \times 100$
* **MASE**.  Mean absolute scaled error, $MAE/Q$ where $Q$ is a scaling constant calculated as the average one-period change in the outcome variable (error from a one-step naive forecast).

The MAE and RMSE are on the same scale as the data, so they are only useful for comparing models fitted to the same series.  MAPE is unitless, but does not work for $y_t = 0$, and it assumes a meaningful zero (ratio data). The MASE is most useful for comparing datasets of different units. 

Use `accuracy()` to evaluate a model. Comparing the naive, drift, and mean methods for forecasting the Google stock price, the naive model wins on all measures.

```{r}
google_mdl <- google_train %>%
  model(Naive = NAIVE(Close),
        Drift = RW(Close ~ drift()),
        Mean = MEAN(Close))

google_fc <- google_mdl %>%
  forecast(new_data = google_test)

ggplot() +
  geom_line(data = google_fc, aes(x = trading_day, y = .mean, color = .model)) +
  geom_line(data = bind_rows(google_train, google_test), aes(x = trading_day, y = Close)) +
  theme_light() +
  scale_y_continuous(labels = scales::dollar_format()) +
  labs(title = "Comparison of three models.",
       subtitle = "Google stock price predicted over 19 days.",
       x = "Trading Day", y = "Closing Price", color = "Forecast")
  
autoplot(google_fc, bind_rows(google_train, google_test), level = NULL)

accuracy(google_fc, google_dat) %>%
  select(.model, RMSE, MAE, MAPE, MASE)
```

There is a better way of evaluating a model than with a single test set.  Time series *cross-validation* break the dataset into multiple training sets by setting the cutoff at varying points and then setting the test set to be a fixed number of steps ahead of the horizon.  Function `tsCV()` returns the errors using time series cross-validation.  Calculate the RMSE by taking the square root of the mean of the squared errors.

```{r}
# e <- tsCV(oil, forecastfunction = naive, h = 1)
# sqrt(mean(e^2, na.rm = TRUE))
```




## Exploratory Analysis

Start an analysis by viewing the data values and structure. Then take some summary statistics. `fabletools::features()` is great for this.

```{r}
prison_tsibble %>%
  features(Count, list(mean = mean, quantile = quantile))
```

There are many autocorrelation features you might want to review. I don't understand why you'd want to know all of these, but `feat_acf` has them.

```{r}
prison_tsibble %>%
  features(Count, feat_acf)
```


### Graphical Analysis

The first task in any data analysis is to plot the data to identify patterns, unusual observations, changes over time, and relationships between variables. Look for *trend*, *cycles*, and *seasonality* in your exploratory plots. These features inform the subsequent forecasting process. `ggplot2::autoplot()` does a great job picking out the right plot, but I feel more comfortable staying old-school for now.

```{r}
data("ansett", package = "tsibbledata")

ansett %>% 
  filter(Airports == "MEL-SYD", Class == "Economy") %>% 
  ggplot(aes(x = Week, y = Passengers)) +
  geom_line(color = "goldenrod") +
  theme_light() +
  labs(title = "Start with a simple time series plot.",
       subtitle = "Weekly passenger volume.", x = NULL, y = NULL)
```

What does this one reveal? 

* There was a period in 1989 of zero passengers (strike). 
* There was a period in 1992 where passenger load dropped (planes temporarily reconfigured). 
* Volume increased during the second half of 1992. 
* Several large post-holiday dips in volume. 
* Some larger trends of increasing and decreasing volume (the data is *cyclic*). 
* Also appears to be some missing observations. (*common practice with missing observations is to impute values with the time series mean.*)

If a data series has trend and seasonality, highlight it with `feasts::gg_season()`. 

```{r message=FALSE, warning=FALSE}
data("PBS", package = "tsibbledata")

a10 <- PBS %>%
  filter(ATC2 == "A10") %>%
  select(Month, Cost) %>%
  summarize(Cost = sum(Cost))

p1 <- a10 %>%
  filter(year(Month) %in% c(1991, 1995, 2000, 2005)) %>%
  ggplot(aes(x = month(Month, label = TRUE, abbr = TRUE), y = Cost, 
             group = factor(year(Month)), 
             color = factor(year(Month)),
             label = if_else(month(Month) %in% c(1, 12), year(Month), NA_real_))) +
  geom_line(show.legend = FALSE) +
  geom_text(show.legend = FALSE) +
  theme_light() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.25)) +
  labs(subtitle = "using gglot", x = NULL, y = NULL)

p2 <- a10 %>%
  filter(year(Month) %in% c(1991, 1995, 2000, 2005)) %>%
  fill_gaps() %>%  # otherwise, gg_season() barks.
  gg_season(Cost, labels = "both") +
  theme_light() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.25)) +
  labs(subtitle = "using feasts", x = NULL, y = NULL)

p1 + p2 +
  plot_annotation(title = "Medicare script costs increase from Feb - following Jan.",
                  subtitle = "12-month seasonality plot, selected years.")
```

Emphasize the seasonality further by faceting on the sub-series with `feasts::gg_subseries()`. 

```{r message=FALSE, warning=FALSE}
yint <- a10 %>% 
  as.tibble() %>% 
  group_by(month(Month, label = TRUE, abbr = TRUE)) %>% 
  mutate(mean_cost = mean(Cost) / 1e6)
p1 <- a10 %>%
  ggplot(aes(x = year(Month), y = Cost / 1e6)) + 
  geom_line(show.legend = FALSE, color = "goldenrod") +
  geom_hline(data = yint, aes(yintercept = mean_cost), linetype = 2) +
  theme_light() +
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        panel.grid = element_blank()) +
  scale_y_continuous(labels = scales::dollar) +
  facet_wrap(vars(month(Month, label = TRUE, abbr = TRUE)), nrow = 1) +
  labs(subtitle = "using gglot", x = NULL, y = NULL)

p2 <- a10 %>%
  mutate(Cost = Cost / 1e6) %>%
  fill_gaps() %>%  # otherwise, gg_subseries() barks.
  gg_subseries(Cost) +
  theme_light() +
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        panel.grid = element_blank()) +
  scale_y_continuous(labels = scales::dollar) +
  labs(subtitle = "using feasts", x = NULL, y = NULL)

p1 / p2 +
  plot_annotation(title = "Medicare script costs follow monthly seasonality, inreasing annually.",
                  subtitle = "Monthly subseries plot of cost $MM, 1991 - 2009.")
```

Explore the correlation between two quantitative variables with the Pearson correlation coefficient. Recall that the covariance between series $X$ and $Y$ is defined $Cov(X, Y) = E[(X - \mu_X) (Y - \mu_Y)]$ which simplifies to $Cov(X, Y) = E[XY] - \mu_X \mu_Y$. The covariance of $X$ and $Y$ is positive if $X$ and $Y$ increase together, negative if they move in opposite directions, and if $X$ and $Y$ are independent, $E[XY] = E[X]E[Y] = \mu_X \mu_Y$, so $Cov(X, Y) = 0$. Covariance is usually inconvenient because its values depend on the units of the series. Dividing $Cov(X, Y)$ by the standard deviations $\sigma_X \sigma_Y$ creates a unit-less variable with range [-1, 1], also known as the Pearson correlation.^[Incidentally, $\rho$ is related to the slope of the linear regression line: $\beta_1 = \frac{\sigma_{XY}}{\sigma_X^2} = \rho \frac{\sigma_Y}{\sigma_X}$.]

$$\rho = \frac{\sigma_{XY}} {\sigma_X \sigma_Y}.$$  

```{r}
PBS %>% 
  group_by(ATC1) %>%
  summarize(.groups = "drop", Cost = sum(Cost)) %>%
  pivot_wider(names_from = ATC1, values_from = Cost) %>%
  as_tibble() %>%
  select(-Month) %>%
  cor() %>%
  ggcorrplot::ggcorrplot() +
  theme_light() +
  labs(title = "Z and P indexes negatively correlated with others.",
       subtitle = "Correlation plot of Medicare ATC1 indexes.", 
       x = NULL, y = NULL)
```

Autocorrelation is correlation with lagging observations. Lag plots of current period vs lags are a particular kind of correlation scatterplot useful for identifying seasonality. `feasts::ACF()` extends the base R `acf()` function to tsibbles. `aus_production` contains quarterly production levels. The 4-period lag is a year-over-year correlation and is strong because of seasonality in production. The 8-period lag is less strong. The 1-, 2-, and 3-period lags are not positively correlated. In fact, the lag-2 is negatively correlated.

```{r}
data("aus_production", package = "tsibbledata")

prod2k <- aus_production %>%
  filter(year(Quarter) >= 2000)

prod2k %>%
  gg_lag(Beer, geom = "point") +
  theme_light() +
  labs(title = "Production is seasonal with Q4 peak, Q1 second.",
       subtitle = "Quarterly lag plot of beer production",
       x = NULL, y = NULL) 

p1 <- prod2k %>% 
  ACF(Beer, lag_max = 16) %>% 
  ggplot(aes(x = lag)) + 
  geom_linerange(aes(ymin = 0, ymax = acf), color = "goldenrod") +
  geom_point(aes(y = acf), color = "goldenrod") +
  geom_hline(yintercept = 0) +
  geom_hline(yintercept = -2 / sqrt(nrow(prod2k)), linetype = 2) +
  geom_hline(yintercept = +2 / sqrt(nrow(prod2k)), linetype = 2) +
  theme_light() +
  theme(panel.grid = element_blank()) +
  labs(subtitle = "using ggplot", y = NULL)

p2 <- prod2k %>% 
  ACF(Beer, lag_max = 16) %>%
  autoplot() +
  theme_light() +
  labs(subtitle = "using autoplot", y = NULL)

p1 + p2 +
  plot_annotation(title = "Strongest correlation is 4-period lag, negative 2-period lag.",
                  subtitle = "Quarterly autocorrelation of beer production.",
                  caption = "Dashed lines are +/- square root of series length, the white noise bound.")
```

* Autocorrelation for *trending* data tends to be large and positive because observations nearby in time are also nearby in size. The ACF tends to have positive values that slowly decrease as the lags increase.

* Autocorrelation for *seasonal* data tends to be larger for the seasonal lags (at multiples of the seasonal frequency).  The Quarterly Australian Beer Production ACF above shows seasonality.

* Autocorrelation for *both trended and seasonal* data has a combination of these effects. 

* Time series that show no autocorrelation are called **white noise**. White noise series have near-zero autocorrelation. Stock price changes often exhibit white noise. Almost all lags have autocorrelation insignificantly different from zero. Expect 95% of spikes to lie withing $\pm 2 / \sqrt{T}$ where $T$ is the length of the data series.

```{r warning=FALSE}
data("gafa_stock", package = "tsibbledata")

stock <- gafa_stock %>% 
  filter(Symbol == "FB") %>%
  mutate(l1 = Close - lag(Close))

stock %>%
  ACF(l1, lag_max = 16) %>%
  ggplot(aes(x = lag)) + 
  geom_linerange(aes(ymin = 0, ymax = acf), color = "goldenrod") +
  geom_point(aes(y = acf), color = "goldenrod") +
  geom_hline(yintercept = 0) +
  geom_hline(yintercept = -2 / sqrt(nrow(stock)), linetype = 2) +
  geom_hline(yintercept = +2 / sqrt(nrow(stock)), linetype = 2) +
  theme_light() +
  theme(panel.grid = element_blank()) +
  labs(title = "Stock price changes tend to exhibit white noise.",
       subtitle = "Daily autocorrelation of stock price changes.",
       caption = "Dashed lines are +/- square root of series length, the white noise bound.")
```

The Ljung-Box test tests the randomness of a series; a *p*-value under 0.05 rejects the null hypothesis of white noise. The test reject white noise for the beer production, but not for stock price changes.

```{r collapse=TRUE}
Box.test(prod2k$Beer, lag = 16, fitdf = 0, type = "Ljung")
Box.test(stock$l1, lag = 16, fitdf = 0, type = "Ljung")
```

### Transformations

Remove known sources of variation (e.g., days per month, population growth, inflation). E.g., monthly totals may vary due to differing month lengths.

```{r warning=FALSE, message=FALSE}
milk <- fma::milk %>% 
  as_tsibble() %>% # milk is a `ts` object
  mutate(daily_avg = value / lubridate::days_in_month(index))

p1 <- milk %>%
  ggplot(aes(x = index, y = value)) +
  geom_line(color = "goldenrod") +
  theme_light() +
  theme(axis.text.x = element_blank()) +
  labs(subtitle = "Original", x = NULL, y = NULL)

p2 <- milk %>%
  ggplot(aes(x = index, y = daily_avg)) +
  geom_line(color = "goldenrod") +
  theme_light() +
  labs(subtitle = "Daily Average", x = NULL, y = NULL)

p1 / p2 +
  plot_annotation(title = "Convert monthly sums into daily averages per month.",
                  subtitle = "Average daily milk production by month.")
```

Make patterns more consistent across the data set. Simpler patterns usually lead to more accurate forecasts. A Box-Cox transformation can equalize seasonal variation.

$$w_t = \begin{cases} \mathrm{log}(y_t), & \mbox{if } \lambda\mbox{ = 0} \\ 
\left(\mathrm{sign}(y_t) |y_t|^\lambda - 1 \right) / \lambda, & \mbox{otherwise} \end{cases}$$

$\lambda$ can take any value, but values near the following yield familiar transformations.

* $\lambda = 1$: no substantive transformation.  
* $\lambda = 0.5$:  square root plus linear transformation.
* $\lambda = 0.333$: cube root plus linear transformation.
* $\lambda = 0$: natural log.
* $\lambda = -1$: inverse.

A good value of $\lambda$ is one which makes the size of the seasonal variation about the same across the whole series.  `fabletools::features()` and `BoxCox.lambda()` optimize $\lambda$, but try to choose a simple value to make interpretation clearer. Note that while forecasts are not sensitive to $\lambda$, prediction intervals *are*.

```{r}
data("aus_production", package = "tsibbledata")

lambda <- aus_production %>%
  # guerrero() applies Guerrero's method to select lambda that minimizes the
  # coefficient of variation: .12.
  features(Gas, features = guerrero) %>%
  pull(lambda_guerrero)

# supposedly the same method, but returns .10 instead.
lambda_v2 <- forecast::BoxCox.lambda(aus_production$Gas, method = "guerrero")

p1 <- aus_production %>%
  mutate(gas_xform = box_cox(Gas, lambda)) %>%
  ggplot(aes(x = Quarter)) +
  geom_line(aes(y = Gas), color = "goldenrod") +
  theme_light() +
  theme(axis.text.x = element_blank()) +
  labs(subtitle = "Original", x = NULL)

p2 <- aus_production %>%
  mutate(gas_xform = box_cox(Gas, lambda)) %>%
  ggplot(aes(x = Quarter)) +
  geom_line(aes(y = gas_xform), color = "goldenrod") +
  theme_light() +
  labs(subtitle = glue("Box-Cox, lambda = {scales::comma(lambda, accuracy = 0.01)}"),
       y = glue("Gas^{scales::comma(lambda, accuracy = 0.01)}"))

p1 / p2 + 
  plot_annotation(title = "Box-Cox transformation equalizes the seasonal component.",
                  subtitle = "Box-Cox transformation (Guerrero's method) of quarterly gas production.")
```

### Decomposition

Time series data often has trending, seasonality, and cycles. It's usually useful to lump trending and cycles into a trend-cycle components, or simply "trend", and treat time series data as consisting of seasonality $S_t$, trend $T_t$, and a remainder $R_t$. If the magnitude of the seasonal fluctuations and the variation in the trend cycle are constant, then these components are additive, $y_t = S_t + T_t + R_t$; if they are proportional to the level, then these components are multiplicative, $y_t = S_t \times T_t \times R_t$.

#### Classical Decomposition

Classical decomposition was commonly used until the 1950s. It is still the basis of other methods, so it is good to understand. Classical decomposition is based on moving averages. An *m*-MA moving average of order $m = 2k + 1$ averages the $k$ observations before $t$ through the $k$ observations after $t$. The `slider` package is great for this. $m$ is usually an odd number so that the number of periods before and after are equal.

$$\hat{T}_t = \frac{1}{m}\sum_{j = -k}^k y_{t+j}$$

```{r}
library(slider)

data("global_economy", package = "tsibbledata")

global_economy %>%
  filter(Country == "Australia") %>%
  mutate(
    `3-MA` = slide_dbl(Exports, mean, .before = 1, .after = 1, .complete = TRUE),
    `5-MA` = slide_dbl(Exports, mean, .before = 2, .after = 2, .complete = TRUE),
    `7-MA` = slide_dbl(Exports, mean, .before = 3, .after = 3, .complete = TRUE),
    `9-MA` = slide_dbl(Exports, mean, .before = 4, .after = 4, .complete = TRUE)
  ) %>%
  pivot_longer(cols = ends_with("-MA"), names_to = "MA_name", values_to = "MA") %>%
  ggplot(aes(x = Year)) +
  geom_line(aes(y = MA, color = MA_name), na.rm = TRUE) +
  geom_line(aes(y = Exports), size = 1.25, color = "#000000", alpha = 0.4) +
  theme_light() +
#  guides(color = guide_legend(title = "series")) +
  labs(title = "m-MA simple moving averages smooth a time series.",
       subtitle = "m-MA of annual export proportion of GDP for m = 3, 5, 7, 9.", 
       x = NULL, y = "Pct of GDP", color = NULL)
```

But what if you have known seasonal periods in the data? For example, with quarterly seasonality it makes sense to take a moving average of the 4 periods at once. Do this with a moving average of a four-period moving average.

```{r}
aus_production %>%
  filter(year(Quarter) >= 1992) %>%
  mutate(
    `4-MA` = slide_dbl(Beer, mean, .before = 1, .after = 2, .complete = TRUE),
    `2x4-MA` = slide_dbl(`4-MA`, mean, .before = 1, .after = 0, .complete = TRUE)
  ) %>%
  pivot_longer(cols = ends_with("-MA"), names_to = "MA_type", values_to = "MA") %>%
  ggplot(aes(x = Quarter)) +
  geom_line(aes(y = Beer), color = "gray", size = 1) +
  geom_line(aes(y = MA, color = MA_type), na.rm = TRUE) +
  theme_light() +
  ggthemes::scale_color_few() +
  guides(color = guide_legend(title = "series")) +
  labs(title = "MAs of MAs smooth seasonal periods.",
       subtitle = "2-MA of a 4-MA for quarterly data.",
       y = NULL)
```

Classical decomposition calculates a trend-cycle component $\hat{T}_t$ with an *m*-MA (odd order) or $2 \times m$-MA. The de-trended series is the difference, $y_t - \hat{T}_t$. The seasonal component $\hat{S}_t$ is the seasonal average (e.g., for monthly seasons, then $\hat{S}_1$ would be the January average). The remainder is the residual, $\hat{R}_t = y_t - \hat{T}_t - \hat{S}_t.$ If the cycle variation and seasonal magnitude increases with the observation level, then the same principles apply except the subtrations are replaced with divisions, $y_t / \hat{T}_t$ and remainder $\hat{R}_t = y_t / (\hat{T}_t \hat{S}_t)$.

```{r fig.height=5}
data("us_employment", package = "fpp3")

us_employment %>%
  filter(year(Month) >= 1990 & Title == "Retail Trade") %>%
  model(classical = classical_decomposition(Employed, type = "additive")) %>%
  components() %>%
  pivot_longer(cols = Employed:random, 
               names_to = "component", values_to = "employed") %>%
  mutate(component = factor(component, 
                            levels = c("Employed", "trend", "seasonal", "random"))) %>%
  ggplot(aes(x = Month, y = employed)) +
  geom_line(na.rm = TRUE, color = "goldenrod") +
  theme_light() +
  facet_grid(vars(component), scales = "free_y") +
  labs(title = "Classical additive decomposition of US retail employment",
       subtitle = "Employed = trend + seasonal + random",
       y = NULL)
```

Classical decomposition has weaknesses: the trend-cycle is unavailable for the first few and and last few periods; it assumes the seasonal component is stable over time; and it also tends to over-smooth the data.

#### X-11 and SEATS

X-11 and Seasonal Extraction in ARIMA Time Series (SEATS) are commonly used by governmental agencies. X-11 overcomes some of classical decomposition's drawbacks by adding extra steps. It creates trend-cycle estimates for all periods, and accommodates a slowly varying seasonal component.

```{r fig.height=5}
us_employment %>%
  filter(year(Month) >= 1990 & Title == "Retail Trade") %>%
  model(x11 = X_13ARIMA_SEATS(Employed ~ x11())) %>%
  components() %>%
  pivot_longer(cols = Employed:irregular, 
               names_to = "component", values_to = "employed") %>%
  mutate(component = factor(component, 
                            levels = c("Employed", "trend", "seasonal", "irregular"))) %>%
  ggplot(aes(x = Month, y = employed)) +
  geom_line(na.rm = TRUE, color = "goldenrod") +
  theme_light() +
  facet_grid(vars(component), scales = "free_y") +
  labs(title = "X11 decomposition of US retail employment",
       subtitle = "Employed = trend + seasonal + irregular",
       y = NULL)
```

SEATS is another one (too complicated to discuss evidently!). 

```{r fig.height=5}
us_employment %>%
  filter(year(Month) >= 1990 & Title == "Retail Trade") %>%
  model(x11 = X_13ARIMA_SEATS(Employed ~ seats())) %>%
  components() %>%
  pivot_longer(cols = Employed:irregular, 
               names_to = "component", values_to = "employed") %>%
  mutate(component = factor(component, 
                            levels = c("Employed", "trend", "seasonal", "irregular"))) %>%
  ggplot(aes(x = Month, y = employed)) +
  geom_line(na.rm = TRUE, color = "goldenrod") +
  theme_light() +
  facet_grid(vars(component), scales = "free_y") +
  labs(title = "SEATS decomposition of US retail employment",
       subtitle = "Employed = f(trend, seasonal, irregular)",
       y = NULL)
```

#### STL

Seasonal and Trend decomposition using Loess (STL) has several advantages over classical decomposition, and the SEATS and X-11 methods. It handles any type of seasonality (not just monthly and quarterly); the seasonality component can change; the smoothness of the trend-cycle can be changed by the modeler; and it is robust to outliers.

The `widow` settings inside `model()` control how rapidly the components change.

```{r fig.height=5}
us_employment %>%
  filter(year(Month) >= 1990 & Title == "Retail Trade") %>%
  model(STL = STL(Employed ~ trend(window = 7) + season(window = "periodic"),
                  robust = TRUE)) %>%
  components() %>%
  pivot_longer(cols = Employed:remainder, 
               names_to = "component", values_to = "employed") %>%
  mutate(component = factor(component, 
                            levels = c("Employed", "trend", "season_year", "remainder"))) %>%
  ggplot(aes(x = Month, y = employed)) +
  geom_line(na.rm = TRUE, color = "goldenrod") +
  theme_light() +
  facet_grid(vars(component), scales = "free_y") +
  labs(title = "STL decomposition of US retail employment",
       subtitle = "Employed = f(trend, seasonal, irregular)",
       y = NULL)
```

STL decomposition is the basis for other insights into the data series. You can measure the relative strength of trend and seasonality by the relative size of their variance: $F_T = 1 - \mathrm{Var}(R_T) / \mathrm{Var}(R_T + T_T)$ and $F_S = 1 - \mathrm{Var}(S_T) / \mathrm{Var}(S_T + T_T)$.

```{r message=FALSE, warning=FALSE}
us_employment_featues <- us_employment %>% 
  features(Employed, feat_stl) %>% 
  inner_join(us_employment %>% as_tibble() %>% select(Series_ID, Title) %>% unique(), 
             by = "Series_ID")

us_1 <- us_employment_featues %>%
  filter(trend_strength >= 0.995 & seasonal_strength_year > 0.9) %>%
  slice_sample(n = 1) %>%
  pull(Series_ID)
us_2 <- us_employment_featues %>%
  filter(trend_strength >= 0.995 & seasonal_strength_year < 0.5) %>%
  slice_sample(n = 1) %>%
  pull(Series_ID)
us_3 <- us_employment_featues %>%
  filter(trend_strength <= 0.985 & seasonal_strength_year < 0.5) %>%
  slice_sample(n = 1) %>%
  pull(Series_ID)
us_4 <- us_employment_featues %>%
  filter(trend_strength <= 0.985 & seasonal_strength_year > 0.9) %>%
  slice_sample(n = 1) %>%
  pull(Series_ID)
us_employment_ids <- c(us_1, us_2, us_3, us_4)
us_employment_featues %>% 
  mutate(Series_lbl = if_else(Series_ID %in% us_employment_ids, Title, NA_character_)) %>%
  ggplot(aes(x = trend_strength, y = seasonal_strength_year)) +
  geom_point(color = "goldenrod") +
  geom_text(aes(label = Series_lbl), color = "goldenrod4") +
  theme_light() +
  labs(title = "Some sectors have seasonal employment, othes trend, others both.")
```

## Toolbox



### Selecting Predictors

There are many strategies to choose regression model predictors when there are many to choose from.  There are five common measures of predictive accuracy: $\bar{R}^2$, CV, AIC, AICc, and BIC. They can be calculated using the `CV()` (cross-validation) function from the `forecast` package.

$\bar{R}^2$ is common and well-established, but tends to select too many predictor variables, making it less suitable for forecasting.  BIC has the feature that if there is a true underlying model, the BIC will select it given enough data. However, there is rarely a true underlying model, and even if there was one, that model would not necessarily produce the best forecasts because the parameter estimates may not be accurate.  The AICc, AIC, and CV statistics are usually best because forecasting is their objective. If the value of time series size $T$ is large enough, they all lead to the same model.

**$R^2$** is not a good measure of predictive ability because it does not measure bias: model that consistently predicts values that are 20% of observed will have $R^2 = 1$.  

$$\bar{R}^2 = 1 - (1 - R^2) \frac{T - 1}{T - k - 1}$$
where $T$ is the number of observations and $k$ is the number of predictors.  Maximizing $\bar{R}^2$ is equivalent to minimized the regression standard error $\hat{\sigma}$.

Classical leave-one-out cross-validation (**CV**) measures the predictive ability of a model.  In concept CV is calculated by fitting a model without observation $t$, then measuring the predictive error at observation $t$.  The process is repeated for all $T$ observations.  $CV$ is the mean squared error.  The model with the minimum CV is the best model for forecasting. 

$$CV = \frac{1}{T} \sum_{t=1}^T [\frac{e_t}{1 - h_t}]^2$$

where $h_t$ are the diagonal values of the hat-matrix $H$ from $\hat{y} = X\beta = X(X`X)^{-1}X'y = Hy$ and $e_t$ is the residual obtained from fitting the model to all $T$ observations.  

Closely related to CV is Akaike's Information Criterion (**AIC**), defined as

$$AIC = T \log(\frac{SSE}{T}) + 2(k + 2)$$

The measure penalises the models by the number of parameters that need to be estimated.  The model with the minimum AIC is the best model for forecasting. For large values of $T$, minimising the AIC is equivalent to minimising the CV.

For small values of $T$, the AIC tends to select too many predictors, and so a bias-corrected version of the AIC has been developed, **AICc**. 

$$AIC_c = AIC + \frac{2(k+2)(k + 3)}{T - k - 3}$$

**BIC** is similar to AIC, but penalises the number of parameters more heavily than the AIC. For large values of $T$, minimising BIC is similar to leave-*v*-out cross-validation when $v = T[1 âˆ’ 1/\log(T) - 1]$.

$$BIC = T \log(\frac{SSE}{T}) + (k + 2)\log(T)$$

There are two common methods for using the measures: **best subsets regression** and **stepwise regression**.  In best subsets regression, fit all possible models, then choose the one with the best metric value (e.g., lowest AIC).  If there are simply too many candidate models (e.g., 40 predictors yield $2^40$ models), the use stepwise regression.  In *backwards stepwise regression*, include *all* candidate predictors initially, then check whether leaving any one predictor out improves the evaulation metric.  If any leave-one-out model is better, then choose the best leave-one-out model.  Repeat until no leave-one-out model is better.

Here is an example from dataset `uschange` in the `fpp2` package.  `uschange` contains quarterly growth rates of personal consumption, income, production, and savings, and the unemployment rate in the US from 1960 to 2016.  The 4 predictors of consumption yield $2^4 = 16$ possible models.

```{r}
# inc <- c(1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0)
# prd <- c(1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0)
# sav <- c(1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0)
# emp <- c(1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0)
# df <- data.frame(inc, prd, sav, emp)
# 
# mat <- matrix(NA, nrow = 16, ncol = 5)
# mat[1,] <- CV(tslm(Consumption ~ Income + Production + Savings + Unemployment, data = uschange))
# mat[2,] <- CV(tslm(Consumption ~ Income + Production + Savings, data = uschange))
# mat[3,] <- CV(tslm(Consumption ~ Income + Production + Unemployment, data = uschange))
# mat[4,] <- CV(tslm(Consumption ~ Income + Production, data = uschange))
# mat[5,] <- CV(tslm(Consumption ~ Income + Savings + Unemployment, data = uschange))
# mat[6,] <- CV(tslm(Consumption ~ Income + Savings, data = uschange))
# mat[7,] <- CV(tslm(Consumption ~ Income + Unemployment, data = uschange))
# mat[8,] <- CV(tslm(Consumption ~ Income, data = uschange))
# mat[9,] <- CV(tslm(Consumption ~ Production + Savings + Unemployment, data = uschange))
# mat[10,] <- CV(tslm(Consumption ~ Production + Savings, data = uschange))
# mat[11,] <- CV(tslm(Consumption ~ Production + Unemployment, data = uschange))
# mat[12,] <- CV(tslm(Consumption ~ Production, data = uschange))
# mat[13,] <- CV(tslm(Consumption ~ Savings + Unemployment, data = uschange))
# mat[14,] <- CV(tslm(Consumption ~ Savings, data = uschange))
# mat[15,] <- CV(tslm(Consumption ~ Unemployment, data = uschange))
# mat[16,] <- CV(tslm(Consumption ~ 1, data = uschange))
# 
# #df2 <- as.data.frame(mat)
# df <- cbind(data.frame(inc, prd, sav, emp), as.data.frame(mat))
# colnames(df) <- c("Inc", "Prd", "Sav", "Emp", "CV", "AIC", "AICc", "BIC", "AdjR2")
# df$CV <- round(df$CV, 3)
# df$AIC <- round(df$AIC, 1)
# df$AICc <- round(df$AICc, 1)
# df$BIC <- round(df$BIC, 1)
# df$AdjR2 <- round(df$AdjR2, 3)
# 
# df %>% arrange(AICc) %>%
# knitr::kable()

```

The best model contains all four predictors. However, there is clear separation between the models in the first four rows and the ones below, so Income and Savings are more important than Production and Unemployment. Also, the first two rows have almost identical values of CV, AIC and AICc. So you could possibly drop the Production variable and get similar forecasts. 

## Time Series Regression

A time series regression forecasts a time series as a linear relationship with the independent variables.

$$y_t = X_t \beta + \epsilon_t$$

The linear regression model assumes there is a **linear** relationship between the forecast variable and the predictor variables. This implies that the errors must have mean zero, otherwise the forecasts are biased: $E(\epsilon | X_j) = 0$. The least squares method guarantees this condition is met. The residuals must not be autocorrelated, otherwise the forecasts are inefficient because there is more information in the data that can be exploited. To produce reliable inferences and prediction intervals, the residuals must be independent normal random variables with constant variance.

`fable::TSLM()` fits a linear regression model to time series data. `TSLM()` is similar to `lm()` with additional facilities for handling time series.

Data set `fpp3::us_change` contains quarterly US growth rates of personal $\mathrm{Consumption}$, $\mathrm{Income}$, and $\mathrm{Production}$, and the quarterly $\mathrm{Unemployment}$ rate and $\mathrm{Savings}$ rate. 

The scatterplot matrix shows positive relationships with income and industrial production, and negative relationships with savings and unemployment. 

```{r}
fpp3::us_change %>%
  as_tibble() %>%
  select(-Quarter) %>%
  cor() %>%
  ggcorrplot::ggcorrplot(type = "upper", lab = TRUE, lab_size = 3) +
  theme_light() +
  labs(title = "Consumption is correlated with predictors",
       subtitle = "Correlation plot of US economic indicators.", 
       caption = "Source: fpp3::us_change.",
       x = NULL, y = NULL)

```

Let's fit this model: 

$$\mathrm{Consumption}_t = \beta_0 + \beta_1 \mathrm{Income}_t + \beta_2 \mathrm{Production}_t + \beta_3 \mathrm{Savings}_t + \beta_4 \mathrm{Unemployment}_t + \epsilon_t$$

```{r}
us_change_lm <- fpp3::us_change %>%
  model(TSLM(Consumption ~ Income + Production + Savings + Unemployment))

report(us_change_lm)
```

The fitted values follow the actual data fairly closely. The modeled $R^2$ is `r us_change_lm %>% broom::glance() %>% pull("r_squared") %>% scales::comma(accuracy = .001)`, the adjusted $R^2$ is `r us_change_lm %>% broom::glance() %>% pull("adj_r_squared") %>% scales::comma(accuracy = .001)`, and the standard error of the regression, $\hat{\sigma}_\epsilon$$ is `r us_change_lm %>% broom::glance() %>% pull(sigma2) %>% sqrt() %>% scales::comma(accuracy = .001)`. The fitted to actual values plot has a strong linear relationship.

```{r}
p1 <- augment(us_change_lm) %>%
  ggplot(aes(x = Quarter)) +
  geom_line(aes(y = Consumption), color = "dark gray", size = 1) +
  geom_line(aes(y = .fitted), color = "goldenrod", size = 1) +
  theme_light() +
  labs(subtitle = "Time series")
p2 <- augment(us_change_lm) %>%
  ggplot(aes(x = Consumption, y = .fitted)) +
  geom_point(color = "goldenrod", size = 1) +
  geom_abline(intercept = 0, slope = 1, linetype = 2, size = 1, color = "dark gray") +
  theme_light() +
  labs(subtitle = "Predicted vs actuals")

p1 + p2 +
  patchwork::plot_annotation(title = "Fitted values plots")
```

Evaluate the regression model with the following diagnostics.

The residuals vs time plot reveals some heteroscedasticity. Heteroscedasticity can make prediction intervals inaccurate.

The histogram shows that the residuals are slightly skewed. Non-normality of the residuals can also make the prediction intervals inaccurate.

The autocorrelation function plot (ACF) finds a significant spike at lag 7. If autocorrelation exists, forecasts are still unbiased, but the prediction intervals are larger than they need to be. Another test of autocorrelation in the residuals is the Breusch-Godfrey test for serial correlation up to a specified order. A small p-value indicates there is significant autocorrelation remaining in the residuals. The Breusch-Godfrey test is similar to the Ljung-Box test, but it is specifically designed for use with regression models. In this case, the spike at lag 7 is not enough for the Breusch-Godfrey to be significant (*p* = 0.062). In any case, the autocorrelation is not particularly large, and at lag 7 it is unlikely to have a noticeable impact on the forecasts or the prediction intervals. 

```{r}
gg_tsresiduals(us_change_lm)
```

The residuals should be independent of each of the explanatory variables *and* independent of candidate variables not used in the model. In this case, the residuals show a random pattern in each of the plots.

```{r}
us_change %>%
  left_join(residuals(us_change_lm), by = "Quarter") %>%
  pivot_longer(Income:Unemployment, names_to = "regressor", values_to = "x") %>%
  ggplot(aes(x = x, y = .resid, color = regressor)) +
  geom_point(show.legend = FALSE) +
  facet_wrap(vars(regressor), scales = "free_x") +
  labs(title = "There is no relationship between residuals and individual regressors.",
       subtitle = "otherwise the relationship may be nonlinear.",
       x = NULL) +
  theme_light() +
  ggthemes::scale_color_few()
```

A second check on the homoscedastity assumption is a plot of the residuals against the fitted values. Again, there should be no pattern. 

```{r}
augment(us_change_lm) %>%
  ggplot(aes(x = .fitted, y = .resid)) +
  geom_point() +
  labs(title = "There is no relationship between residuals and fitted values.",
       subtitle = "otherwise the response variable may require transformation.",
       y = "Residuals", x = "Fitted") +
  theme_light()
# cbind(Fitted = fitted(uschange.lm),
#       Residuals = residuals(uschange.lm)) %>%
#   as.data.frame() %>%
#   ggplot(aes(x=Fitted, y=Residuals)) + geom_point()
```

Check for *outliers*, *leverage points*, and *influential points*. For multiple linear regression models there is no straight-forward visual diagnostic like the simple linear regression scatter plot. Recall, an outlier is a point far from the others (in either the x or y direction); a leverage point is far from the others in the x direction, potentially affecting the measured slope; and influential point is a leverage point that *does* affect the slope.

The "hat matrix" $H$ identifies leverage points. Recall that in the linear regression model $\hat{y} = X \hat{\beta}$ the slope coefficients are estimated by $\hat{\beta} = (X'X)^{-1}X'y$. Substituting, $\hat{y} = X(X'X)^{-1}X'y$, or $\hat{y} = Hy$, where 

$$H = X(X'X)^{-1}X'.$$ 

$H$ is called the hat matrix because $H$ puts the hat on $y$. $H$ is an $n \times n$ matrix.  The diagonal elements $H_{ii}$ are a measure of the distances between each observation $i$'s predictor variables $X_i$ and the average of the entire data set predictor variables $\bar{X}$. $H_{ii}$ are the leverage that the observed responses $y_i$ exert on the predicted responses $\hat{y}_i$. Each $H_{ii}$ is in the unit interval [0, 1] and the values sum to the number of regression parameters (including the intercept) $\sum{H_{ii}} = k + 1$.  A common rule is to research any observation whose leverage value is more than 3 times larger than the mean leverage value, which since the sum of the leverage values is $k + 1$, equals 

$$H_{ii} > 3 \frac{k + 1}{n}.$$

There are multiple methods to identify influential points. One is Cook's distance. Cook's distance for observation $i$ is defined 

$$D_i = \frac{(y_i - \hat{y}_i)^2}{p \times MSE} \frac{H_{ii}}{(1 - H_{ii})^2}.$$

$D_i$ directly summarizes how much all of the fitted values change when the ith observation is deleted. A data point with $D_i > 1$ is probably influential. $D_i > 0.5$ is at least worth investigating.

```{r}
car::influenceIndexPlot(us_change_lm)
cooks.distance(us_change_lm)
# autoplot(cooks.distance(uschange.lm)) +
#   geom_hline(yintercept = 0.5, linetype="dashed") +
#   labs(title = "Cook's Distance",
#        subtitle = "Investigate distances > 0.5.",
#        x = "Observation Number",
#        y = "Cook's")
```

There are several predictor variables that you may add to a time series regression model.  

The **trend** is the slope of $y_t = \beta_0 + \beta_1 t + \epsilon_t$.  The **season** is a factor indicating the season (month, quarter, etc.) based on the frequency of the data.  The time series trend and seasaon is calculated on the fly in the `tslm()` function as variables `trend` and season.  Here is an example using the Autralian beer production dataset `ausbeer`.  This model includes both a trend variable and a seasonal variable.

$$y_t = \beta_0 + \beta_1 t + \beta_2 S2 + \beta_3 S3 + \beta_4 S4$$

where $S2$, $S3$, and $S4$ are seasonal (quarterly) dummiess, with season 1 as the reference.  The model is concisely formulated in `tslm()`.  There is a downward trend of 0.34 megalitres per quarter. On average, Q2 production is lower than Q1 by 34.7 megalitres, Q3 production is lower than Q1 by 17.8 megalitres, and Q4 production is higher than Q1 by 72.8 megalitres.

```{r}
# y <- window(ausbeer, start = 1992)
# summary(ausbeer.lm <- tslm(y ~ trend + season))
# autoplot(y, series = "Data") +
#   autolayer(forecast(ausbeer.lm, h=20), series = "Forecast") +
#   autolayer(fitted(ausbeer.lm), series = "Fitted") +
#   labs( x = "Quarter",
#         y = "Megalitres",
#         title = "Quarterly Beer Production",
#         subtitle = "Linear time series regression with trend and seasonal dummies.")
```

You can also create dummy variables to flag holidays and outliers.

Predict future values with ex-ante forecasts or ex-post forecasts.  An ex-ante forecasts are possible when the model is based only on calendar effects.

```{r}
# y <- window(ausbeer, start = 1992)
# ausbeer.lm <- tslm(y ~ trend + season)
# ausbeer.fcst <- forecast(ausbeer.lm, h = 8)
# autoplot(ausbeer.fcst) +
#   ggtitle("Time Series Regression with Forecast")
```

Ex-post forecasts require assumptions (scenarios) about future values.

```{r}
# uschange.lm <- tslm(Consumption ~ Income + Savings + Unemployment,
#                     data = uschange)
# newdata = data.frame(
#     Income = c(1, 1, 1, 1),
#     Savings = c(0.5, 0.5, 0.5, 0.5),
#     Unemployment = c(0, 0, 0, 0))
# uschange.fcst <- forecast(uschange.lm, newdata = newdata)
# autoplot(uschange.fcst) +
#   ggtitle("US Change in Consumption, ex-post Model")
```


## Exponential Smoothing

Exponential smoothing weights recent observations more heavily than remote observations.  Think of exponential smoothing as a family of methods varying by their *trend* and *seasonal* components. Additionally, the *errors* may be additive or multiplicative. 

* There can be no trend (N), or it can be an additive (A) linear trend from the forecast horizon, or it can be a damped additive (A<sub>d</sub>) trend leveling off from the forecast horizon.

* There can be no seasonality (N), or it can be additive (A), or additive damped (A<sub>d</sub>).

* The errors may be constant over time (A), or increase with the level (M).

The trend and seaonality combinations produce 3 x 3 = 9 possible exponential smoothing methods.  The parameters determining the level, trend, and seasonality of the exponential smoothing model are based on minimization of the sum of square errors (SSE) of the simultaneous equations.  

The two treatment of errors double the number of possible *state space models* to 18.  State space models include error, trend, and seasonality components and are therefore called **ETS models**.  ETS models do not just extend the exponential smoothing models to account for treatment of the error variance.  They also estimate their parameters differently.  ETS models use maximum likelihood estimation.  For models with additive errors, this is equivalent to minimizing the sum of squared errors (SSE). The great advantage of using ETS models is that you can optimize the parameter settings by minimizing the Akaike Information Criterion (AIC<sub>c</sub>).

The sections below describe the basic exponential smoothing models, focusing on the structure and parameters.

### Simple Exponential Smoothing

Simple exponential smoothing models have no seasonal or trend components.  Simple exponential smoothing models are of the form $\hat{y}_{t+h|t} = \alpha y_t + \alpha(1-\alpha)y_{t-1} + \alpha(1-\alpha)^2y_{t-2} \dots$ where $0 < \alpha < 1$ is a weighting parameter.  Exponential smoothing models are commonly expressed in a component form as a regressive model. The first component, the forecast, is the last value of the estimated level.

$$\hat{y}_{t+h|t} = l_t$$
The second component, the level, describes how the level changes over time.

$$l_t = \alpha y_t + (1 - \alpha)l_{t-1}$$

$l_t$ is the level (or smoothed value) of the series at time $t$.  Expressed this way, it is clear there are two parameters to estimate: $\alpha$ and $l_0$.  Simple exponential smoothing estimates the parameters by minimizing the SSE.  Unlike regression, which returns exact parameter estimates, the SSE for the exponential equation is minimized with nonlinear optimization.  The `ses()` function performs simple exponential smoothing.

Here is simple exponential smoothing applied to the `marathon` dataset to produce a 10-year forecast.  $\alpha = 0.3457$, and $l_0 = 167.1741$.

```{r}
# marathon.train <- subset(marathon, end = length(marathon) - 10)
# marathon.test <- subset(marathon, start = length(marathon) - 9)
# marathon.ses <- ses(marathon.train, h = 10)
# summary(marathon.ses)
```

Simple exponential smoothing produces a flat line that is exponentially weighted from the prior values, then extended into the forecast period.

```{r}
# autoplot(marathon.ses) +
#   autolayer(marathon, series="Actual") +
#   autolayer(fitted(marathon.ses), series="Fitted") +
#   autolayer(marathon.ses$mean, series="Forecast") +
#   labs(title = "Boston Marathon Winning Times with 10-year Forecast",
#        subtitle = "Method: Simple Exponential Smoothing",
#        y = "Minutes",
#        x = "Year") +
#   guides(colour=guide_legend(title="Series"), 
#          fill=guide_legend(title="Prediction interval")) +
#   scale_color_manual(values = c("black", "red", "blue"))
```

Check the model assumptions with `checkresiduals()`.  The residuals plot has constant and independent variance, at least after 1930.  The histogram has a normal distribution.  The autocorrelation function (ACF) plot shows spikes all within the insignificance band, yet the Ljung-Box test rejects the null hypothesis of no autocorrelation of the residuals (p = 0.0240).  The forecast might still be useful even with residuals that don't quite pass the white noise test.

```{r}
# checkresiduals(marathon.ses)
```



### Holt

Holt's linear trend model expands simple exponential smoothing with a trend component. The forecast contains both a level and a trend.

$$\hat{y}_{t+h|t} = l_t + hb_t$$
The level is adjusted for the trend too.  

$$l_t = \alpha y_t + (1 - \alpha)(l_{t-1} + hb_{t-1})$$

A third equation, the trend, describes how the slope changes over time.  $\beta^*$ describes how quickly the slope can change.

$$b_t = \beta^*(l_t - l_{t-1}) + (1 - \beta^*)b_{t-1}$$

Now there are four parameter to estimate, $\alpha$, $\beta^*$, $l_0$, and $b_0$.  Holt estimates the parameters by minimizing the SSE.  The `holt` function performs Holt's linear method.  A variation of Holt's linear trend method is Holt's *damped trend* method.  Whereas Holt's linear trend stays constant over time, the damped trend levels off to a constant.  Add the `damped = TRUE` parameter to `holt()` to use the damped trend method.

Here is Holt's linear trend applied to the `austa` dataset from the `fpp2` package to produce a 10-year forecast.  `austa` contains total annual international visitors to Australia, 1980-2015.

```{r}
# austa.train <- subset(austa, end = length(austa) - 10)
# austa.test <- subset(austa, start = length(austa) - 9)
# austa.holt.lin <- holt(austa.train, h = 10)
# austa.holt.dmp <- holt(austa.train, h = 10, damped = TRUE)
# summary(austa.holt.lin)
```

Holt's linear trend produces a sloped, but straight line.  The forecasted values from the damped trend version are overlaid in green. They just start to even out at the forecast horizon.

```{r}
# autoplot(austa.holt.lin) +
#   autolayer(austa, series = "Actual") +
#   autolayer(fitted(austa.holt.lin), series = "Fitted") +
#   autolayer(austa.holt.lin$mean, series = "Forecast") +
#   autolayer(austa.holt.dmp$mean, series = "Forecast (damped)") +
#   labs(title = "International Visitors to Australia with 10-year Forecast",
#        subtitle = "Method: Holt's Linear Trend",
#        y = "Visitors",
#        x = "Year") +
#   guides(colour=guide_legend(title = "Series"), 
#          fill=guide_legend(title = "Prediction interval")) +
#   scale_color_manual(values = c("black", "red", "blue", "green"))
```

Check the model assumptions with `checkresiduals`.  The residuals plot looks has constant and independent variance.  The histogram does not really show a normal distribution.  The autocorrelation function (ACF) plot shows spikes all within the insignificance band, and the Ljung-Box test fails to reject the null hypothesis of no autocorrelation of the residuals (p = 0.3253).

```{r}
# checkresiduals(austa.holt.lin)
```

### Holt-Winters

The Holt-Winters method adds a seasonality component.  There are two versions of this model, the *additive* and the *multiplicative*.  The additive method assumes the error variance is constant, and the multiplicative version assumes the error variance scales with the level.  Here is the additive version first.  The forecast includes a level, trend, and now also a season.

$$\hat{y}_{t+h|t} = l_t + hb_t + s_{t-m+h_m^+}$$

The level is adjusted for the trend and now the season too.

$$l_t = \alpha(y_t - s_{t-m}) + (1 - \alpha)(l_{t-1} + b_{t-1})$$
The trend is not affected by the seasonal component.
$$b_t = \beta^*(l_t - l_{t-1}) + (1 - \beta^*)b_{t-1}$$
The seasonal compenent changes over time in relation to the $\gamma$ parameter.  $m$ is the period of seasonality.  

$$s_t = \gamma(y_t - l_{t-1} - b_{t-1}) + (1 - \gamma)s_{t-m}$$

There are now three smoothing parameters: $0 \le \alpha \le 1$, $0 \le \beta^* \le 1$, and $0 \le \gamma \le 1-\alpha$.  In the additive version, the seasonal component averages to zero.  In the multiplicative version, the seasonality averages to one.  Use the multiplicative method if the seasonal variation increases with the level.
$$\hat{y}_{t+h|t} = (l_t + hb_t) s_{t-m+h_m^+}$$
$$l_t = \alpha\frac{y_t}{s_{t-m}} + (1 - \alpha)(l_{t-1} + b_{t-1})$$
$$b_t = \beta^*(l_t - l_{t-1}) + (1-\beta*)b_{t-1}$$
$$s_t = \gamma\frac{y_t}{(l_{t-1} - b_{t-1})} + (1 - \gamma)s_{t-m}$$

Here is the Holt-Winters model applied to the `a10` dataset from the `fpp2` package to produce a 36-month forecast.  `a10` contains monthly anti-diabetic drug sales in Australia, 1991-2008.  The error variance increases with the series level, so the multiplicative method applies.  The model estimates the three smoothing parameters, plus initial states, including 12 initial season states - one for each month in the year.

```{r}
# a10.train <- subset(a10, end = length(a10) - 36)
# a10.test <- subset(a10, start = length(a10) - 35)
# a10.hw <- hw(a10, seasonal = "multiplicative", h = 36)
# summary(a10.hw)
```

```{r}
# autoplot(a10.hw) +
#   autolayer(a10, series = "Actual") +
#   autolayer(fitted(a10.hw), series = "Fitted") +
#   autolayer(a10.hw$mean, series = "Forecast") +
#   labs(title = "Anti-Diabetic Drug Sales in Australia with 36-month Forecast",
#        subtitle = "Method: Holt-Winters (multiplicative)",
#        y = "Scripts",
#        x = "Month") +
#   guides(colour=guide_legend(title = "Series"), 
#          fill=guide_legend(title = "Prediction interval")) +
#   scale_color_manual(values = c("black", "red", "blue"))
```

Check the model assumptions with `checkresiduals`. The residuals plot shows some long-term autocorrelation (a long hump), and the variance increases in the latter years. The histogram shows a normal distribution. The autocorrelation function (ACF) plot shows many spikes outside the insignificance band, and the Ljung-Box test rejects the null hypothesis of no autocorrelation of the residuals (p < 0.0001).

```{r}
# checkresiduals(a10.hw)
```

Here is a four-week Holt-Winters forecast of the `hyndsight` dataset of daily pageviews on the Hyndsight blog for one year starting April 30, 2014. Create a training dataset consisting of all obserations minus the last four weeks.  Then forecast those four weeks with Holt-Winters.  Use the `additive` method because the variance is not scaling with page volume.  Creae a second forecast with the seasonal naive method as a benchmark.



Notice that the Ljung-Box test rejects the null hypothesis of no autocorrelation of the residuals.  The forecast might still provide useful information even with residuals that fail the white noise test.

```{r}
# hyndsight.train <- subset(hyndsight, end = length(hyndsight) - 4*7)
# 
# hyndsight.hw <- hw(hyndsight.train, seasonal = "additive", h = 4*7)
# 
# hyndsight.sn <- snaive(hyndsight.train, h = 4*7)
# 
# checkresiduals(hyndsight.hw)
```

Compare Holt-Winters to the seasonal naive forecast.  The RMSE of Holt-Winters (201.7656) is smaller than the RMSE of seasonal naive (202.7610), so it is the more accurate forecast.

```{r}
# accuracy(hyndsight.hw, hyndsight)
# accuracy(hyndsight.sn, hyndsight)
```

Here finally is a plot of the forecasted page views.

```{r}
# autoplot(hyndsight) +
#   autolayer(hyndsight.hw$mean)
```

### ETS

The namesake function for finding errors, trend, and seasonality (ETS) provides a completely automatic way of producing forecasts for a wide range of time series.

```{r}
# fets <- function(y, h) {forecast(ets(y), h = h)}
# a10.ets <- ets(a10.train)
# a10.snaive <- snaive(a10.train)
# a10.ets.cv <- tsCV(a10.train, fets, h = 4)
# a10.snaive.cv <- tsCV(a10.train, snaive, h = 4)
# mean(a10.ets.cv^2, na.rm = TRUE)
# mean(a10.snaive.cv^2, na.rm = TRUE)
```

## ARIMA

ARIMA models are another approach to time series forecasting. Exponential smoothing and ARIMA models are the two most widely-used approaches to time series forecasting, and provide complementary approaches to the problem. While exponential smoothing models are based on a description of the trend and seasonality in the data, ARIMA models aim to describe the autocorrelations in the data.

An **autoregressive** (AR) model is a multiple regression with *lagged observations* as predictors.

$$y_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \dots + \phi_p y_{t-p} + e_t$$

A **moving average** (MA) model is a multiple regression with *lagged errors* as predictors.

$$y_t = c + e_t + \theta_1 y_{t-1} + \theta_2 y_{t-2} + \dots + \theta_p y_{t-q}$$

An **autoregressive moving average** (ARMA) model is a multiple regression with *lagged observations and lagged errors* as predictors.

$$y_t = c + \phi_1 y_{t-1} + \dots + \phi_p y_{t-p} + \theta_1 y_{t-1} + \dots + \theta_p y_{t-q} + e_t$$

An **ARMA model with differencing** (ARIMA(p,d,q)) model is an ARMA model with *d* levels of differencing.

Whereas ETS models can handle non-constant variance with multiplicative errors and seasonality, ARIMA models require that you explicitly transform the data.  There are an infinite number of tranformations.  In increasing strength, there is the square root ($y_t^{.5}$), cube root ($y_t^{.333}$), log ($log(y_t)$), and inverse ($y_t^{-1}$).

Notice below how the various transformations dampen the seasonal fluctuations.  The square root and log transformations are not quite strong enough to even out the error variance, but the inverse transformation is a little too large.  We need a transformation somewhere in between.

```{r}
# x <- cbind(usmelec,
#            `lambda = .5` = sqrt(usmelec),
#            `lambda = log` = log(usmelec),
#            `lambda = -1` = -usmelec^-1)
# autoplot(x, facet=TRUE) +
#   labs(title = "US Net Electricity Generation",
#        x = "Month", y = "kWh") 
```

The Box-Cox transformation (described above) can find the optimal transformation.  In this case, the optimal transformation is $w_t = (y_t^\lambda - 1) / \lambda$ where $\lambda = -0.5738$.

```{r}
# (lambda <- BoxCox.lambda(usmelec))
# df <- cbind(Raw = usmelec,
#             BoxCox = BoxCox(usmelec, lambda = lambda))
# autoplot(df, facet=TRUE) +
#   xlab("Month") + ylab("kWh") +
#   ggtitle("Electricity Production: Jan 1956 - Aug 1995")

```

Here is how you would use the transformation in a forecast.  (*This example is contrived because ets models can handle non-constant variance with multiplicative errors and seasonality*).

```{r}
# usmelec %>% 
#   ets(lambda = BoxCox.lambda(usmelec)) %>%
#   forecast(h = 60) %>%
#   autoplot()
```

R function `auto.arima()` from the `forecast` package chooses the optimal ARIMA model parameters using the Akaike criterion.^[You can use the Akaike criterion to compare models of the same class, but not different models, so do not use it to compare an ARIMA model to an ETS model.  You cannot use the Akaike criterion for models of different levels of differencing.]

Here is a plot of annual US net electricity generation from the `usnetelec` dataset in the `expsmooth` package.  You can probably guess straight away that this time series will require one level of differencing (*d = 1*).

```{r}
# autoplot(usnetelec) +
#   labs(title = "Annual US Net Electricity Generation",
#        y = "billion kWh",
#        x = "Year")
```

The `auto.arima()` function chooses an ARIMA(2,1,2) with drift.

```{r}
# usnetelec.arima <- auto.arima(usnetelec)
# summary(usnetelec.arima)
```

Here is a plot of the forecast.

```{r}
# usnetelec.arima %>% forecast(h = 10) %>% autoplot()
```

Explicitly choose a model with the `Arima()` function.

```{r}
# usnetelec %>% Arima(order = c(2, 1, 2), include.constant = TRUE) %>% forecast() %>% autoplot()
```

Compare models of different classes with cross-validation.  

```{r}
# # Set up forecast functions for ETS and ARIMA models
# fets <- function(x, h) {
#   forecast(ets(x), h = h)
# }
# farima <- function(x, h) {
#   forecast(auto.arima(x), h = h)
# }
# 
# # Compute CV errors for ETS on austa as e1
# e1 <- tsCV(austa, fets, h = 1)
# 
# # Compute CV errors for ARIMA on austa as e2
# e2 <- tsCV(austa, farima, h = 1)
# 
# # Find MSE of each model class
# mean(e1^2, na.rm = TRUE)
# mean(e2^2, na.rm = TRUE)
# 
# # Plot 10-year forecasts using the best model class
# austa %>% farima(h = 10) %>% autoplot()
```

If the time series includes seasonality, an ARIMA(p,d,q)(P,D,Q)[m] model includes seasonality.

```{r}
# # Check that the logged h02 data have stable variance
# h02 %>% log() %>% autoplot()
# 
# # Fit a seasonal ARIMA model to h02 with lambda = 0
# fit <- auto.arima(h02, lambda = 0)
# 
# # Summarize the fitted model
# summary(fit)
# 
# # Plot 2-year forecasts
# fit %>% forecast(h = 24) %>% autoplot()
```

```{r}
# # Use 20 years of the qcement data beginning in 1988
# train <- window(qcement, start = 1988, end = c(2007, 4))
# 
# # Fit an ARIMA and an ETS model to the training data
# fit1 <- auto.arima(train)
# fit2 <- ets(train)
# 
# # Check that both models have white noise residuals
# checkresiduals(fit1)
# checkresiduals(fit2)
# 
# # Produce forecasts for each model
# fc1 <- forecast(fit1, h = 1 + 4 * (2013 - 2007))
# fc2 <- forecast(fit2, h = 1 + 4 * (2013 - 2007))
# 
# # Use accuracy() to find better model based on RMSE
# accuracy(fc1, qcement)
# accuracy(fc2, qcement)
# bettermodel <- fit2
```






The *White Noise* (WN) model is the simplest example of a stationary process.  It has a fixed mean and variance.  The WN model is one of several autoregressive integrated moving average (ARIMA) models.  An ARIMA(p, d, q) model has three parts, the autoregressive order `p` (number of time lags), the order of integration (or differencing) `d`, and the moving average order `q`.  When two out of the three terms are zeros, the model may be referred to based on the non-zero parameter, dropping "AR", "I" or "MA" from the acronym describing the model. For example, ARIMA (1, 0,0) is AR(1), ARIMA(0,1,0) is I(1), and ARIMA(0,0,1) is MA(1). The WN model is ARIMA(0,0,0).  

Simulate a WN time series using the `arima.sim()` function with argument `model = list(order = c(0, 0, 0))`.  Here is a 50-period WN model with `mean` 100 and standard deviation `sd` of 10.

```{r}
wn <- arima.sim(model = list(order = c(0, 0, 0)), 
                n = 50, 
                mean = 100, 
                sd = 10)
ts.plot(wn,
        xlab = "Period", 
        ylab = "", 
        main = "WN Model, mean = 100, sd = 10")
```

Fit a WN model to a dataset with `arima(x, order = c(0, 0, 0))`.  The model returns the mean, var, and se.  

```{r}
arima(wn, order = c(0, 0, 0))
```
The model mean will be identical to the series mean.  The variance will be close to the series variance.

```{r}
# mean = intercept
mean(wn)

# se ~ s.e.
sqrt(var(wn) / length(wn))

# var ~ sigma^2
var(wn)
```

The *Random Walk* (RW) model is a WN model with a strong time dependance, so there is no fixed mean or variance.  It is a non-stationary model.  The random walk model is $Y_t = c + Y_{t-1} + \epsilon_t$ where $c$ is the drift coefficient, an overall slope parameter.  

Simulate a RW time series using the `arima.sim()` function with argument `model = list(order = c(0, 1, 0))`.  Here is a 50-period RW model with `mean` 0.

```{r}
rw <- arima.sim(model = list(order = c(0, 1, 0)), 
                n = 50)
ts.plot(rw,
        xlab = "Period", 
        ylab = "", 
        main = "RW Model, mean = 0")
```

The first difference of a RW model is just a WN model.

```{r}
ts.plot(diff(rw),
        xlab = "Period", 
        ylab = "", 
        main = "Diff(RW) = WN")
```

Specify the drift parameter with `mean`.  The drift parameter is the slope of the RW model.

```{r}
rw <- arima.sim(model = list(order = c(0, 1, 0)), 
                n = 100, 
                mean = 1)
ts.plot(rw,
        xlab = "Period", 
        ylab = "", 
        main = "RW Model, mean = 1")
```

Fit a random walk model with drift by first differencing the data, then fitting the WN model to the differenced data.  The `arima()` intercept is the drift variable.

```{r}
wn.mod <- arima(diff(rw), 
                order = c(0, 0, 0))

ts.plot(rw)
abline(a = 0, b = wn.mod$coef)

rw.mod <- arima(rw, 
                order = c(0, 1, 0))
points(rw - residuals(rw.mod), type = "l", col = 2, lty = 2)
```

When dealing with time series data, ask first if it stationary because stationary models are much simpler.  A stationary process oscillates randomly about a fixed mean, a phenomenon called *reversion to the mean*.  In contrast, nonstationary processes have time trends, periodicity, or lack mean reversion.  Even when a process is nonstationary, the *changes* in the series may be approximately stationary.  For example, inflation rates show a pattern over time related to Federal Reserve policy, but the *changes* in interest rates are stationary.

WN processes are stationary, but RW processes (the cumulative sum of the WN process) are not.  Here are plots of a WN process and corresponding RW process using the `cumsum()` of the WN.  Only the WN process is stationary.

```{r}
# White noise
wn <- arima.sim(model = list(order = c(0, 0, 0)), 
                n = 100) 

# Random walk from white noise
rw <- cumsum(wn)
  
plot.ts(cbind(wn, rw),
        xlab = "Period",
        main = "WN with Zero Mean, and Corresponding RW")
```

Here is another WN process with corresponding RW process, this time with a drift parameter of 0.4.

```{r}
# White noise with mean <> 0
wn <- arima.sim(model = list(order = c(0, 0, 0)), 
                n = 100, 
                mean = 0.4) 
  
# Random walk with drift from white noise
rw <- cumsum(wn)

plot.ts(cbind(wn, rw),
        xlab = "Period",
        main = "WN with Nonzero Mean, and Corresponding RW")
```



### Autoregression

The autoregressive (AR) model is the most widely used time series model. It shares the familiar interpretation of a simple linear regression, but each observation is regressed on the previous observation. 

$$Y_t - \mu = \phi(Y_{t-1} - \mu) + \epsilon_t$$

where $\epsilon_t \sim WN(0, \sigma_\epsilon^2)$.  The AR model also includes the white noise (WN) and random walk (RW) models as special cases. 

The `arima.sim()` function can simulate data from an AR model by setting the `model` argument equal to `list(ar = phi)` where `phi` is a slope parameter in the interval (-1, 1).  As `phi` approaches 1, the plot smooths.  With negative `phi` values, the plot oscillates.

```{r}
# small autocorrelation
x <- arima.sim(model = list(ar = 0.5), n = 100)

# large autocorrelation
y <- arima.sim(model = list(ar = 0.9), n = 100)

# negative autocorrelation (oscillation)
z <- arima.sim(model = list(ar = -0.75), n = 100)

plot.ts(cbind(x, y, z))
```

The plots generated by the `acf()` function provide useful information about each lag. Series `x` (small slope parameter) has positive autocorrelation for the first couple lags, but they quickly decay toward zero. Series `y` (large slope parameter) has positive autocorrelation for many lags. Series `z` (negative slope parameter) has an oscillating pattern. 

```{r}
par(mfrow = c(2,2))
acf(x)
acf(y)
acf(z)
```

The stationary AR model has a slope parameter between -1 and 1. The AR model exhibits higher persistence when its slope parameter is closer to 1, but the process reverts to its mean fairly quickly. Its sample ACF also decays to zero at a quick (geometric) rate, meaning values far in the past have little impact on the present value of the process.

Below, the AR model with slope parameter 0.98 exhibits greater persistence than with slope parameter 0.90, but both decay to 0. 

```{r}
ar90 <- arima.sim(model = list(ar = 0.9), n = 200)
ar98 <- arima.sim(model = list(ar = 0.98), n = 200)

par(mfrow = c(2,2))
ts.plot(ar90)
ts.plot(ar98)
acf(ar90)
acf(ar98)
```


By contrast, the random walk (RW) model is a special case of the AR model in which the slope parameter is equal to 1.  The RW model is nonstationary, and shows considerable persistence and relatively little decay in the ACF. 

```{r}
ar100 <- arima.sim(model = list(order = c(0, 1, 0)), n = 200)
par(mfrow = c(2,1))
ts.plot(ar100)
acf(ar100)
```

Fit the AR(1) model (autoregressive model with one time lag) using the `arima()` function with `order = c(1, 0, 0)`, meaning 1 time lag, 0 differencing, and 0 order moving average.  

Below is an AR(1) model fit to the `AirPassengers` dataset.  The output of the `arima` function shows $\phi$ as `ar1 = 0.9646`, $\mu$ as `intercept = 278.4649`, and $\hat{\sigma}_\epsilon^2$ as `sigma^2 = 1119`. 

```{r}
ar1 <- arima(AirPassengers, order = c(1, 0, 0))
ar1.fit <- AirPassengers - residuals(ar1)
print(ar1)

par(mfrow = c(2, 1))
ts.plot(AirPassengers)
points(ar1.fit, type = "l", col = 2, lty = 2)
acf(AirPassengers)
```

Use the ARIMA model to forecast observations with the `predict()` function. Specify the number of periods beyond the last observation with the `n.ahead` parameter.

Below is a forecast of 10 periods (years) beyond the 1871-1970 annual observations in the `Nile` dataset. The relatively wide band of confidence (dotted lines) is a result of the low persistence in the data. 

```{r}
ar1 <-arima(Nile, order  = c(1, 0, 0))
print(ar1)

ts.plot(Nile, xlim = c(1871, 1980))
ar1.fit <- Nile - resid(ar1)

ar.pred <- predict(ar1, n.ahead = 10)$pred
ar.pred.se <- predict(ar1, n.ahead = 10)$se
points(ar.pred, type = "l", col = 2)
points(ar.pred - 2*ar.pred.se, type = "l", col = 2, lty = 2)
points(ar.pred + 2*ar.pred.se, type = "l", col = 2, lty = 2)
```


### Simple Moving Average

The simple moving average (MA) model is

$$Y_t = \mu + \epsilon_t + \theta\epsilon_{t-1}$$ 

If the slope parameter $\theta$ is zero, $Y_t$ is a white noise process, $Y_t \sim (\mu, \sigma_\epsilon^2)$.  Large $\theta$ indicates large autocorrelation.  Negative $\theta$ indicates an oscillating series.

The MA model is used to account for very short-run autocorrelation. Each observation is regressed on the previous innovation, which is not actually observed. Like the AR model, the MA model includes the white noise (WN) model as special case. 

Simulate the MA model using `arima.sim()` with parameter `list(ma = theta)`, where `theta` is a slope parameter from the interval (-1, 1).  

Here are three MA models.  The first has slope paramter 0.5 and the second has slope parameter 0.9.  The second plot shows more persistance as a result.  THe third plot has a negative slope parameter and oscillates as a result.

```{r}
x <- arima.sim(model = list(ma = 0.5), n = 100)
y <- arima.sim(model = list(ma = 0.9), n = 100)
z <- arima.sim(model = list(ma = -0.5), n = 100)

plot.ts(cbind(x, y, z))
```

Use the `acf()` function to estimate the autocorrelation functions.  The MA series x with slope = 0.5 has a positive sample autocorrelation at the first lag, but it is approximately zero at other lags. The series y with slope = 0.9 has a larger sample autocorrelation at its first lag, but it is also approximately zero for the others. The series z with slope = -0.5 has a negative sample autocorrelation at the first lag and alternates, but is approximately zero for all higher lags. 

```{r}
par(mfrow = c(2, 2))
acf(x)
acf(y)
acf(z)
```

Fit the MA model using the `arima()` function with `order = c(0, 0, 1), meaning 0 time lag, 0 differencing, and 1st order moving average.

Below is an MA model fit to the `Nile` dataset. The output of the arima function shows $\theta$ as ma1 = 0.3783, $\mu$ as intercept = 919.2433, and $\sigma_\epsilon^2$ as sigma^2 = 23272.

```{r}
ma <- arima(Nile, order = c(0, 0, 1))
print(ma)

ts.plot(Nile)
ma.fit <- Nile - resid(ma)
points(ma.fit, type = "l", col = 2, lty = 2)

```

Use the ARIMA model to forecast observations with the `predict()` function. The MA model can only produce a 1-step forecast. Except for the 1-step forecast, all forecasts from the MA model are equal to the estimated mean (intercept).

Below is a forecast of 10 periods (years) beyond the 1871-1970 annual observations in the  Nile dataset. 

```{r}
predict(ma, n.ahead = 10)

ts.plot(Nile, xlim = c(1871, 1980))
ma.pred <- predict(ma, n.ahead = 10)$pred
ma.pred.se <- predict(ma, n.ahead = 10)$se
points(ma.pred, type = "l", col = 2)
points(ma.pred - 2*ma.pred.se, type = "l", col = 2, lty = 2)
points(ma.pred + 2*ma.pred.se, type = "l", col = 2, lty = 2)
```

How do you decide which model provides the best fit?  Measure the model fit with the Akaike information criterion (AIC) and/or Bayesian information criterion (BIC). These indicators penalize models with more estimated parameters to avoid overfitting, so smaller indicator values are preferable.

Use the `AIC()` and `BIC()` functions to estimate the indicators.  Below are the AIC and BIC for the AR(1) and MA models.  Although the predictions from both models are similar (they have a correlation coeffiicent of 0.94), both the AIC and BIC indicate that the AR model is a slightly better fit for the `Nile` data. 

```{r}
cor(ar1.fit, ma.fit)

AIC(ar1)
AIC(ma)

BIC(ar1)
BIC(ma)

```

## Dynamic Regression

Dynamic regression is like ordinary regression with explanatory variables, but the error term is now an ARIMA process instead of white noise.

Dataset `uschange` from the `fpp2` package contains growth rates of personal consumption and personal income in the US.  You might want to model consumption as a function of income.  Add parameter `xreq` to the `auto.arima()` function.  `xreg` is a matrix of predictor variables.

```{r}
fpp2::uschange %>% head()
# (uschange.arima <- auto.arima(uschange[, "Consumption"],
#                              xreg = uschange[, "Income"]))
```

```{r}
# # Forecast fit as fc
# uschange.fc <- forecast(uschange.arima, xreg = rep(10, 6))
# 
# # Plot fc with x and y labels
# autoplot(uschange.fc) + xlab("Month") + ylab("Sales")
```

```{r}
# Time plots of demand and temperatures
#autoplot(elec[, c("Demand", "Temperature")], facets = TRUE)

# Matrix of regressors
#xreg <- cbind(MaxTemp = elec[, "Temperature"], 
#              MaxTempSq = elec[, "Temperature"]^2, 
#              Workday = elec[, "Workday"])

# Fit model
#fit <- auto.arima(elec[, "Demand"], xreg = xreg)

# Forecast fit one day ahead
#forecast(fit, xreg = cbind(20, 20^2, 1))
```

### Dynamic Harmonic Regression

Dynamic harmonic regression is based on the principal that a combination of sine and cosine funtions can approximate any periodic function.  

$$y_t = \beta_0 + \sum_{k=1}^{K}[\alpha_k s_k(t) + \gamma_k c_k(t)] + \epsilon_t$$

where $s_k(t) = sin(\frac{2\pi k t}{m})$ and $c_k(t) = cos(\frac{2\pi k t}{m})$, $m$ is the seasonal period, $\alpha_k$ and $\gamma_k$ are regression coefficients, and $\epsilon_t$ is modeled as a non-seasonal ARIMA process.

The optimal model has the lowest *AICc*, so start with *K=1* and increase until the *AICc* is no longer decreasing.  *K* cannot be greater than $m/2$.

With weekly data, it is difficult to handle seasonality using ETS or ARIMA models as the seasonal length is too large (approximately 52). Instead, you can use harmonic regression which uses sines and cosines to model the seasonality. 

The fourier() function makes it easy to generate the required harmonics. The higher the order (*K*), the more "wiggly" the seasonal pattern is allowed to be. With *K=1*, it is a simple sine curve. You can select the value of *K* by minimizing the AICc value. Function `fourier()` takes in a required time series, required number of Fourier terms to generate, and optional number of rows it needs to forecast.

```{r}
# # Set up harmonic regressors of order 13
# harmonics <- fourier(gasoline, K = 13)
# 
# # Fit a dynamic regression model to fit. Set xreg equal to harmonics and seasonal to FALSE because seasonality is handled by the regressors.
# fit <- auto.arima(gasoline, xreg = harmonics, seasonal = FALSE)
# 
# # Forecasts next 3 years
# newharmonics <- fourier(gasoline, K = 13, h = 3*52)
# fc <- forecast(fit, xreg = newharmonics)
# 
# # Plot forecasts fc
# autoplot(fc)
```

Harmonic regressions are also useful when time series have multiple seasonal patterns. For example, taylor contains half-hourly electricity demand in England and Wales over a few months in the year 2000. The seasonal periods are 48 (daily seasonality) and 7 x 48 = 336 (weekly seasonality). There is not enough data to consider annual seasonality. 

```{r}
# # Fit a harmonic regression using order 10 for each type of seasonality
# fit <- tslm(taylor ~ fourier(taylor, K = c(10, 10)))
# 
# # Forecast 20 working days ahead
# fc <- forecast(fit, newdata = data.frame(fourier(taylor, K = c(10, 10), h = 20*48)))
# 
# # Plot the forecasts
# autoplot(fc)
# 
# # Check the residuals of fit
# checkresiduals(fit)
```

Another time series with multiple seasonal periods is calls, which contains 20 consecutive days of 5-minute call volume data for a large North American bank. There are 169 5-minute periods in a working day, and so the weekly seasonal frequency is 5 x 169 = 845. The weekly seasonality is relatively weak, so here you will just model daily seasonality. 

The residuals in this case still fail the white noise tests, but their autocorrelations are tiny, even though they are significant. This is because the series is so long. It is often unrealistic to have residuals that pass the tests for such long series. The effect of the remaining correlations on the forecasts will be negligible.

```{r}
# # Plot the calls data
# autoplot(calls)
# 
# # Set up the xreg matrix
# xreg <- fourier(calls, K = c(10, 0))
# 
# # Fit a dynamic regression model
# fit <- auto.arima(calls, xreg = xreg, seasonal = FALSE, stationary = TRUE)
# 
# # Check the residuals
# checkresiduals(fit)
# 
# # Plot forecasts for 10 working days ahead
# fc <- forecast(fit, xreg =  fourier(calls, c(10, 0), h = 169*8))
# autoplot(fc)
```

### TBATS Model

Thte TBATS model (Trigonometric terms for seasonality, Box-Cox transformations for hetergeneity, ARMA errors for short-term dynamics, Trend (possibly damped), and Seasonal (including multiple and non-integer periods)).

```{r}
# gasoline %>% tbats() %>% forecast() %>% autoplot()
```

TBATS is easy to use, but the prediction intervals are often too wide, and it can be quite slow with large time series.  TBATS returns output similar to this: TBATS(1, {0,0}, -, {<51.18,14>}), meaning 1=Box-Cox parameter, {0,0} = ARMA error, - = damping parameter, {<51.18,14>} = seasonal period and Fourier terms.


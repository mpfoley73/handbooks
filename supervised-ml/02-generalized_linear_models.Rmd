# Generalized Linear Models (GLM)

```{r include=FALSE}
library(tidyverse)
library(broom)  # for augment()
library(WVPlots)  # for GainCurvePlot()
library(caret)  # for RMSE()
library(foreign)
library(readr)
library(gtsummary)
library(nnet)
library(VGAM)
library(ordinal)
library(scales)
library(glue)

theme_set(
  jtools::theme_apa() +
    theme(
      plot.caption = element_text(hjust = 0)
    )
)
```

The linear regression model, $E(y|X) = X \beta$, structured as $y_i = X_i \beta + \epsilon_i$ where $X_i \beta = \mu_i$, assumes the response is a linear function of the predictors and the residuals are independent random variables normally distributed with zero mean and constant variance, $\epsilon \sim N \left(0, \sigma^2 \right)$. This implies that given a set of predictors, the response is normally distributed about its expected value, $y_i \sim N \left(\mu_i, \sigma^2 \right)$. However, there are many situations where this normality assumption does not hold. Generalized linear models (GLMs) are a generalization of the linear regression model that addresses non-normal response distributions.^[These notes are primarily from PSU's [Analysis of Discrete Data](https://online.stat.psu.edu/stat504) which uses Alan Agresti's **Categorical Data Analysis** [@Agresti2013]. I also reviewed PSU's [Regression Methods](https://newonlinecourses.science.psu.edu/stat501/lesson/15), DataCamp's [Generalized Linear Models in R](https://www.datacamp.com/courses/generalized-linear-models-in-r), DataCamp's [Multiple and Logistic Regression](https://www.datacamp.com/courses/multiple-and-logistic-regression), and **Interpretable machine learning** [@Molner2020].]

The response will not have a normal distribution if the underlying data-generating process is binomial (Section \@ref(binomiallogistic)) or multinomial (Section \@ref(multinomiallogistic)), ordinal (Section \@ref(ordinallogistic)), Poisson (counts, Section \@ref(poissonregression)), or exponential (time-to-event). In these situations the linear regression model can predict probabilities and proportions outside [0, 1], or negative counts or times. GLMs solve this problem by modeling a *function* of the expected value of $y$, $f(E(y|X)) = X \beta$. There are three components to a GLM: a *random component* specified by the response variable's probability distribution (normal, binomial, etc.); a *systematic component* in the form $X\beta$; and a *link function*, $\eta$, that specifies the link between the random and systematic components and converts the response to a range of $[-\infty, +\infty]$.  

Linear regression is a special case of GLM where the link function is the identity function, $f(E(y|X)) = E(y|X)$. For binary logistic regression, the link function is the logit,

$$f(E(y|X)) = \log \left( \frac{E(y|X)}{1 - E(y|X)} \right) = \log \left( \frac{\pi}{1 - \pi} \right) = \mathrm{logit}(\pi)$$

where $\pi$ is the response probability.^[The related probit regression link function is $f(E(Y|X)) = \Phi^{-1}(E(Y|X)) = \Phi^{-1}(\pi)$. The difference between the logistic and probit link function is theoretical, and [the practical significance is slight](https://www.theanalysisfactor.com/the-difference-between-logistic-and-probit-regression/). You can safely ignore probit.] For multinomial logistic regression, the link function is the logit again, but with respect to the baseline level, and there is set of logits (one for each non-baseline level),

$$f(E(y|X)) = \log \left( \frac{E(y_j|X)}{E(y_{j^*}|X)} \right) = \log \left( \frac{\pi_j}{\pi_{j^*}} \right) = \mathrm{logit}(\pi_j)$$

Where $j$ is a level in the dependent variable and $j^*$ is the baseline level. For Poisson regression, the link function is

$$f(E(y|X)) = \ln (E(y|X)) = \ln(\lambda)$$

where $\lambda$ is the expected event rate. For exponential regression, the link function is 

$$f(E(y|X) = -E(y|X) = -\lambda$$

where $\lambda$ is the expected time to event.

GLM uses maximum likelihood estimation (MLE) rather than ordinary least squares (OLS) to estimate the parameters, and thus relies on large-sample approximations. 

Fit a GLM just like an linear model, but with the `glm()` function, specifying the distribution with the `family = c("gaussian", "binomial", "poisson")` parameter. Fit a mulinomial logistic regression model with `nnet::multinom()`.

## Binomial Logistic Regression {#binomiallogistic}

Logistic regression estimates the probability that a categorical dependent variable is a particular level. The dependent variable levels can be binomial, multinomial, or ordinal. The *binary* logistic regression model is

$$y_i = \mathrm{logit}(\pi_i) = \log \left( \frac{\pi_i}{1 - \pi_i} \right) = X_i \beta$$

where $\pi_i$ is the response $i$'s binary level probability. The model predicts the *log odds* of the level. Why do this? The range of outcomes need to be between 0 and 1, and a sigmoid function, $f(y) = 1 / \left(1 + e^y \right)$, does that. If the _log odds_ of the level equals $X_i\beta$, then the _odds_ of the level equals $e^{X\beta}$. You can solve for $\pi_i$ to get $\pi = \mathrm{odds} / (\mathrm{odds} + 1)$. Substituting, 

$$\pi_i = \frac{\exp(y_i)}{1 + \exp(y_i)} = \frac{e^{X_i\beta}}{1 + e^{X_i\beta}}$$

which you can simplify to $\pi_i = 1 / (1 + e^{-X_i\beta})$, a sigmoid function. The upshot is $X\beta$ is the functional relationship between the independent variables and _a function of the response_, not the response itself.

The model parameters are estimated either by _iteratively reweighted least squares optimization_ or by _maximum likelihood estimation_ (MLE). MLE maximizes the probability produced by a set of parameters $\beta$ given a data set and probability distribution.^[Notes from [Machine Learning Mastery](https://machinelearningmastery.com/logistic-regression-with-maximum-likelihood-estimation/)]. In logistic regression the probability distribution is the binomial and each observation is the outcome of a single Bernoulli trial. 

$$L(\beta; y, X) = \prod_{i=1}^n \pi_i^{y_i}(1 - \pi_i)^{(1-y_i)} = \prod_{i=1}^n\frac{\exp(y_i X_i \beta)}{1 + \exp(X_i \beta)}.$$

In practice, multiplying many small probabilities can be unstable, so MLE optimizes the log likelihood instead. 

$$\begin{align}
l(\beta; y, X) &= \sum_{i = 1}^n \left(y_i \log(\pi_i) + (1 - y_i) \log(1 - \pi_i) \right) \\
               &= \sum_{i = 1}^n \left(y_i X_i \beta - \log(1 + e^{X_i\beta}) \right)
\end{align}$$

Sometimes you will see the _cost function_ optimized. The cost function is the negative of of the log likelihood function.

**Assumptions**

The binomial logistic regression model requires a dichotomous dependent variable and independent of observations. There should be at least 15 observations per independent variable (some sources recommend at least 50). There are also three conditions related to the data distribution: i) the logit transformation is linearly related to any continuous independent variables, ii) there is no multicollinearity, and iii) there are no significant outliers, leverage points, or influential points.

### Case Study 1 {.unnumbered #cs1}

This case study uses the [data set](https://statistics.laerd.com/premium/spss/spss-files/logistic-regression.sav) from Laerd Statistics for binomial logistic regression.

```{r}
cs1 <- list()

cs1$dat <- read.spss("./input/logistic-regression.sav", to.data.frame = TRUE)
```

An analysis investigates the relationship between the incidence of heart disease (Y|N) and age, weight, gender, and maximal aerobic capacity (maxVO2) using data from $n$ = `r nrow(cs1$dat)` participants.

```{r}
cs1$dat %>%
  tbl_summary(
    by = heart_disease,
    include = -caseno,
    percent = "row",
    statistic = list(all_continuous() ~ "{mean}, {sd}")
  )
```

<br>

#### Assumptions

Four assumptions relate to the study design: (1) the dependent variable is dichotomous; (2) the observations are independent; (3) the categories of all nominal variables are mutually exclusive and exhaustive; and (4) there is at least 15 cases per independent variable. These assumptions are all valid. Three more assumptions related to the data distribution: 

- linear relationship between the logit transformation and the continuous independent variables. Test with a plot and with Box-Tidwell.

- No multicollinearity between or among independent varaibles. Test with correlation coefficients and tolerance/VIF values.

- No significant outliers, leverage points, or influential points. 

Test the linearity assumption first. There are two ways to do this (do both). First, fit your model, then plot the fitted values against the continuous predictors. This is like plotting the the dependent variable against the continuous variable in the OLS setting, but here the relationship in question is the _logit_ transformation of the response variable. That's what fitting the model gets you. This plot looks pretty good.

```{r}
cs1$fmla <- formula(heart_disease ~ age + weight + VO2max + gender)
cs1$dat %>%
  glm(cs1$fmla, data = ., family = "binomial") %>%
  augment() %>%
  pivot_longer(c(age, weight, VO2max)) %>%
  ggplot(aes(x = value, y = .fitted)) +
  geom_point() +
  facet_wrap(facets = vars(name), scales = "free_x") +
  geom_smooth(method = "lm", formula = "y~x") +
  labs(
    title = "Test of the linearity: predictive valus vs continuous predictors",
    subtitle = glue("Model: {as.expression(cs1$fmla)}."),
    x = NULL
  )
```

The second way to test for linearity is the Box-Tidwell approach. Add transformations of the continuous independent variables to the model then test their significance level in the fit. The transformation is the product of the log of the variable and itself. E.g. `age_tx = log(age) * age`, etc.

```{r}
cs1$boxtidwell <- cs1$dat %>%
  mutate(
    age_tx = log(age) * age,
    weight_tx = log(weight) * weight,
    VO2max_tx = log(VO2max) * VO2max
  ) %>%
  glm(heart_disease ~ age + weight + VO2max + gender + age_tx + weight_tx + VO2max_tx, 
      data = ., family = "binomial")

summary(cs1$boxtidwell)
```

Focus on the three transformed variables. `age_tx` is the only one with a *p*-value <.05, but it is customary to apply a Bonferroni adjustment when testing for linearity. There are eight predictors in the model, so the adjusted *p*-value is the reported *p*-value multiplied by eight. The *p*-value for `age_tx` is `r comma(summary(cs1$boxtidwell)$coefficients["age_tx","Pr(>|z|)"] * 8, .001)`. So you can say that, the linearity of the continuous variables with respect to the logit of the dependent variable was assessed via the Box-Tidwell (1962) procedure. A Bonferroni correction was applied using all eight terms in the model resulting in statistical significance being accepted when p < .00625 (Tabachnick & Fidell, 2014). Based on this assessment, all continuous independent variables were found to be linearly related to the logit of the dependent variable.

Had this assumption failed, you could try transforming the variable using the Box-Tidwell power transformation. Raise the variable to $\lambda = 1 + b / \gamma$ where $b$ is the estimated coefficient of the model without the interaction terms, and $\gamma$ is the estimated coefficient of the interaction term of the model with interactions. In the case of `age`, $\gamma$ is `r comma(coef(cs1$boxtidwell)["age_tx"], .001)`. $b$, estimated in the diagnostic plot exercise, is `r comma(coef(cs1$fit)["age"], .001)`. That means $\lambda$ = 1 + `r comma(coef(cs1$fit)["age"], .001)` / `r comma(coef(cs1$boxtidwell)["age_tx"], .001)` = `r comma(1 + coef(cs1$fit)["age"] / coef(cs1$boxtidwell)["age_tx"], .001)`. This is approximately 1, so no transformation. It seems customary to apply general transformations like .5 (square root), 1/3 (cube root), ln, and reciprocal.

Now check for outliers using case diagnostics.

```{r}
augment(cs1$fit, type.predict = "response") %>%
  filter(abs(.std.resid) >= 2)
```

These two fitted values missed the mark by fairly wide margin, but only one had a standardized residual substantially more than 2. You might examine the observation for validity. Otherwise, proceed and explain that There was one standardized residual with a value of `r augment(cs1$fit, type.predict = "response") %>% filter(abs(.std.resid) >= 2.1) %>% pull(.std.resid) %>% comma(.01)` standard deviations, which was kept in the analysis.

https://towardsdatascience.com/assumptions-of-logistic-regression-clearly-explained-44d85a22b290

#### Evaluation

There are several ways to evaluate the model. One is the pseudo-R-squared measures. A second is accuracy and ROC.



```{r}
DescTools::PseudoR2(cs2$fit, which = c("CoxSnell", "Nagelkerke", "McFadden"))

glance(cs1$fit)
```


#### Reporting

A binomial logistic regression was performed to ascertain the effects of age, weight, gender and VO2max on the likelihood that participants have heart disease. Linearity of the continuous variables with respect to the logit of the dependent variable was assessed via the Box-Tidwell (1962) procedure. A Bonferroni correction was applied using all eight terms in the model resulting in statistical significance being accepted when *p* < `r comma(.05/8, .00001)` (Tabachnick & Fidell, 2014). Based on this assessment, all continuous independent variables were found to be linearly related to the logit of the dependent variable. There was one standardized residual with a value of `r augment(cs1$fit, type.predict = "response") %>% filter(abs(.std.resid) >= 2.1) %>% pull(.std.resid) %>% comma(.01)` standard deviations, which was kept in the analysis. The logistic regression model was statistically significant, χ2(4) = 27.40, p < .001. The model explained 33.0% (Nagelkerke R2) of the variance in heart disease and correctly classified 71.0% of cases. Sensitivity was 45.7%, specificity was 84.6%, positive predictive value was 61.5% and negative predictive value was 74.3%. Of the five predictor variables only three were statistically significant: age, gender and VO2max (as shown in Table 1). Males had 7.02 times higher odds to exhibit heart disease than females. Increasing age was associated with an increased likelihood of exhibiting heart disease, but increasing VO2max was associated with a reduction in the likelihood of exhibiting heart disease.

```{r}
gtsummary::tbl_regression(
  cs1$fit,
  exponentiate = TRUE
)
```


<br>

CWWS score increased from the sedentary (`r inline_text(cs3$gt, variable = coping_stress, level = "Mean, SD", column = "Sedentary", pattern = "*M* = {mean}, _SD_ = {sd}")`), to low (`r inline_text(cs3$gt, variable = coping_stress, level = "Mean, SD", column = "Low", pattern = "*M* = {mean}, _SD_ = {sd}")`), to moderate (`r inline_text(cs3$gt, variable = coping_stress, level = "Mean, SD", column = "Moderate", pattern = "*M* = {mean}, _SD_ = {sd}")`) to high (`r inline_text(cs3$gt, variable = coping_stress, level = "Mean, SD", column = "High", pattern = "*M* = {mean}, _SD_ = {sd}")`) physical activity groups, in that order. 

### Case Study

Dataset `donner` contains _n_ = 45 observations of the survival status (`surv`) of the members of the Donner party with explanatory variables `age` and `sex`.

```{r include=FALSE}
donner <- tribble(
  ~age, ~sex, ~surv,
  23, 1, 0,
  40, 0, 1,
  40, 1, 1,
  30, 1, 0,
  28, 1, 0,
  40, 1, 0,
  45, 0, 0,
  62, 1, 0,
  65, 1, 0,
  45, 0, 0,
  25, 0, 0,
  28, 1, 1,
  28, 1, 0,
  23, 1, 0,
  22, 0, 1,
  23, 0, 1,
  28, 1, 1,
  15, 0, 1,
  47, 0, 0,
  57, 1, 0,
  20, 0, 1,
  18, 1, 1,
  25, 1, 0,
  60, 1, 0,
  25, 1, 1,
  20, 1, 1,
  32, 1, 1,
  32, 0, 1,
  24, 0, 1,
  30, 1, 1,
  15, 1, 0,
  50, 0, 0,
  21, 0, 1,
  25, 1, 0,
  46, 1, 1,
  32, 0, 1,
  30, 1, 0,
  25, 1, 0,
  25, 1, 0,
  25, 1, 0,
  30, 1, 0,
  35, 1, 0,
  23, 1, 1,
  24, 1, 0,
  25, 0, 1,
) %>%
  mutate(
    surv = factor(surv, levels = c(0:1), labels = c("Died", "Lived")),
    sex = factor(sex, levels = c(0:1), labels = c("F", "M"))
  )
```

```{r collapse=TRUE}
glimpse(donner)
```

```{r, fig.height=3.5, echo=FALSE}
donner %>%  
  ggplot(aes(x = age, y = as.integer(surv)-1, color = sex)) +
  geom_jitter(height = 0, alpha = 0.6) +
  geom_smooth(aes(y = as.integer(surv) - 1),
              method = "glm", method.args = list(family = "binomial"), formula = "y ~ x") +
  scale_y_continuous(breaks = c(0,1), labels = c("Died", "Lived")) +
  labs(title = "Survival odds varied by age and gender.", 
       subtitle = glue::glue("{nrow(donner)} outcomes from Donner party."),
       color = NULL, y = NULL) +
  theme_light() +
  theme(legend.position = "right")
```

### Fit the Model

Fit a logistic regression $\mathrm{surv} = \mathrm{sex} + \mathrm{age} + \mathrm{sex:age}$. The reference case based on the factor definition is `sex` = "F", and `age` = `r median(donner$age)` (the median age).

```{r}
donner_centered <- donner %>%
  mutate(age = age - median(donner$age))
m <- glm(surv ~ sex*age, data = donner_centered, family = "binomial")
summary(m)
```

The "z value" is the Wald $z$ statistic, $z = \hat{\beta} / SE(\hat{\beta})$. Its square, $z^2$, is the Wald chi-squared statistic. The _p_-value is the area to the right of $z^2$ in the $\chi_1^2$ density curve:

```{r}
m %>% tidy() %>% 
  mutate(
    z = estimate / std.error, 
    p_z2 = pchisq(z^2, df = 1, lower.tail = FALSE)
  ) %>%
  select(term, z, p_z2)
```

Below the Coefficients table, the "dispersion parameter" refers to overdispersion, a common issue with GLM. For a logistic regression, the response variable should be distributed $y_i \sim \mathrm{Bin}(n_i, \pi_i)$ with $\mu_i = n_i \pi_i$ and $\sigma^2 = \pi (1 - \pi)$.  Overdispersion means the data shows evidence of variance greater than $\sigma^2$.

"Fisher scoring" is a method for ML estimation. Logistic regression uses interatively reweighted least squares to fit the model, so this section indicates whether the algorithm converged.

The null deviance is the likelihood ratio $G^2$ = `r scales::number(summary(m)$null.deviance, accuracy = .001)` of the intercept-only model. The residual deviance is the likelihood ratio $G^2$ = `r scales::number(summary(m)$deviance, accuracy = .001)` after including the model covariates. $G^2$ is large, so reject the null hypothesis of no age or sex effects. An ANOVA table shows the change in deviance from successively adding each variable to the model.

```{r}
anova(m)
```

### Interpret Results

Plug in values to interpret the model. The log odds of a 24 year-old female surviving is $\hat{y}$ = `r predict(m, newdata = data.frame(sex = "F", age = 24 - median(donner$age))) %>% scales::number(accuracy = .01)`.  The log odds of a 24 year-old male surviving is $\hat{y}$ = `r predict(m, newdata = data.frame(sex = "M", age = 24 - median(donner$age))) %>% scales::number(accuracy = .01)`.

```{r collapse=TRUE}
new_dat <- expand.grid(sex = factor(c("F", "M")), age = 24 - median(donner$age))
(preds <- predict(m, newdata = new_dat))
```

Exponentiate to get the **odds**.

$$\mathrm{odds}(\hat{y}) = \exp (\hat{y}) = \frac{\pi}{1 - \pi}.$$

```{r collapse=TRUE}
exp(preds)
```

Solve for $\pi$ to get the **probability**.  

$$\pi = \frac{\exp (\hat{y})}{1 + \exp (\hat{y})}$$

You can do the math, or use `predict(type = "response")`.

```{r collapse=TRUE}
(preds <- predict(m, newdata = new_dat, type = "response"))
```

Interpret the coefficient estimates as the change in the log odds of $y$ due to a $\delta = x_1 - x_0$ unit change in $x$.

$$\log \left(\pi / (1 - \pi) |_{X = X_1} \right) - \log \left(\pi / (1 - \pi) |_{X = X_0} \right) = \log \left( \frac{\pi / (1 - \pi) |_{X = X_1}}{\pi / (1 - \pi) |_{X = X_0}} \right) = \delta \hat{\beta}$$

Better yet, interpret the exponential of the coefficient estimates as the change in the _odds_ of $y$ due to a $\delta$ unit change in $x$.

$$\mathrm{odds}(y) = e^{\delta \hat{\beta}}$$

The reference case is `sex` = "F" and `age` = 0 (centered on median age, `r median(donner$age)`). 

```{r include=FALSE}
odds_f_28 <- exp(coef(m)["(Intercept)"])
odds_f_24 <- exp(coef(m)["(Intercept)"] + coef(m)["age"] * (-4))
odds_m_28 <- exp(coef(m)["(Intercept)"] + coef(m)["sexM"])
odds_m_24 <- exp(coef(m)["(Intercept)"] + coef(m)["sexM"] + coef(m)["age"] * (-4) + coef(m)["sexM:age"]*(-4))

prob_f_28 <- odds_f_28 / (1 + odds_f_28)
prob_f_24 <- odds_f_24 / (1 + odds_f_24)
prob_m_28 <- odds_m_28 / (1 + odds_m_28)
prob_m_24 <- odds_m_24 / (1 + odds_m_24)

odds_f_28_fmt <- scales::number(odds_f_28, accuracy = .1)
odds_f_24_fmt <- scales::number(odds_f_24, accuracy = .1)
odds_m_28_fmt <- scales::number(odds_m_28, accuracy = .1)
odds_m_24_fmt <- scales::number(odds_m_24, accuracy = .1)
```

* The survival odds of a `r median(donner$age)` year old female are $\hat{y} = e^{(Intercept)}$ = `r odds_f_28_fmt` to 1. The expected probability of surviving is `r glue::glue("{odds_f_28_fmt} / (1 + {odds_f_28_fmt}) = {scales::number(prob_f_28 * 100, accuracy = 1)}")`%.
* The survival odds of a `r median(donner$age) - 4` year old female are $\hat{y} = e^{(Intercept) -4 \cdot \mathrm{age}}$ = `r odds_f_24_fmt` to 1. The expected probability of surviving is `r glue::glue("{odds_f_24_fmt} / (1 + {odds_f_24_fmt}) = {scales::number(prob_f_24 * 100, accuracy = 1)}")`%.
* The survival odds of a `r median(donner$age)` year old male are $\hat{y} = e^{(Intercept) + sexM}$ = `r odds_m_24_fmt`. The expected probability of surviving is `r glue::glue("{odds_m_24_fmt} / (1 + {odds_m_24_fmt}) = {scales::number(prob_m_28 * 100, accuracy = 1)}")`%.
* The survival odds of a `r median(donner$age) - 4` year old male are $\hat{y} = e^{(Intercept) -4 \cdot \mathrm{age} -4 \cdot \mathrm{sexM} \cdot \mathrm{sexM:age}}$ = `r odds_m_24_fmt`. The expected probability of surviving is `r glue::glue("{odds_m_24_fmt} / (1 + {odds_m_24_fmt}) = {scales::number(prob_m_24 * 100, accuracy = 1)}")`%.
* A female's survival odds are multiplied by a factor of $e^{(1 \cdot \mathrm{age})}$ = `r scales::number(exp(coef(m)["age"]), accuracy = .001)` per additional year of age (i.e. the odds _fall_ by `r scales::number((1 - exp(coef(m)["age"]))*100, accuracy = .1)`%).
* A male's survival odds are multiplied by a factor of $e^{1 \cdot \mathrm{age}} e^{1 \cdot \mathrm{sexM:age}}$ = `r scales::number(exp(coef(m)["age"]) * exp(coef(m)["sexM:age"]), accuracy = .001)` per additional year of age (i.e. the odds _fall_ by `r scales::number((1 - exp(coef(m)["age"]) * exp(coef(m)["sexM:age"]))*100, accuracy = .1)`%).
* A `r median(donner$age)` year old female's survival odds are multiplied by a factor of $e^{1 \cdot \mathrm{sex}}$ = `r scales::number(exp(coef(m)["sexM"]), accuracy = .001)` if she were instead a male. (i.e. the odds _fall_ by `r scales::number((1 - exp(coef(m)["sexM"]))*100, accuracy = .1)`%)

`oddsratio::or_glm()` calculates the odds ratio from an increment in the predictor values.

```{r}
oddsratio::or_glm(donner_centered, m, incr = list(age = 1))
```

Expressing the predicted values as probabilities produces the familiar signmoidal shape of the binary relationship.

```{r fig.height=3.5}
augment(m, type.predict = "response") %>%
  ggplot(aes(x = age, color = sex)) +
  geom_point(aes(y = as.integer(surv))) +
  geom_line(aes(y = .fitted+1)) +
  labs(x = "Age (relative to median)",
       y = NULL,
       title = "Binary Fitted Line Plot") +
  scale_y_continuous(breaks = c(1,2), labels = c("Died", "Lived")) +
  theme_light() +
  theme(legend.position = "right")
```

### Assess Model Fit

Evaluate a logistic regression using a [Gain curve or ROC curve](https://community.tibco.com/wiki/gains-vs-roc-curves-do-you-understand-difference). 

#### Gain Curve {-}

In the **gain curve**, the x-axis is the fraction of items seen when sorted by the predicted value, and the y-axis is the cumulatively summed true outcome. The "wizard" curve is the gain curve when the data is sorted by the true outcome.  If the model's gain curve is close to the wizard curve, then the model predicts the response well. The gray area is the "gain" over a random prediction.

`r sum(donner$surv == "Lived")` of the `r nrow(donner)` members of the Donner party survived. 

* The gain curve encountered 10 survivors (50%) within the first 12 observations (27%).  It encountered all 20 survivors on the 37th observation.
* The bottom of the grey area is the outcome of a random model.  Only half the survivors would be observed within 50% of the observations.  
* The top of the grey area is the outcome of the perfect model, the "wizard curve".  Half the survivors would be observed in 10/45=22% of the observations.

```{r, fig.height=3.5}
augment(m, type.predict = "response") %>%
  # event_level = "second" sets the second level as success
  yardstick::gain_curve(surv, .fitted, event_level = "second") %>%
  autoplot() +
  labs(title = "Gain Curve")
```

#### ROC Curve {-}

The ROC (Receiver Operating Characteristics) curve plots sensitivity vs specificity at different cut-off values for the probability, ranging cut-off from 0 to 1.

```{r, fig.height=3.5}
augment(m, type.predict = "response") %>%
  # event_level = "second" sets the second level as success
  yardstick::roc_curve(surv, .fitted, event_level = "second") %>%
  autoplot() +
  labs(title = "ROC Curve")
```


## Multinomial Logistic Regression {#multinomiallogistic}

The _multinomial_ logistic regression model is $J - 1$ baseline logits,

$$y_i = \log \left( \frac{\pi_{ij}}{\pi_{ij^*}} \right) = X_i \beta_j, \hspace{5mm} j \ne j^*$$

where $j$ is a level of the multinomial response variable. Whereas the binomial logistic regression equation models the *log odds* of the response level, the multinomial logistic regression models the *log relative risk*, the probability relative to the baseline $j^*$ level (as opposed to $\pi_{ij} / (1 - \pi_{ij}$).^[These notes rely on the course notes from [PSU STAT 504, Analysis of Discrete Data](https://online.stat.psu.edu/stat504/) and [UCLA](https://stats.oarc.ucla.edu/r/dae/multinomial-logistic-regression/).]

As with binary logistic regression, interpret the $k^{th}$ element of $\beta_j$ as the increase in log odds (relative risk) of $Y_i = j$ relative to $Y_i = j^*$ given a one-unit increase in the $k^{th}$ element of $X$, holding the other terms constant.

The individual probabilities, $\pi_{ij}$, are

$$\pi_{ij} = \frac{\exp(y_{ij})}{1 + \sum_{j \ne j^*} \exp(y_{ij})} = \frac{e^{X_i\beta_j}}{1 + \sum_{j \ne j^*} e^{X_i\beta_j}}$$

and for the baseline category, 

$$\pi_{ij^*} = \frac{1}{1 + \sum_{j \ne j^*} \exp(y_{ij})} = \frac{1}{1 + \sum_{j \ne j^*} e^{X_i\beta_j}}$$

**Assumptions**

Multinomial logistic regression applies when the dependent variable is categorical. It presumes a linear relationship between the log odds of the dependent variable and $X$ with residuals $\epsilon$ that are independent. It also assumes there is no severe multicollinearity in the predictors, and there is independence of irrelevant alternatives (IIA). IIA means the relative likelihood of being in one category compared to the base category would not change if you added other categories. 

### Case Study

This case study uses data from [this tutorial at UCLA](https://stats.oarc.ucla.edu/r/dae/multinomial-logistic-regression/).

```{r eval=FALSE}
download.file(
  "https://stats.idre.ucla.edu/stat/data/hsbdemo.dta",
  "./input/hsbdemo.dta",
  mode = "wb"
)
```

```{r}
cs2 <- list()
cs2$dat <- foreign::read.dta("./input/hsbdemo.dta") %>%
  mutate(prog = fct_relevel(prog, "academic"))
```

A study measures the association between a student's academic program (academic, general, and vocational) and their socioeconomic status (SES) (low, middle, high) and writing score.

```{r}
cs2$dat %>%
  tbl_summary(
    by = prog,
    include = c(prog, ses, write),
    statistic = list(all_continuous() ~ "{mean}, {sd}")
  ) %>%
  gtsummary::add_overall()
```

<br>

```{r}
cs2$dat %>%
  mutate(write_bin = cut(write, breaks = 5, dig.lab = 1, right = FALSE)) %>%
  count(prog, ses, write_bin) %>%
  group_by(ses, write_bin) %>%
  mutate(prob = n / sum(n)) %>%
  ungroup() %>%
  ggplot(aes(x = write_bin, y = prob, color = ses)) +
  geom_point() +
  geom_line(aes(group = ses)) +
  facet_wrap(facets = vars(prog)) +
  theme(legend.position = "top") +
  labs(title = "Program Proportions by SES")
```

Fit the multinomial logistic regression with `nnet::multinom()`, `mlogit::mlogit()`,  or `VGAM::vglm()`. `nnet::multinom()` seems to be most popular.

```{r results="hide"}
# model = TRUE saves data frame in model. Used to fit null model for PseudoR2.
cs2$fit <- multinom(prog ~ ses + write, model = TRUE, data = cs2$dat)
# cs2$fit2 <- VGAM::vglm(prog ~ ses + write, data = cs2$dat, family = "multinomial")
```

```{r}
summary(cs2$fit)
```

Interpret the model as a one unit increase in the writing score is associated with a `r comma(abs(coef(cs2$fit)["general", "write"]), .001)` decrease in the log odds of being in a general vs academic program. The log odds of being in a general vs academic program is `r comma(abs(coef(cs2$fit)["general", "seshigh"]), .001)` lower for high SES than low SES. The exponential of the coefficients is the relative risk (ratio of probabilities) relative to the baseline.

```{r}
exp(coef(cs2$fit))
```

A one unit increase in the writing score multiplies the odds of being in a general vs academic program by `r comma(exp(coef(cs2$fit)["general", "write"]), .001)` (i.e., reduces the odds by `r percent(1-exp(coef(cs2$fit)["general", "write"]), .1)`). The odds of being in a general vs academic program for high SES is `r comma(exp(coef(cs2$fit)["general", "seshigh"]), .001)` the odds for low SES.

```{r}
newdat <- expand.grid(
  ses = cs2$dat$ses,
  write = seq(from = round(min(cs2$dat$write)), to = round(max(cs2$dat$write)), by = 1)
)

bind_cols(
  newdat,
  predict(cs2$fit, newdata = newdat, type = "probs")
) %>%
  pivot_longer(cols = -c(ses, write), names_to = "prog", values_to = "probability") %>%
  ggplot(aes(x = write, y = probability, color = ses)) +
  geom_line() + 
  facet_wrap(facets = vars(prog))
```

Evaluate the model by running an ANOVA on the model and an intercept-only model.

```{r}
cs2$fit_io <- multinom(prog ~ 1, data = cs2$dat)

(cs2$anova <- anova(cs2$fit_io, cs2$fit))
```

The log-likelihood measures the unexplained variability in the model. The decrease in the log-likelihood from the intercept only model, `r comma(cs2$anova$"Resid. Dev"[1], .1)`, to the full model `r comma(cs2$anova$"Resid. Dev"[2], .1)` is `r comma(cs2$anova$"LR stat."[2], .1)`. This statistics is distributed $\chi^2_{6}$. The *p*-value is <.0001, meaning the explanatory values is signficantly different from zero. The residual deviance is $G^2 = 2 \sum_{ij}y_{ij} \log \frac{y_{ij}}{\hat{y}_{ij}}$ on $(N - p)(r - 1)$ degrees of freedom where $p \in [1, 4]$ are the number of estimated coefficients in the two models, including the intercept, and $r = 3$ is the number of levels in the dependent variable. I manually calculated $G^2$ below. $y_{ij}$ takes values of 0 or 1, and $\log 0 = +\infty$, so I filtered those values out. If the model perfectly predicted the dependent variable, the ratio $\frac{y_{ij}}{\hat{y}_{ij}}$ would always equal 1 and $G^2$ would equal 0.

```{r collapse=TRUE}
tmp <- tibble(
  y = as.vector(cs2$fit$residuals + cs2$fit$fitted.values),
  y_hat = as.vector(cs2$fit$fitted.values)
) %>%
  filter(y != 0)
(g2 <- 2 * sum(tmp$y * log(tmp$y / tmp$y_hat)))
# But you can also get it directly from the model this way
(g2 <- deviance(cs2$fit))
# or this way.
(g2 <- cs2$fit$deviance)
```

Test the goodness of fit with Pearson's Chi-square.

```{r}
(cs2$chisq_test <- chisq.test(cs2$dat$prog, predict(cs2$fit)))
```

Logistic regression does not a direct R-squared statistic like OLS does (the proportion of variance explained by the model). However, there are some analogs, called pseudo R-squared. You'll encounter three pseudo R-squared measures: Cox and Snell, Nagelkerke, and McFadden.

```{r}
DescTools::PseudoR2(cs2$fit, which = c("CoxSnell", "Nagelkerke", "McFadden"))
```

The likelihood ratio test measures the significance of predictors.

```{r}
(cs2$lrtest <- lmtest::lrtest(cs2$fit, "ses"))
```

SES had significant main effects on program selection, $\chi^2(4)$ = `r comma(cs2$lrtest$Chisq[2], .1)`, *p* = `r comma(cs2$lrtest$"Pr(>Chisq)"[2], .001)`.

To get the *p*-values associated with the individual coefficient estimates, just use `tidy()`.

```{r}
(cs2$tidy <- broom::tidy(cs2$fit))
```

```{r}
cs2$dat %>%
  bind_cols(.fitted = cs2$fit$fitted.values) %>%
  yardstick::gain_curve(prog, .fitted) %>%
  autoplot() +
  labs(title = "Gain Curve")
```

```{r}
cs2$dat %>%
  bind_cols(.fitted = cs2$fit$fitted.values) %>%
  yardstick::roc_curve(prog, .fitted) %>%
  autoplot() +
  labs(title = "ROC Curve")
```

```{r}
caret::confusionMatrix(
  predict(cs2$fit),
  reference = cs2$dat$prog
)
```

## Ordinal Logistic Regression {#ordinallogistic}

Ordinal logistic regression, also call cumulative link model (CLM), is a generalized linear model (GZLM), an extension of the general linear model (GLM) to non-continuous outcome variables. There are many approaches to ordinal logistic regression, including cumulative, adjacent, and continuation categories, but the most popular is the **cumulative odds ordinal logistic regression with proportional odds**. These notes rely on [UVA](https://data.library.virginia.edu/fitting-and-interpreting-a-proportional-odds-model/), [PSU](https://online.stat.psu.edu/stat504/),  [Laerd](https://statistics.laerd.com/spss-tutorials/ordinal-regression-using-spss-statistics.php), and the CLM package [article vignette](https://cran.r-project.org/web/packages/ordinal/vignettes/clm_article.pdf).

The model for ordinal response random variable $Y_i$ with $J$ levels is

$$\gamma_{ij} = F(\eta_{ij}), \hspace{5 mm} \eta_{ij} = \theta_j - x_i^\mathrm{T}\beta, \hspace{5 mm} i = 1, \ldots, n, \hspace{5 mm} j = 1, \ldots, J-1$$

where $\gamma_{ij} = P(Y_i \le j) = \pi_{i1} + \cdots + \pi_{ij}$. $\eta_{ij}$ is a linear predictor with $J-1$ intercepts. $F$ is the inverse link function. The regression models the logit link function of $\gamma_{ij}$.

$$\mathrm{logit}(\gamma_{ij}) = \log \left[\frac{P(Y_i \le j)}{P(Y_i \gt j)} \right] = \theta_j - x_i^\mathrm{T}\beta$$
The cumulative logit is the log-odds of the cumulative probabilities that the response is in category $\le j$ versus $\gt j$. $\theta_j$ is the log-odds when $x_i^\mathrm{T}=0$ and $\beta$ is the increase in the log odds attributed to a one unit increase in $x_i^\mathrm{T}$. Notice $\beta$ is the same for all $j$. 

Once you fit the model, you will either generate predicted values or evaluate the coefficient estimators. The predicted value is a log-odds by default (useless), so you will at least take exponential to get the odds. From there you can solve for the probability, 

$$P(Y_i \gt j) = \frac{\mathrm{exp}(\hat{y}_i)}{1 + \mathrm{exp}(\hat{y}_i)}.$$

The exponential of $\beta$ is the odds ratio of $x_1^\mathrm{T} - x_0^\mathrm{T}$. You can solve for the odds ratio 

$$\mathrm{OR} = \frac{\mathrm{exp}(\theta_j - x_1^\mathrm{T}\beta)}{\mathrm{exp}(\theta_j - x_2^\mathrm{T}\beta)} = \mathrm{exp}(\beta(x_1^\mathrm{T} - x_0^\mathrm{T}))$$

If $x$ is a binary factor factor, then $exp(\beta)$ is the odds ratio of $x=1$ vs $x=0$. Thus the odds-ratio is *proportional* to the difference between values of $x$ and $\beta$ is the constant of proportionality.

The model is estimated with a regularized Newton-Raphson algorithm with step-halving (line search) using analytic expressions for the gradient and Hessian of the negative log-likelihood function. What this means is beyond me right now, but the upshot is that the estimation is an iterative maximization exercise, not a formulaic matrix algebra process. It is possible for the model estimation to fail to converge on a maximum.

You will sometimes encounter discussion about the *latent variable*. That is just the underlying quality you are trying to measure. If you rate something a 4 on a 5-level likert scale, 4 is the expression of your valuation, the latent variable. Your precise valuation is somewhere between 3 and 5 on a continuous scale. The link function defines the distribution of the latent variable.

There are variations on the ordinal model. Structured thresholds impose restrictions on $\theta_j$, for example requiring equal distances between levels. Partial proportional odds allow $\theta_j$ to vary with nominal predictors. You can also use link functions other than logit.

There are two conditions to ordinal logistic regression: (a) no multicollinearity, and (b) proportional odds.

## Case Study 3

```{r include=FALSE}
tax <- list()

tax$dat <- foreign::read.spss("./input/ordinal-logistic-regression.sav", to.data.frame = TRUE) %>%
  mutate(tax_too_high = factor(tax_too_high, ordered = TRUE),
         biz_owner = fct_relevel(biz_owner, "No", "Yes"),
         politics = fct_relevel(politics, "Lab")) %>%
  select(-c(biz_friends, uni_educated))
tax$n <- nrow(tax$dat)
```

`r tax$n` participants in a study respond to the statement "Taxes are too high" on a 4-level likert scale (`tax_too_high`, *Strongly Disagree*, *Disagree*, *Agree*, *Strongly Agree*). Participant attributes include business owner (`biz_owner`, *Y*|*N*), age (`age`), and political affiliation (`politics`, *Liberal*, *Conservative*, *Labor*). 

```{r}
tax$gt <- tbl_summary(
  tax$dat %>% select(-age), 
  by = politics, 
  statistic = list(all_continuous() ~ "{mean} ({sd})")
)
tax$gt
```

```{r}
tax$dat %>%
  mutate(income = case_when(income < quantile(income, .25) ~ "low income",
                            income < quantile(income, .75) ~ "med income",
                            TRUE ~ "high income"),
         income = factor(income, levels = c("low income", "med income", "high income"))) %>%
  count(tax_too_high, biz_owner, politics, income) %>%
  ggplot(aes(x = tax_too_high, y = n, fill = biz_owner)) +
  geom_col(position = position_dodge2(preserve = "single")) +
  facet_grid(rows = vars(income), cols = vars(politics), space = "free") +
  scale_x_discrete(labels = function (x) str_wrap(x, width = 10)) +
  theme_bw() +
  theme(legend.position = "bottom") +
  labs(title = "Taxes too high?",
       subtitle = "Reponse count by business owner, income level, and party.")
```

Fit a cumulative link model for the cumulative probability of the $i$th response falling in $j$th category or below where $i$ indexes the ($n = 192$) responses, $j = 1, \ldots, J$ indexes the ($J = 4$) response categories, and $\theta_j$ is the threshold for the $j$th cumulative logit.

$$\mathrm{logit}(P(Y_i \le j)) = \theta_j - \beta_1(\mathrm{politics}_i) - \beta_2(\mathrm{biz\_owner}_i) - \beta_3(\mathrm{age}_i)$$

#### Fit the Model {-}

```{r}
tax$fmla <- formula(tax_too_high ~ biz_owner + age + politics)
tax$clm <- clm(tax$fmla, data = tax$dat)
summary(tax$clm)
```

The summary table shows two fit statistics at the top: the log-likelihood and the AIC. The log-likelihood is the sum of the likelihoods for each observation that the predicted value correctly predicts the observed value. Its value ranges from $-\infty$ to $+\infty$. It's value increases with observations, additional variables, and fit quality. I think you just use it to compare alternative model formulations. Akaike Information Criterion (AIC) is a measure of the relative quality of a statistical model. Again, I think the value is only useful as a comparison benchmark between alternative model fits. You'd want the model with the lowest AIC.

The Coefficients table is the familiar parameter estimates. E.g., for `biz_ownerYes`, the coefficient estimate is `r coef(summary(tax$clm))[4, "Estimate"] %>% scales::comma(accuracy = .001)` with standard error `r coef(summary(tax$clm))[4, "Std. Error"] %>% scales::comma(accuracy = .001)`. The *z*-value is the ratio $z = \hat{\beta} / se =$ `r coef(summary(tax$clm))[4, "z value"] %>% scales::comma(accuracy = .001)` with *p*-value equal to $2 \cdot P(Z>z) =$ `r coef(summary(tax$clm))[4, "Pr(>|z|)"] %>% scales::comma(accuracy = .001)`. Some programs (e.g., SPSS) also show the Wald chi-squared statistic, the square of the *z* statistic, $z^2 =$, `r coef(summary(tax$clm))[4, "z value"]^2 %>% scales::comma(accuracy = .001)`. The square of a normal variable has a chi-square distribution, so the *p* value for the Wald chi-squared statistic is the `pchisq(z^2, df = 1)` $=$ `r coef(summary(tax$clm))[4, "z value"]^2 %>% pchisq(df = 1, lower.tail = FALSE) %>% scales::comma(accuracy = .001)`.

The Threshold coefficients table are the intercepts, or cut-points. The first cut-point is log-odds of a response level *Strongly Disagree* (or less) vs greater than *Strongly Disagree* when all factor variables are at their reference level and the continuous vars are at 0.

There may be interaction effects between `biz_owner` and `politics`. Fit a saturate model, then compare their log likelihoods with a likelihood ratio test.
```{r}
tax$fmla_sat <- formula(tax_too_high ~ biz_owner*politics + age)
tax$clm_sat <- clm(tax$fmla_sat, data = tax$dat)
tax$sat_anova <- anova(tax$clm, tax$clm_sat)
tax$sat_anova
```

The likelihood ratio test indicates the main-effects model fits about the same in comparison to the saturated model ($\chi^2($ `r tax$sat_anova[["df"]][2]` $) = $ *LR* = `r tax$sat_anova$LR.stat[2] %>% scales::comma(accuracy = .01)`, *p* = `r tax$sat_anova[["Pr(>Chisq)"]][2] %>% scales::comma(accuracy = .001)`)

#### Verify Assumptions {-}

Cumulative odds ordinal logistic regression with proportional odds models require a) no multicollinearity, and b) proportional odds.

##### Multicollinearity {-}

Multicollinearity occurs when two or more independent variables are *highly correlated* so that they do not provide unique or independent information in the regression model. Multicollinearity inflates the variances of the estimated coefficients, resulting in larger confidence intervals. The usual interpretation of a slope coefficient as the change in the mean response per unit increase in the predictor when all the other predictors are held constant breaks down because changing one predictor *necessarily* changes other predictors.

Test for multicollinearity with variance inflation factors (VIF). The VIF is the inflation percentage of the parameter variance due to multicollinearity. E.g., a VIF of 1.9 means the parameter variance is 90% larger than what it would be if it was not correlated with other predictors.

Predictor $K$'s variance, $Var(\hat{\beta_k})$, is inflated by a factor of

$$VIF_k = \frac{1}{1 - R_k^2}$$

due to collinearity with other predictors, where $R_k^2$ is the $R^2$ of a regression of the $k^{th}$ predictor on the remaining predictors. If there is zero relationship between predictor $k$ and the other variables, $R_k^2 = 0$ and $VIF = 1$ (no variance inflation). If 100% of the variance in predictor $k$ is explained by the other predictors, then $R_k^2 = 1$ and $VIF = \infty$. A good rule of thumb is that $VIF \le 5$ is acceptable.

```{r}
# Cannot use CLM model with vif(). Re-express as a linear model.
tax$vif <- lm(as.numeric(tax_too_high) ~ politics + biz_owner + age, dat = tax$dat) %>%
  car::vif()
tax$vif
```

The VIFs in column GVIF are all below 5, so this model is not compromised by multicollinearity.

##### Proportional Odds {-}

The assumption of proportional odds means the independent variable effects are constant across each cumulative split of the ordinal dependent variable. Test for proportional odds using a full [likelihood ratio test](https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faqhow-are-the-likelihood-ratio-wald-and-lagrange-multiplier-score-tests-different-andor-similar/) comparing the proportional odds model with a multinomial logit model, also called an unconstrained baseline logit model. This is also called the *test of parallel lines*. The multinomial logit model fits a slope to each of the $J – 1$ levels. The proportional odds model is nested within the multinomial model, so you can use a likelihood ratio test to see if the models are statistically different. Fit the proportional odds model and a multinomial model using `VGAM::vglm()` and capture the log likelihoods and degrees of freedom. Perform a likelihood ratio test on the differences in log likelihoods, $D = -2 \mathrm{loglik}(\beta)$.

```{r}
tax$vglm_ordinal     <- vglm(tax$fmla, propodds,   data = tax$dat)
tax$vglm_multinomial <- vglm(tax$fmla, cumulative, data = tax$dat)
tax$po_lrt <- lrtest(tax$vglm_multinomial, tax$vglm_ordinal)
tax$po_lrt
```

> The assumption of proportional odds was met, as assessed by a full likelihood ratio test comparing the fit of the proportional odds model to a model with varying location parameters, $\chi^2$(8) = `r tax$po_lrt %>% slot("Body") %>% tail(1) %>% pull(Chisq) %>% scales::number(accuracy = .001)`, *p* = `r tax$po_lrt %>% slot("Body") %>% tail(1) %>% pull("Pr(>Chisq)") %>% scales::number(accuracy = .001)`.

Another option is the partial proportional odds test. This test locates specific variables causing the rejection of proportional odds.

```{r}
tax$po_lrt2 <- clm(tax$fmla, data = tax$dat) %>%
  nominal_test()
tax$po_lrt2
```

> The assumption of proportional odds was met, as assessed by a full likelihood ratio test comparing the fit of the proportional odds model to a model with varying location parameters for business owner, $\chi^2$(`r tax$po_lrt2["biz_owner", "Df"]`) = `r tax$po_lrt2["biz_owner", "LRT"] %>% scales::number(accuracy = .001)`, *p* = `r tax$po_lrt2["biz_owner", "Pr(>Chi)"] %>% scales::number(accuracy = .001)` and politics, $\chi^2$(`r tax$po_lrt2["politics", "Df"]`) = `r tax$po_lrt2["politics", "LRT"] %>% scales::number(accuracy = .001)`, *p* = `r tax$po_lrt2["politics", "Pr(>Chi)"] %>% scales::number(accuracy = .001)`.

#### Assess the Model Fit {-}

There are three ways to assess overall model fit: The Deviance and Pearson goodness-of-fit tests of the overall model fit; the Cox and Snell, Nagelkerke, and McFadden pseudo R measures of explained variance; and the likelihood ratio test comparing the model fit to the intercept-only model. 

###### Deviance and Pearson {-}

However, these tests rely on large frequencies in each *cell*, that is, each possible combination of predictor values. Overall goodness-of-fit statistics should be treated with suspicion when a continuous independent variable is present and/or there are a large number of cells with zero frequency.The Pearson goodness-of-fit statistic is $X^2 = \sum \frac{(O_{ij} - E_{ij})^2}{E_{ij}}$ where $i$ is the observation number and $j$ is the response variable level. It is a summary of the Pearson residuals, the difference between the observed and expected cell counts, $O_{ij} - E_{ij}$. The deviance goodness-of-fit statistic is the difference in fit between the model and a full model; a full model being a model that fits the data perfectly, $G^2 = 2 \sum_{ij} O_{ij} \log \left( \frac{O_{ij}}{E_{ij}} \right)$. Neither of these tests are reliable if there are many cells with zero frequencies and/or small expected frequencies and are generally not recommended. Generally, the chi-squared test requires a frequency count of at least 5 per cell. 

```{r}
# Observed combinations of model vars
tax$cell_patterns <- tax$dat %>% count(biz_owner, age, politics, tax_too_high) %>% nrow()

# Observed combinations of predictor vars * levels of response var
tax$covariate_patterns <- tax$dat %>% count(biz_owner, age, politics) %>% nrow()
tax$possible_cells <- tax$covariate_patterns * length(levels(tax$dat$tax_too_high))

# 1 - ratio of observed to possible
tax$pct_freq_zero <- 1 - tax$cell_patterns / tax$possible_cells
```

There are `r tax$cell_patterns` observed combinations of model variables (predictors), and `r tax$possible_cells` possible combinations (predictors * outcome levels), so `r tax$pct_freq_zero %>% scales::percent(accuracy = .1)` of cells have zero frequencies. Ideally, zero frequencies should be less than 20%, so if you were to use the deviance or Pearson tests, you would need to report this. The results below are contradictory and bogus. I think you'd only use this test if you didn't have continuous predictor variables.

```{r}
observed <- tax$dat %>% 
  count(biz_owner, age, politics, tax_too_high) %>%
  pivot_wider(names_from = tax_too_high, values_from = n) %>%
  replace_na(list(`Strongly Disagree` = 0, Disagree = 0, Agree = 0, `Strongly Agree` = 0)) %>%
  pivot_longer(cols = `Strongly Disagree`:`Strongly Agree`, names_to = "outcome", values_to = "observed")

expected <- bind_cols(
  tax$dat, 
  predict(tax$clm, subset(tax$dat, select = -tax_too_high))$fit %>% data.frame()
) %>%
  rename(c("Strongly Disagree" = "Strongly.Disagree", "Strongly Agree" = "Strongly.Agree")) %>%
  group_by(biz_owner, age, politics) %>%
  summarize(.groups = "drop", across(`Strongly Disagree`:`Strongly Agree`, sum)) %>%
  pivot_longer(cols = `Strongly Disagree`:`Strongly Agree`, names_to = "outcome", values_to = "expected")

obs_exp <- observed %>%
  inner_join(expected, by = c("politics", "biz_owner", "age", "outcome")) %>%
  mutate(epsilon_sq = (observed - expected)^2,
         chi_sq = epsilon_sq / expected,
         g_sq = 2 * observed * log((observed+.0001) / expected)
  )

tax$chisq <- list()
tax$chisq$X2 = sum(obs_exp$chi_sq)
tax$chisq$G2 = sum(obs_exp$g_sq)
tax$chisq$df = tax$covariate_patterns * (length(levels(tax$dat$tax_too_high)) - 1) - 7
tax$chisq$X2_p.value = pchisq(tax$chisq$X2, df = tax$chisq$df, lower.tail = FALSE)
tax$chisq$G2_p.value = pchisq(tax$chisq$G2, df = tax$chisq$df, lower.tail = FALSE)
```

> The Pearson goodness-of-fit test indicated that the model was *not* a good fit to the observed data, $\chi^2$(`r tax$chisq$df`) = `r tax$chisq$X2 %>% scales::comma(accuracy = 0.1)`, *p* < .001$. The deviance goodness-of-fit test indicated that the model was a good fit to the observed data, $G^2$(`r tax$chisq$df`) = `r tax$chisq$G2 %>% scales::comma(accuracy = 0.1)`, *p* = `r tax$chisq$G2_p.value %>% scales::comma(accuracy = 0.001)`. 

###### Pseudo-R2 Measures {-}

There are a number of measures in ordinal regression that attempt to provide a similar "variance explained" measure as that provided in ordinary least-squares linear regression. However, these measures do not have the direct interpretation that they do in ordinary linear regression and are often, therefore, referred to as "pseudo" R2 measures. The three most common measures (Cox and Snell, Nagelkerke, and McFadden) are not particularly good and not universally used. It is presented in the SPSS output, so you might encounter it in published work.

```{r}
tax$nagelkerke <- rcompanion::nagelkerke(tax$clm)
tax$nagelkerke$Pseudo.R.squared.for.model.vs.null
```

###### Likelihood Ratio Test {-}

The best way to assess model fit is the likelihood ratio test comparing the model to an intercept-only model. The difference in the -2 log likelihood between the models has a $\chi^2$ distribution with degrees of freedom equal to the difference in the number of parameters.

```{r}
intercept_only <- clm(tax_too_high ~ 1, data = tax$dat)
tax$lrt <- anova(tax$clm, intercept_only)
tax$lrt
```

The table shows the log likelihoods of the two models. LR.stat is the difference between 2 * the logLik values.

> The final model statistically significantly predicted the dependent variable over and above the intercept-only model, $\chi^2(4)$ = `r tax$lrt$LR.stat[2] %>% scales::comma(accuracy = .1)`, *p* = `r tax$lrt[[6]][2] %>% scales::comma(accuracy = .001)`.

#### Interpret Results {-}

Return to the model summary. 

```{r}
tidy(tax$clm)
```

The coefficients for `biz_owner`, `age`, and politics are positive. Positive parameters *increase* the likelihood of stronger agreement with the statement. In this case, discontent with taxes are higher for business owners, increase with age, and are higher for Liberal Democrats and Conservatives relative to the Labor Party. The expected cumulative log-odds of declaring $\le j$ level of agreement with the statement for the baseline group (`biz_ownerNo`, `age = 0`, `politicsLib`) is `r tax$clm$coefficients["Strongly Disagree|Disagree"] %>% scales::comma(accuracy = .001)` for $j = 1$ (Strongly Disagree), `r tax$clm$coefficients["Disagree|Agree"] %>% scales::comma(accuracy = .001)` for $j = 2$ (Disagree), and `r tax$clm$coefficients["Agree|Strongly Agree"] %>% scales::comma(accuracy = .001)` for $j = 3$ (Agree). 

You could solve the logit equation for 

$$\pi_j = \frac{\mathrm{exp}(Y_i)} {1 + \mathrm{exp}(Y_i)}$$
to get the cumulative probabilities for each level. That's what `predict(type = "cum.prob")` does. But it might be more intuitive to work with individual probabilities, the lagged differences to get the individual probabilities for each $j$. That's what `predict(type = "prob")` does. I like to play with predicted values to get a sense of the outcome distributions. In this case, I'll take the median `age`, and each combination of `biz_owner` and `politics`.

```{r fig.height=2.5, fig.width=6.5}
new_data <- tax$dat %>% 
  mutate(age = median(tax$dat$age)) %>% 
  expand(age, politics, biz_owner)

preds <- predict(tax$clm, newdata = new_data, type = "prob")[["fit"]] %>% as.data.frame()

bind_cols(new_data, preds) %>%
  pivot_longer(cols = `Strongly Disagree`:`Strongly Agree`) %>% 
  mutate(name = factor(name, levels = levels(tax$dat$tax_too_high))) %>%
  ggplot(aes(y = politics, x = value, fill = fct_rev(name))) +
  geom_col() +
  geom_text(aes(label = scales::percent(value, accuracy = 1)), 
            size = 3, position = position_stack(vjust=0.5)) +
  facet_grid(~paste("Bus Owner = ", biz_owner)) +
  scale_fill_grey(start = 0.5, end = 0.8) +
  theme_bw() + 
  theme(legend.position = "top",
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank()) +
  guides(fill = guide_legend(reverse = TRUE)) +
  labs(title = "Taxes too High for Conservative Business Owners?", 
       x = NULL, fill = NULL)
```

You will want to establish whether `politics` is statistically significant overall before exploring any specific contrasts. The ANOVA procedure with type I test reports an overall test of significance for each variable entered into the model.

```{r}
(tax$anovaI <- anova(tax$clm, type = "I"))
```

> The political party last voted for has a statistically significant effect on the prediction of whether tax is thought to be too high, Wald $\chi^2$(`r tax$anovaI$Df[3]`) = `r tax$anovaI$Chisq[3] %>% scales::comma(accuracy = .1)`, *p* = `r tax$anovaI[[3]][3] %>% scales::comma(accuracy = .001)`.

The best way to work with the data is with the `tidy(exponentiate = TRUE)` version. 

```{r}
tax$tidy <- tax$clm %>% tidy(conf.int = TRUE, exponentiate = TRUE)
tax$tidy
```

Then you can summarize the table in words.

> The odds of business owners considering tax to be too high was `r tax$tidy %>% filter(term == "biz_ownerYes") %>% pull(estimate) %>% scales::comma(accuracy = .001)` (95% CI, `r tax$tidy %>% filter(term == "biz_ownerYes") %>% pull(conf.low) %>% scales::comma(accuracy = .001)` to `r tax$tidy %>% filter(term == "biz_ownerYes") %>% pull(conf.high) %>% scales::comma(accuracy = .001)`) times that of non-business owners, a statistically significant effect, *z* = `r tax$tidy %>% filter(term == "biz_ownerYes") %>% pull(statistic) %>% scales::comma(accuracy = .001)`, *p* = `r tax$tidy %>% filter(term == "biz_ownerYes") %>% pull(p.value) %>% scales::comma(accuracy = .001)`.

> The odds of Conservative voters considering tax to be too high was `r tax$tidy %>% filter(term == "politicsCon") %>% pull(estimate) %>% scales::comma(accuracy = .001)` (95% CI, `r tax$tidy %>% filter(term == "politicsCon") %>% pull(conf.low) %>% scales::comma(accuracy = .001)` to `r tax$tidy %>% filter(term == "politicsCon") %>% pull(conf.high) %>% scales::comma(accuracy = .001)`) times that of Labour voters, a statistically significant effect, *z* = `r tax$tidy %>% filter(term == "politicsCon") %>% pull(statistic) %>% scales::comma(accuracy = .001)`, *p* = `r tax$tidy %>% filter(term == "politicsCon") %>% pull(p.value) %>% scales::comma(accuracy = .001)`. The odds of Liberal Democrat voters considering tax to be too high was similar to that of Labour voters (odds ratio of `r tax$tidy %>% filter(term == "politicsLib") %>% pull(estimate) %>% scales::comma(accuracy = .001)` (95% CI, `r tax$tidy %>% filter(term == "politicsLib") %>% pull(conf.low) %>% scales::comma(accuracy = .001)` to `r tax$tidy %>% filter(term == "politicsLib") %>% pull(conf.high) %>% scales::comma(accuracy = .001)`), *p* = `r tax$tidy %>% filter(term == "politicsLib") %>% pull(p.value) %>% scales::comma(accuracy = .001)`.

> An increase in age (expressed in years) was associated with an increase in the odds of considering tax too high, with an odds ratio of `r tax$tidy %>% filter(term == "age") %>% pull(estimate) %>% scales::comma(accuracy = .001)` (95% CI, `r tax$tidy %>% filter(term == "age") %>% pull(conf.low) %>% scales::comma(accuracy = .001)` to `r tax$tidy %>% filter(term == "age") %>% pull(conf.high) %>% scales::comma(accuracy = .001)`), *z* = `r tax$tidy %>% filter(term == "age") %>% pull(statistic) %>% scales::comma(accuracy = .001)`, *p* = `r tax$tidy %>% filter(term == "age") %>% pull(p.value) %>% scales::comma(accuracy = .001)`.

#### Reporting {-}

Here is the complete write-up.

> A cumulative odds ordinal logistic regression with proportional odds was run to determine the effect of business ownership, political party voted for, and age, on the belief that taxes are too high. There were proportional odds, as assessed by a full likelihood ratio test comparing the fitted model to a model with varying location parameters, $\chi^2$(8) = `r tax$po_lrt %>% slot("Body") %>% tail(1) %>% pull(Chisq) %>% scales::number(accuracy = .001)`, *p* = `r tax$po_lrt %>% slot("Body") %>% tail(1) %>% pull("Pr(>Chisq)") %>% scales::number(accuracy = .001)`. The final model statistically significantly predicted the dependent variable over and above the intercept-only model, $\chi^2(4)$ = `r tax$lrt$LR.stat[2] %>% scales::comma(accuracy = .1)`, *p* = `r tax$lrt[[6]][2] %>% scales::comma(accuracy = .001)`. The odds of business owners considering tax to be too high was `r tax$tidy %>% filter(term == "biz_ownerYes") %>% pull(estimate) %>% scales::comma(accuracy = .001)` (95% CI, `r tax$tidy %>% filter(term == "biz_ownerYes") %>% pull(conf.low) %>% scales::comma(accuracy = .001)` to `r tax$tidy %>% filter(term == "biz_ownerYes") %>% pull(conf.high) %>% scales::comma(accuracy = .001)`) times that of non-business owners, a statistically significant effect, *z* = `r tax$tidy %>% filter(term == "biz_ownerYes") %>% pull(statistic) %>% scales::comma(accuracy = .001)`, *p* = `r tax$tidy %>% filter(term == "biz_ownerYes") %>% pull(p.value) %>% scales::comma(accuracy = .001)`. The political party last voted for has a statistically significant effect on the prediction of whether tax is thought to be too high, Wald $\chi^2$(`r tax$anovaI$Df[3]`) = `r tax$anovaI$Chisq[3] %>% scales::comma(accuracy = .1)`, *p* = `r tax$anovaI[[3]][3] %>% scales::comma(accuracy = .001)`. The odds of Conservative voters considering tax to be too high was `r tax$tidy %>% filter(term == "politicsCon") %>% pull(estimate) %>% scales::comma(accuracy = .001)` (95% CI, `r tax$tidy %>% filter(term == "politicsCon") %>% pull(conf.low) %>% scales::comma(accuracy = .001)` to `r tax$tidy %>% filter(term == "politicsCon") %>% pull(conf.high) %>% scales::comma(accuracy = .001)`) times that of Labour voters, a statistically significant effect, *z* = `r tax$tidy %>% filter(term == "politicsCon") %>% pull(statistic) %>% scales::comma(accuracy = .001)`, *p* = `r tax$tidy %>% filter(term == "politicsCon") %>% pull(p.value) %>% scales::comma(accuracy = .001)`. The odds of Liberal Democrat voters considering tax to be too high was similar to that of Labour voters (odds ratio of `r tax$tidy %>% filter(term == "politicsLib") %>% pull(estimate) %>% scales::comma(accuracy = .001)` (95% CI, `r tax$tidy %>% filter(term == "politicsLib") %>% pull(conf.low) %>% scales::comma(accuracy = .001)` to `r tax$tidy %>% filter(term == "politicsLib") %>% pull(conf.high) %>% scales::comma(accuracy = .001)`), *p* = `r tax$tidy %>% filter(term == "politicsLib") %>% pull(p.value) %>% scales::comma(accuracy = .001)`. An increase in age (expressed in years) was associated with an increase in the odds of considering tax too high, with an odds ratio of `r tax$tidy %>% filter(term == "age") %>% pull(estimate) %>% scales::comma(accuracy = .001)` (95% CI, `r tax$tidy %>% filter(term == "age") %>% pull(conf.low) %>% scales::comma(accuracy = .001)` to `r tax$tidy %>% filter(term == "age") %>% pull(conf.high) %>% scales::comma(accuracy = .001)`), *z* = `r tax$tidy %>% filter(term == "age") %>% pull(statistic) %>% scales::comma(accuracy = .001)`, *p* = `r tax$tidy %>% filter(term == "age") %>% pull(p.value) %>% scales::comma(accuracy = .001)`.

Package **gtsummary** shows a nice summary table.

```{r}
tbl_regression(tax$clm)
```

## Poisson Regression {#poissonregression}

Poisson models count data, like "traffic tickets per day", or "website hits per day".  The response is an expected *rate* or intensity.  For count data, specify the generalized model, this time with `family = poisson` or `family = quasipoisson`. 

Recall that the probability of achieving a count $y$ when the expected rate is $\lambda$ is distributed 

$$P(Y = y|\lambda) = \frac{e^{-\lambda} \lambda^y}{y!}.$$


The poisson regression model is

$$\lambda = \exp(X \beta).$$ 
 
You can solve this for $y$ to get

$$y = X\beta = \ln(\lambda).$$

That is, the model predicts the log of the response rate.  For a sample of size *n*, the likelihood function is

$$L(\beta; y, X) = \prod_{i=1}^n \frac{e^{-\exp({X_i\beta})}\exp({X_i\beta})^{y_i}}{y_i!}.$$

The log-likelihood is

$$l(\beta) = \sum_{i=1}^n (y_i X_i \beta - \sum_{i=1}^n\exp(X_i\beta) - \sum_{i=1}^n\log(y_i!).$$

Maximizing the log-likelihood has no closed-form solution, so the coefficient estimates are found through interatively reweighted least squares.  

Poisson processes assume the variance of the response variable equals its mean.  "Equals" means the mean and variance are of a similar order of magnitude.  If that assumption does not hold, use the quasi-poisson.  Use Poisson regression for large datasets.  If the predicted counts are much greater than zero (>30), the linear regression will work fine.  Whereas RMSE is not useful for logistic models, it is a good metric in Poisson.


Dataset `fire` contains response variable `injuries` counting the number of injuries during the month and one explanatory variable, the month `mo`.

```{r}
fire <- read_csv(file = "C:/Users/mpfol/OneDrive/Documents/Data Science/Data/CivilInjury_0.csv")
fire <- fire %>% 
  mutate(mo = as.POSIXlt(`Injury Date`)$mon + 1) %>%
  rename(dt = `Injury Date`,
         injuries = `Total Injuries`)
str(fire)
```

In a situation like this where there the relationship is bivariate, start with a visualization.

```{r}
ggplot(fire, aes(x = mo, y = injuries)) +
  geom_jitter() +
  geom_smooth(method = "glm", method.args = list(family = "poisson")) +
  labs(title = "Injuries by Month")
```


Fit a poisson regression in R using `glm(formula, data, family = poisson)`.  But first, check whether the mean and variance of `injuries` are the same magnitude?  If not, then use `family = quasipoisson`.

```{r}
mean(fire$injuries)
var(fire$injuries)
```

They are of the same magnitude, so fit the regression with `family = poisson`.

```{r}
m2 <- glm(injuries ~ mo, family = poisson, data = fire)
summary(m2)
```

The *predicted* value $\hat{y}$ is the estimated **log** of the response variable, 

$$\hat{y} = X \hat{\beta} = \ln (\lambda).$$

Suppose `mo` is January (mo = `), then the log of `injuries` is $\hat{y} = 0.323787$. Or, more intuitively, the expected count of injuries is $\exp(0.323787) = 1.38$  

```{r}
predict(m2, newdata = data.frame(mo=1))
predict(m2, newdata = data.frame(mo=1), type = "response")
```

Here is a plot of the predicted counts in red.

```{r}
augment(m2, type.predict = "response") %>%
  ggplot(aes(x = mo, y = injuries)) +
  geom_point() +
  geom_point(aes(y = .fitted), color = "red") + 
  scale_y_continuous(limits = c(0, NA)) +
  labs(x = "Month",
       y = "Injuries",
       title = "Poisson Fitted Line Plot")
```

Evaluate a logistic model fit with an analysis of deviance.  

```{r}
(perf <- glance(m2))
(pseudoR2 <- 1 - perf$deviance / perf$null.deviance)
```

The deviance of the null model (no regressors) is 139.9.  The deviance of the full model is 132.2.  The psuedo-R2 is very low at .05.  How about the RMSE?

```{r}
RMSE(pred = predict(m2, type = "response"), obs = fire$injuries)
```

The average prediction error is about 0.99.  That's almost as much as the variance of `injuries` - i.e., just predicting the mean of `injuries` would be almost as good!  Use the `GainCurvePlot()` function to plot the gain curve.

```{r}
augment(m2, type.predict = "response") %>%
  ggplot(aes(x = injuries, y = .fitted)) +
  geom_point() +
  geom_smooth(method ="lm") +
  labs(x = "Actual",
       y = "Predicted",
       title = "Poisson Fitted vs Actual")
```


```{r}
augment(m2) %>% data.frame() %>% 
  GainCurvePlot(xvar = ".fitted", truthVar = "injuries", title = "Poisson Model")
```

It seems that `mo` was a poor predictor of `injuries`.  


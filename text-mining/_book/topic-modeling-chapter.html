<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="2.1 Topic Modeling Chapter | Text Mining using R" />
<meta property="og:type" content="book" />


<meta property="og:description" content="Background and tutorial on text mining (topic modeling, sentiment analysis) using R." />


<meta name="author" content="Michael Foley" />

<meta name="date" content="2020-11-21" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="Background and tutorial on text mining (topic modeling, sentiment analysis) using R.">

<title>2.1 Topic Modeling Chapter | Text Mining using R</title>

<script src="assets/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="assets/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="assets/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="assets/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="assets/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="assets/navigation-1.1/tabsets.js"></script>
<script src="assets/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="assets/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="assets/anchor-sections-1.0/anchor-sections.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#intro">Intro</a></li>
<li><a href="topicmodeling.html#topicmodeling"><span class="toc-section-number">1</span> Topic Modeling</a></li>
<li class="has-sub"><a href="text-mining.html#text-mining"><span class="toc-section-number">2</span> Text Mining</a><ul>
<li class="has-sub"><a href="topic-modeling-chapter.html#topic-modeling-chapter"><span class="toc-section-number">2.1</span> Topic Modeling Chapter</a><ul>
<li><a href="topic-modeling-chapter.html#lda"><span class="toc-section-number">2.1.1</span> LDA</a></li>
<li><a href="topic-modeling-chapter.html#stm"><span class="toc-section-number">2.1.2</span> STM</a></li>
</ul></li>
<li class="has-sub"><a href="sentiment-analysis.html#sentiment-analysis"><span class="toc-section-number">2.2</span> Sentiment Analysis</a><ul>
<li><a href="sentiment-analysis.html#n-grams"><span class="toc-section-number">2.2.1</span> N-Grams</a></li>
<li><a href="sentiment-analysis.html#converting-to-and-from-non-tidy-formats"><span class="toc-section-number">2.2.2</span> Converting to and from non-tidy formats</a></li>
<li><a href="sentiment-analysis.html#example"><span class="toc-section-number">2.2.3</span> Example</a></li>
</ul></li>
<li><a href="text-classification-modeling.html#text-classification-modeling"><span class="toc-section-number">2.3</span> Text Classification Modeling</a></li>
<li><a href="named-entity-recognition.html#named-entity-recognition"><span class="toc-section-number">2.4</span> Named Entity Recognition</a></li>
<li><a href="tidy-text.html#tidy-text"><span class="toc-section-number">2.5</span> Tidy Text</a></li>
<li class="has-sub"><a href="appendix-regular-expressions.html#appendix-regular-expressions"><span class="toc-section-number">2.6</span> Appendix: Regular Expressions</a><ul>
<li><a href="appendix-regular-expressions.html#base-r"><span class="toc-section-number">2.6.1</span> Base R</a></li>
<li><a href="appendix-regular-expressions.html#stringr"><span class="toc-section-number">2.6.2</span> stringr</a></li>
<li><a href="appendix-regular-expressions.html#regular-expressions"><span class="toc-section-number">2.6.3</span> Regular Expressions</a></li>
</ul></li>
<li><a href="appendix-tidytext.html#appendix-tidytext"><span class="toc-section-number">2.7</span> Appendix: tidytext</a></li>
<li><a href="appendix-tm.html#appendix-tm"><span class="toc-section-number">2.8</span> Appendix: tm</a></li>
</ul></li>
<li><a href="drop.html#drop"><span class="toc-section-number">3</span> DROP</a></li>
<li><a href="methods.html#methods"><span class="toc-section-number">4</span> Methods</a></li>
<li class="has-sub"><a href="applications.html#applications"><span class="toc-section-number">5</span> Applications</a><ul>
<li><a href="example-one.html#example-one"><span class="toc-section-number">5.1</span> Example one</a></li>
<li><a href="example-two.html#example-two"><span class="toc-section-number">5.2</span> Example two</a></li>
</ul></li>
<li><a href="final-words.html#final-words"><span class="toc-section-number">6</span> Final Words</a></li>
<li><a href="references.html#references">References</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="topic-modeling-chapter" class="section level2">
<h2><span class="header-section-number">2.1</span> Topic Modeling Chapter</h2>
<p>Topic models are generative probabilistic models to quantitatively investigate a large text corpora. In generative probability models, the researcher postulates a model of a data-generating process, then uses the corpus data to find the most likely values for the parameters within the model.</p>
<p>Topic models such as Latent Dirichlet Allocation (LDA) and Structural Topic Modeling (STM) treat documents within a corpora as a “bags of words” and identifies groups of words that tend to co-occur. These groups of words are the topics, formally conceptualized as probability distributions over vocabulary. While the topics are a product of model estimation, the topic name and interpretation are assigned by the researcher by examining the most important words for each topic and the particular texts featuring a topic most prominently. They are not produced to match words and documents to concrete issues specified in advance by the researcher as they would be in a supervised model.</p>
<p>LDA and STM are generative models, meaning you define a data generating process for each document then use the document text to find the most likely values for the model parameters. The generative model defines document-topic and word-topic distributions generating documents. The sum of topic proportions (topical prevalence) in each document is one, and the sum of word probabilities in each topic (topical content) is one. STM differs from LDA in that it uses document metadata as covariates to explain prevalence and topical content.</p>
<p>Whether you use LDA or STM, you’ll start by creating a bag-of-words representation of the data. In the chunk below, I <a href="https://en.wikipedia.org/wiki/Lemmatisation">lemmatize</a> the tokens and remove my own list of stop words (e.g., “tom” is so common that I don’t want to include it in anything.)</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="topic-modeling-chapter.html#cb4-1"></a>my_stop_words &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;tom&quot;</span>)</span>
<span id="cb4-2"><a href="topic-modeling-chapter.html#cb4-2"></a></span>
<span id="cb4-3"><a href="topic-modeling-chapter.html#cb4-3"></a>sawyer_tokens &lt;-<span class="st"> </span>sawyer <span class="op">%&gt;%</span></span>
<span id="cb4-4"><a href="topic-modeling-chapter.html#cb4-4"></a><span class="st">  </span><span class="kw">unnest_tokens</span>(<span class="dt">output =</span> <span class="st">&quot;word&quot;</span>, <span class="dt">input =</span> text, <span class="dt">token =</span> <span class="st">&quot;words&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb4-5"><a href="topic-modeling-chapter.html#cb4-5"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">word_stem =</span> <span class="kw">lemmatize_words</span>(word)) <span class="op">%&gt;%</span></span>
<span id="cb4-6"><a href="topic-modeling-chapter.html#cb4-6"></a><span class="st">  </span><span class="kw">anti_join</span>(stop_words, <span class="dt">by =</span> <span class="st">&quot;word&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb4-7"><a href="topic-modeling-chapter.html#cb4-7"></a><span class="st">  </span><span class="kw">filter</span>(<span class="op">!</span>word <span class="op">%in%</span><span class="st"> </span>my_stop_words)</span></code></pre></div>
<p>It’s a good idea to explore the data before modeling. One good way to do that is with the TF-IDF statistic. A term’s frequency (TF) is its proportion of the words in the document. The inverse document frequency (IDF) is the log of the inverse ratio of documents in which the term appears. The product of TF and IDF (TF-IDF) indicates how important a word is to a document in a collection or corpus. In this case, the TF-IDF indicates how important a word is to the chapter. A term’s TF-IDF increases with its frequency in the document (chapter) and decreases with the number of documents (chapters) in the corpus that contain it. Use <code>tidytext::bind_tf_idf()</code> to append the <code>tf</code>, <code>idf</code>, and <code>tf_idf</code> columns to your data frame.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="topic-modeling-chapter.html#cb5-1"></a>sawywer_tf_idf &lt;-<span class="st"> </span>sawyer_tokens <span class="op">%&gt;%</span></span>
<span id="cb5-2"><a href="topic-modeling-chapter.html#cb5-2"></a><span class="st">  </span><span class="kw">count</span>(chapter, word_stem, <span class="dt">sort =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span></span>
<span id="cb5-3"><a href="topic-modeling-chapter.html#cb5-3"></a><span class="st">  </span><span class="kw">bind_tf_idf</span>(word_stem, chapter, n) <span class="op">%&gt;%</span></span>
<span id="cb5-4"><a href="topic-modeling-chapter.html#cb5-4"></a><span class="st">  </span><span class="kw">group_by</span>(chapter) <span class="op">%&gt;%</span></span>
<span id="cb5-5"><a href="topic-modeling-chapter.html#cb5-5"></a><span class="st">  </span><span class="kw">slice_max</span>(<span class="dt">order_by =</span> tf_idf, <span class="dt">n =</span> <span class="dv">10</span>) <span class="op">%&gt;%</span></span>
<span id="cb5-6"><a href="topic-modeling-chapter.html#cb5-6"></a><span class="st">  </span><span class="kw">ungroup</span>()</span>
<span id="cb5-7"><a href="topic-modeling-chapter.html#cb5-7"></a></span>
<span id="cb5-8"><a href="topic-modeling-chapter.html#cb5-8"></a>sawywer_tf_idf <span class="op">%&gt;%</span></span>
<span id="cb5-9"><a href="topic-modeling-chapter.html#cb5-9"></a><span class="st">  </span><span class="kw">filter</span>(chapter <span class="op">&lt;=</span><span class="st"> </span><span class="dv">6</span>) <span class="op">%&gt;%</span></span>
<span id="cb5-10"><a href="topic-modeling-chapter.html#cb5-10"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">word_stem =</span> <span class="kw">reorder_within</span>(word_stem, <span class="dt">by =</span> tf_idf, <span class="dt">within =</span> chapter)) <span class="op">%&gt;%</span></span>
<span id="cb5-11"><a href="topic-modeling-chapter.html#cb5-11"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(word_stem, tf_idf, <span class="dt">fill =</span> <span class="kw">as.factor</span>(chapter))) <span class="op">+</span></span>
<span id="cb5-12"><a href="topic-modeling-chapter.html#cb5-12"></a><span class="st">  </span><span class="kw">geom_col</span>(<span class="dt">alpha =</span> <span class="fl">0.8</span>, <span class="dt">show.legend =</span> <span class="ot">FALSE</span>) <span class="op">+</span></span>
<span id="cb5-13"><a href="topic-modeling-chapter.html#cb5-13"></a><span class="st">  </span><span class="kw">scale_fill_manual</span>(<span class="dt">values =</span> RColorBrewer<span class="op">::</span><span class="kw">brewer.pal</span>(<span class="dt">n =</span> <span class="dv">6</span>, <span class="dt">name =</span> <span class="st">&quot;Set2&quot;</span>), <span class="dt">name =</span> <span class="st">&quot;Topic&quot;</span>) <span class="op">+</span></span>
<span id="cb5-14"><a href="topic-modeling-chapter.html#cb5-14"></a><span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span><span class="st"> </span>chapter, <span class="dt">scales =</span> <span class="st">&quot;free&quot;</span>, <span class="dt">ncol =</span> <span class="dv">3</span>) <span class="op">+</span></span>
<span id="cb5-15"><a href="topic-modeling-chapter.html#cb5-15"></a><span class="st">  </span><span class="kw">scale_x_reordered</span>() <span class="op">+</span></span>
<span id="cb5-16"><a href="topic-modeling-chapter.html#cb5-16"></a><span class="st">  </span><span class="kw">coord_flip</span>() <span class="op">+</span></span>
<span id="cb5-17"><a href="topic-modeling-chapter.html#cb5-17"></a><span class="st">  </span><span class="kw">theme</span>(<span class="dt">strip.text=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">11</span>)) <span class="op">+</span></span>
<span id="cb5-18"><a href="topic-modeling-chapter.html#cb5-18"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="ot">NULL</span>, <span class="dt">y =</span> <span class="st">&quot;tf-idf&quot;</span>,</span>
<span id="cb5-19"><a href="topic-modeling-chapter.html#cb5-19"></a>       <span class="dt">title =</span> <span class="st">&quot;Highest tf-idf words in The Adventures of Tom Sawyer (ch 1-6)&quot;</span>,</span>
<span id="cb5-20"><a href="topic-modeling-chapter.html#cb5-20"></a>       <span class="dt">subtitle =</span> <span class="st">&quot;Individual chapters focus on different characters and narrative elements&quot;</span>)</span></code></pre></div>
<p><img src="text-book_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<div id="lda" class="section level3">
<h3><span class="header-section-number">2.1.1</span> LDA</h3>
<p>LDA treats each document as a mixture of topics (X% topic <em>A</em>, Y% topic <em>B</em>, etc.), and each topic as a mixture of word probabilities (x% word <em>a</em>, y% word <em>b</em>, etc.) for all words in the corpus. LDA is implemented in the <strong>topicmodels</strong> package.</p>
<p>Prepare the data by creating a document-term matrix. You will improve performance by removing infrequently appearing tokens with <code>tm::removeSparseTemrs()</code>.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="topic-modeling-chapter.html#cb6-1"></a>sawyer_dtm &lt;-<span class="st"> </span>sawyer_tokens <span class="op">%&gt;%</span></span>
<span id="cb6-2"><a href="topic-modeling-chapter.html#cb6-2"></a><span class="st">  </span><span class="kw">count</span>(chapter, word_stem) <span class="op">%&gt;%</span></span>
<span id="cb6-3"><a href="topic-modeling-chapter.html#cb6-3"></a><span class="st">  </span><span class="kw">cast_dtm</span>(<span class="dt">document =</span> chapter, <span class="dt">term =</span> word_stem, <span class="dt">value =</span> n) <span class="op">%&gt;%</span></span>
<span id="cb6-4"><a href="topic-modeling-chapter.html#cb6-4"></a><span class="st">  </span>tm<span class="op">::</span><span class="kw">removeSparseTerms</span>(<span class="dt">sparse =</span> <span class="fl">0.9</span>)</span>
<span id="cb6-5"><a href="topic-modeling-chapter.html#cb6-5"></a></span>
<span id="cb6-6"><a href="topic-modeling-chapter.html#cb6-6"></a>sawyer_dtm</span></code></pre></div>
<pre><code>## &lt;&lt;DocumentTermMatrix (documents: 35, terms: 1213)&gt;&gt;
## Non-/sparse entries: 9933/32522
## Sparsity           : 77%
## Maximal term length: 13
## Weighting          : term frequency (tf)</code></pre>
<p>LDA is an optimization algorithm that performs a random search through the parameter space to find the model with the largest log-likelihood. There are multiple search algorithms, but the preferred one appears to be Gibbs sampling, a type of Monte Carlo Markov Chain (MCMC) algorithm.</p>
<p>Create a topic model with <code>topicmodels::LDA()</code>. Parameter <code>k</code> specifies the number of topics. In general, you only want as many topics as are clearly distinct and that you can easily communicate to others. You can use the perplexity statistic to help identify <em>k</em>. Perplexity is a measure of how well a probability model fits a new set of data. Look for the elbow in a scree plot. Here are models with 5 - 50 topics.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="topic-modeling-chapter.html#cb8-1"></a><span class="kw">set.seed</span>(<span class="dv">1012</span>)</span>
<span id="cb8-2"><a href="topic-modeling-chapter.html#cb8-2"></a>train_ind &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">nrow</span>(sawyer_dtm), <span class="kw">floor</span>(<span class="fl">0.75</span><span class="op">*</span><span class="kw">nrow</span>(sawyer_dtm)))</span>
<span id="cb8-3"><a href="topic-modeling-chapter.html#cb8-3"></a>sawyer_train &lt;-<span class="st"> </span>sawyer_dtm[train_ind, ]</span>
<span id="cb8-4"><a href="topic-modeling-chapter.html#cb8-4"></a>sawyer_test &lt;-<span class="st"> </span>sawyer_dtm[<span class="op">-</span>train_ind, ]</span>
<span id="cb8-5"><a href="topic-modeling-chapter.html#cb8-5"></a></span>
<span id="cb8-6"><a href="topic-modeling-chapter.html#cb8-6"></a>k =<span class="st"> </span><span class="kw">c</span>(<span class="kw">seq</span>(<span class="dt">from =</span> <span class="dv">5</span>, <span class="dt">to =</span> <span class="dv">50</span>, <span class="dt">by =</span> <span class="dv">5</span>))</span>
<span id="cb8-7"><a href="topic-modeling-chapter.html#cb8-7"></a>perp &lt;-<span class="st"> </span>k <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb8-8"><a href="topic-modeling-chapter.html#cb8-8"></a><span class="st">  </span><span class="kw">map</span>(<span class="op">~</span><span class="st"> </span><span class="kw">LDA</span>(sawyer_train, <span class="dt">k =</span> .x, <span class="dt">method =</span> <span class="st">&quot;Gibbs&quot;</span>, <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">seed =</span> <span class="dv">1234</span>))) <span class="op">%&gt;%</span></span>
<span id="cb8-9"><a href="topic-modeling-chapter.html#cb8-9"></a><span class="st">  </span><span class="kw">map</span>(<span class="op">~</span><span class="st"> </span><span class="kw">perplexity</span>(.x, <span class="dt">newdata =</span> sawyer_test)) <span class="op">%&gt;%</span></span>
<span id="cb8-10"><a href="topic-modeling-chapter.html#cb8-10"></a><span class="st">  </span><span class="kw">as.numeric</span>()</span>
<span id="cb8-11"><a href="topic-modeling-chapter.html#cb8-11"></a></span>
<span id="cb8-12"><a href="topic-modeling-chapter.html#cb8-12"></a><span class="kw">data.frame</span>(<span class="dt">k =</span> k, <span class="dt">perplexity =</span> perp) <span class="op">%&gt;%</span></span>
<span id="cb8-13"><a href="topic-modeling-chapter.html#cb8-13"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> k, <span class="dt">y =</span> perplexity)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb8-14"><a href="topic-modeling-chapter.html#cb8-14"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span></span>
<span id="cb8-15"><a href="topic-modeling-chapter.html#cb8-15"></a><span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;loess&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>) <span class="op">+</span></span>
<span id="cb8-16"><a href="topic-modeling-chapter.html#cb8-16"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Perplexity Plot for LDM model&quot;</span>,</span>
<span id="cb8-17"><a href="topic-modeling-chapter.html#cb8-17"></a>       <span class="dt">subtitle =</span> <span class="st">&quot;Elbow at k = 30?&quot;</span>)</span></code></pre></div>
<p><img src="text-book_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>There may be an elbow at <em>k</em> = 15 topics. Using STM, the optimal number of clusters was 30. That looks like an elbow to me too. Frankly, if my arm looked like that I would consult an orthopedic surgeon.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="topic-modeling-chapter.html#cb9-1"></a>sawyer_lda &lt;-<span class="st"> </span>topicmodels<span class="op">::</span><span class="kw">LDA</span>(sawyer_dtm, <span class="dt">k =</span> <span class="dv">30</span>, <span class="dt">method =</span> <span class="st">&quot;Gibbs&quot;</span>, <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">seed =</span> <span class="dv">1234</span>))</span></code></pre></div>
<p><code>LDA()</code> returns two outputs: a “beta” matrix of probabilities of terms belonging to topics; a “gamma” matrix of probabilities of topics contributing to documents. The tidytext package provides a <code>tidy()</code> method for extracting these matrices.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="topic-modeling-chapter.html#cb10-1"></a>sawyer_lda_beta &lt;-<span class="st"> </span><span class="kw">tidy</span>(sawyer_lda, <span class="dt">matrix =</span> <span class="st">&quot;beta&quot;</span>) </span>
<span id="cb10-2"><a href="topic-modeling-chapter.html#cb10-2"></a>sawyer_lda_gamma &lt;-<span class="st"> </span><span class="kw">tidy</span>(sawyer_lda, <span class="dt">matrix =</span> <span class="st">&quot;gamma&quot;</span>, <span class="dt">document_names =</span> <span class="kw">rownames</span>(sawyer_dfm)) </span>
<span id="cb10-3"><a href="topic-modeling-chapter.html#cb10-3"></a></span>
<span id="cb10-4"><a href="topic-modeling-chapter.html#cb10-4"></a><span class="co"># All sums equal 1</span></span>
<span id="cb10-5"><a href="topic-modeling-chapter.html#cb10-5"></a><span class="co"># sawyer_lda_beta %&gt;% group_by(topic) %&gt;% summarize(.groups = &quot;drop&quot;, sum_beta = sum(beta))</span></span>
<span id="cb10-6"><a href="topic-modeling-chapter.html#cb10-6"></a><span class="co"># sawyer_lda_gamma %&gt;% group_by(document) %&gt;% summarize(.groups = &quot;drop&quot;, sum_beta = sum(gamma))</span></span></code></pre></div>
<p>I have 30 topics here, so it would be hard to show the top words per topic, but here are the first six topics.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="topic-modeling-chapter.html#cb11-1"></a>sawyer_lda_beta <span class="op">%&gt;%</span></span>
<span id="cb11-2"><a href="topic-modeling-chapter.html#cb11-2"></a><span class="st">  </span><span class="kw">filter</span>(topic <span class="op">&lt;=</span><span class="st"> </span><span class="dv">6</span>) <span class="op">%&gt;%</span></span>
<span id="cb11-3"><a href="topic-modeling-chapter.html#cb11-3"></a><span class="st">  </span><span class="kw">group_by</span>(topic) <span class="op">%&gt;%</span></span>
<span id="cb11-4"><a href="topic-modeling-chapter.html#cb11-4"></a><span class="st">  </span><span class="kw">slice_max</span>(<span class="dt">order_by =</span> beta, <span class="dt">n =</span> <span class="dv">10</span>, <span class="dt">with_ties =</span> <span class="ot">FALSE</span>) <span class="op">%&gt;%</span></span>
<span id="cb11-5"><a href="topic-modeling-chapter.html#cb11-5"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">term =</span> <span class="kw">reorder_within</span>(term, beta, topic)) <span class="op">%&gt;%</span></span>
<span id="cb11-6"><a href="topic-modeling-chapter.html#cb11-6"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> term, <span class="dt">y =</span> beta, <span class="dt">fill =</span> <span class="kw">factor</span>(topic))) <span class="op">+</span></span>
<span id="cb11-7"><a href="topic-modeling-chapter.html#cb11-7"></a><span class="st">  </span><span class="kw">geom_col</span>(<span class="dt">show.legend =</span> <span class="ot">FALSE</span>) <span class="op">+</span></span>
<span id="cb11-8"><a href="topic-modeling-chapter.html#cb11-8"></a><span class="st">  </span><span class="kw">scale_fill_manual</span>(<span class="dt">values =</span> RColorBrewer<span class="op">::</span><span class="kw">brewer.pal</span>(<span class="dt">n =</span> <span class="dv">6</span>, <span class="dt">name =</span> <span class="st">&quot;Set2&quot;</span>), <span class="dt">name =</span> <span class="st">&quot;Topic&quot;</span>) <span class="op">+</span></span>
<span id="cb11-9"><a href="topic-modeling-chapter.html#cb11-9"></a><span class="st">  </span><span class="kw">scale_x_reordered</span>() <span class="op">+</span></span>
<span id="cb11-10"><a href="topic-modeling-chapter.html#cb11-10"></a><span class="st">  </span><span class="kw">coord_flip</span>() <span class="op">+</span></span>
<span id="cb11-11"><a href="topic-modeling-chapter.html#cb11-11"></a><span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span><span class="st"> </span>topic, <span class="dt">scales =</span> <span class="st">&quot;free&quot;</span>) <span class="op">+</span></span>
<span id="cb11-12"><a href="topic-modeling-chapter.html#cb11-12"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;LDA Beta Matrix for first six topics&quot;</span>,</span>
<span id="cb11-13"><a href="topic-modeling-chapter.html#cb11-13"></a>       <span class="dt">subtitle =</span> <span class="st">&quot;Showing top 10 word probabilities&quot;</span>)</span></code></pre></div>
<p><img src="text-book_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>You can use a Word cloud as an alternative to the bar chart. The advantage of the word cloud is that it creates an instantaneous impression rather than providing you the analytic means to construct the same conclusion. Here are word clouds for the first two topics.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="topic-modeling-chapter.html#cb12-1"></a>colors6 &lt;-<span class="st"> </span>RColorBrewer<span class="op">::</span><span class="kw">brewer.pal</span>(<span class="dt">n =</span> <span class="dv">6</span>, <span class="dt">name =</span> <span class="st">&quot;Set2&quot;</span>)</span>
<span id="cb12-2"><a href="topic-modeling-chapter.html#cb12-2"></a>x &lt;-<span class="st"> </span><span class="kw">map</span>(<span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>), <span class="op">~</span><span class="st"> </span><span class="kw">with</span>(sawyer_lda_beta <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(topic <span class="op">==</span><span class="st"> </span>.x), </span>
<span id="cb12-3"><a href="topic-modeling-chapter.html#cb12-3"></a>                        wordcloud<span class="op">::</span><span class="kw">wordcloud</span>(term, beta, <span class="dt">max.words =</span> <span class="dv">20</span>,</span>
<span id="cb12-4"><a href="topic-modeling-chapter.html#cb12-4"></a>                                             <span class="dt">colors =</span> colors6[.x])))</span></code></pre></div>
<p><img src="text-book_files/figure-html/unnamed-chunk-11-1.png" width="672" /><img src="text-book_files/figure-html/unnamed-chunk-11-2.png" width="672" /></p>
<p>And here are the most prevalent topics across chapters. The figure shows the average probability that the topic appears in the chapter for all chapters.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="topic-modeling-chapter.html#cb13-1"></a>top_terms &lt;-<span class="st"> </span>sawyer_lda_beta <span class="op">%&gt;%</span></span>
<span id="cb13-2"><a href="topic-modeling-chapter.html#cb13-2"></a><span class="st">  </span><span class="kw">group_by</span>(topic) <span class="op">%&gt;%</span></span>
<span id="cb13-3"><a href="topic-modeling-chapter.html#cb13-3"></a><span class="st">  </span><span class="kw">slice_max</span>(<span class="dt">order_by =</span> beta, <span class="dt">n =</span> <span class="dv">7</span>) <span class="op">%&gt;%</span></span>
<span id="cb13-4"><a href="topic-modeling-chapter.html#cb13-4"></a><span class="st">  </span><span class="kw">summarise</span>(<span class="dt">.groups =</span> <span class="st">&quot;drop&quot;</span>, <span class="dt">terms =</span> <span class="kw">list</span>(term)) <span class="op">%&gt;%</span></span>
<span id="cb13-5"><a href="topic-modeling-chapter.html#cb13-5"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">terms =</span> <span class="kw">map</span>(terms, paste, <span class="dt">collapse =</span> <span class="st">&quot;, &quot;</span>)) <span class="op">%&gt;%</span></span>
<span id="cb13-6"><a href="topic-modeling-chapter.html#cb13-6"></a><span class="st">  </span><span class="kw">unnest</span>(terms)</span>
<span id="cb13-7"><a href="topic-modeling-chapter.html#cb13-7"></a></span>
<span id="cb13-8"><a href="topic-modeling-chapter.html#cb13-8"></a>sawyer_lda_gamma <span class="op">%&gt;%</span></span>
<span id="cb13-9"><a href="topic-modeling-chapter.html#cb13-9"></a><span class="st">  </span><span class="kw">group_by</span>(topic) <span class="op">%&gt;%</span></span>
<span id="cb13-10"><a href="topic-modeling-chapter.html#cb13-10"></a><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">.groups =</span> <span class="st">&quot;drop&quot;</span>, <span class="dt">gamma =</span> <span class="kw">mean</span>(gamma)) <span class="op">%&gt;%</span></span>
<span id="cb13-11"><a href="topic-modeling-chapter.html#cb13-11"></a><span class="st">  </span><span class="kw">left_join</span>(top_terms, <span class="dt">by =</span> <span class="st">&quot;topic&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb13-12"><a href="topic-modeling-chapter.html#cb13-12"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">topic =</span> <span class="kw">paste</span>(<span class="st">&quot;topic&quot;</span>, topic),</span>
<span id="cb13-13"><a href="topic-modeling-chapter.html#cb13-13"></a>         <span class="dt">topic =</span> <span class="kw">fct_reorder</span>(topic, gamma)) <span class="op">%&gt;%</span></span>
<span id="cb13-14"><a href="topic-modeling-chapter.html#cb13-14"></a><span class="st">  </span><span class="kw">slice_max</span>(<span class="dt">order_by =</span> gamma, <span class="dt">n =</span> <span class="dv">10</span>) <span class="op">%&gt;%</span></span>
<span id="cb13-15"><a href="topic-modeling-chapter.html#cb13-15"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> topic, <span class="dt">y =</span> gamma, <span class="dt">label =</span> terms)) <span class="op">+</span></span>
<span id="cb13-16"><a href="topic-modeling-chapter.html#cb13-16"></a><span class="st">  </span><span class="kw">geom_col</span>(<span class="dt">fill =</span> <span class="st">&quot;#D8A7B1&quot;</span>, <span class="dt">show.legend =</span> <span class="ot">FALSE</span>) <span class="op">+</span></span>
<span id="cb13-17"><a href="topic-modeling-chapter.html#cb13-17"></a><span class="st">  </span><span class="kw">geom_text</span>(<span class="dt">hjust =</span> <span class="dv">0</span>, <span class="dt">nudge_y =</span> <span class="fl">0.0005</span>, <span class="dt">size =</span> <span class="dv">3</span>) <span class="op">+</span></span>
<span id="cb13-18"><a href="topic-modeling-chapter.html#cb13-18"></a><span class="st">  </span><span class="kw">coord_flip</span>() <span class="op">+</span></span>
<span id="cb13-19"><a href="topic-modeling-chapter.html#cb13-19"></a><span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">expand =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>),</span>
<span id="cb13-20"><a href="topic-modeling-chapter.html#cb13-20"></a>                     <span class="dt">limits =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">0.09</span>),</span>
<span id="cb13-21"><a href="topic-modeling-chapter.html#cb13-21"></a>                     <span class="dt">labels =</span> scales<span class="op">::</span><span class="kw">percent_format</span>()) <span class="op">+</span></span>
<span id="cb13-22"><a href="topic-modeling-chapter.html#cb13-22"></a><span class="st">  </span><span class="kw">theme_minimal</span>() <span class="op">+</span></span>
<span id="cb13-23"><a href="topic-modeling-chapter.html#cb13-23"></a><span class="st">  </span><span class="kw">theme</span>(<span class="dt">panel.grid =</span> <span class="kw">element_blank</span>()) <span class="op">+</span></span>
<span id="cb13-24"><a href="topic-modeling-chapter.html#cb13-24"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="ot">NULL</span>, <span class="dt">y =</span> <span class="kw">expression</span>(gamma),</span>
<span id="cb13-25"><a href="topic-modeling-chapter.html#cb13-25"></a>       <span class="dt">title =</span> <span class="st">&quot;Top 10 LDA topics by prevalence in The Adentures of Tom Sawyer&quot;</span>,</span>
<span id="cb13-26"><a href="topic-modeling-chapter.html#cb13-26"></a>       <span class="dt">subtitle =</span> <span class="st">&quot;With top words in each topic&quot;</span>)</span></code></pre></div>
<p><img src="text-book_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>Another way to look at the betas is to identify terms that had the greatest difference in beta between the first and second most probable topic. A good way to do this is with their log ratio, <span class="math inline">\(log_2(\beta_2 / \beta_1)\)</span>. Filter for relatively common words having a beta greater than 1/100 in at least one topic.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="topic-modeling-chapter.html#cb14-1"></a>sawyer_lda_beta <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb14-2"><a href="topic-modeling-chapter.html#cb14-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">topic =</span> <span class="kw">paste0</span>(<span class="st">&quot;topic&quot;</span>, topic)) <span class="op">%&gt;%</span></span>
<span id="cb14-3"><a href="topic-modeling-chapter.html#cb14-3"></a><span class="st">  </span><span class="kw">group_by</span>(term) <span class="op">%&gt;%</span></span>
<span id="cb14-4"><a href="topic-modeling-chapter.html#cb14-4"></a><span class="st">  </span><span class="kw">slice_max</span>(<span class="dt">order_by =</span> beta, <span class="dt">n =</span> <span class="dv">2</span>) <span class="op">%&gt;%</span></span>
<span id="cb14-5"><a href="topic-modeling-chapter.html#cb14-5"></a><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">.groups =</span> <span class="st">&quot;drop&quot;</span>, <span class="dt">min_beta =</span> <span class="kw">min</span>(beta), <span class="dt">max_beta =</span> <span class="kw">max</span>(beta)) <span class="op">%&gt;%</span></span>
<span id="cb14-6"><a href="topic-modeling-chapter.html#cb14-6"></a><span class="st">  </span><span class="kw">filter</span>(max_beta <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.01</span>) <span class="op">%&gt;%</span></span>
<span id="cb14-7"><a href="topic-modeling-chapter.html#cb14-7"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">log_ratio =</span> <span class="kw">log2</span>(max_beta <span class="op">/</span><span class="st"> </span>min_beta)) <span class="op">%&gt;%</span></span>
<span id="cb14-8"><a href="topic-modeling-chapter.html#cb14-8"></a><span class="st">  </span><span class="kw">top_n</span>(<span class="dt">n =</span> <span class="dv">20</span>, <span class="dt">w =</span> <span class="kw">abs</span>(log_ratio)) <span class="op">%&gt;%</span></span>
<span id="cb14-9"><a href="topic-modeling-chapter.html#cb14-9"></a><span class="st">  </span><span class="kw">arrange</span>(<span class="op">-</span>log_ratio) <span class="op">%&gt;%</span></span>
<span id="cb14-10"><a href="topic-modeling-chapter.html#cb14-10"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> <span class="kw">fct_rev</span>(<span class="kw">fct_inorder</span>(term)), <span class="dt">y =</span> log_ratio)) <span class="op">+</span></span>
<span id="cb14-11"><a href="topic-modeling-chapter.html#cb14-11"></a><span class="st">  </span><span class="kw">geom_col</span>(<span class="dt">fill =</span> <span class="st">&quot;#D8A7B1&quot;</span>) <span class="op">+</span></span>
<span id="cb14-12"><a href="topic-modeling-chapter.html#cb14-12"></a><span class="st">  </span><span class="kw">theme_minimal</span>() <span class="op">+</span></span>
<span id="cb14-13"><a href="topic-modeling-chapter.html#cb14-13"></a><span class="st">  </span><span class="kw">coord_flip</span>() <span class="op">+</span><span class="st"> </span></span>
<span id="cb14-14"><a href="topic-modeling-chapter.html#cb14-14"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;LDA beta matrix log ratios&quot;</span>,</span>
<span id="cb14-15"><a href="topic-modeling-chapter.html#cb14-15"></a>       <span class="dt">subtitle =</span> <span class="st">&quot;showing greatest differences in beta values&quot;</span>,</span>
<span id="cb14-16"><a href="topic-modeling-chapter.html#cb14-16"></a>       <span class="dt">x =</span> <span class="st">&quot;&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;log(beta ratio)&quot;</span>)</span></code></pre></div>
<p><img src="text-book_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
</div>
<div id="stm" class="section level3">
<h3><span class="header-section-number">2.1.2</span> STM</h3>
<p>Prepare the data by creating a document-term matrix, just as you would with LDA, except this time use the <code>cast_dfm()</code> to create a document <em>feature</em> matrix for the <strong>quanteda</strong> package (the <strong>stm</strong> package uses <strong>quanteda</strong>). As LDA, you will improve performance by removing infrequently appearing tokens, this time with <code>tm::removeSparseTemrs()</code>.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="topic-modeling-chapter.html#cb15-1"></a>sawyer_dfm &lt;-<span class="st"> </span>sawyer_tokens <span class="op">%&gt;%</span></span>
<span id="cb15-2"><a href="topic-modeling-chapter.html#cb15-2"></a><span class="st">  </span><span class="kw">count</span>(chapter, word_stem) <span class="op">%&gt;%</span></span>
<span id="cb15-3"><a href="topic-modeling-chapter.html#cb15-3"></a><span class="st">  </span><span class="kw">cast_dfm</span>(<span class="dt">document =</span> chapter, <span class="dt">term =</span> word_stem, <span class="dt">value =</span> n) <span class="op">%&gt;%</span></span>
<span id="cb15-4"><a href="topic-modeling-chapter.html#cb15-4"></a><span class="st">  </span>quanteda<span class="op">::</span><span class="kw">dfm_trim</span>(<span class="dt">min_docfreq =</span> <span class="fl">0.1</span>, <span class="dt">docfreq_type =</span> <span class="st">&quot;prop&quot;</span>)</span>
<span id="cb15-5"><a href="topic-modeling-chapter.html#cb15-5"></a></span>
<span id="cb15-6"><a href="topic-modeling-chapter.html#cb15-6"></a>sawyer_dfm</span></code></pre></div>
<pre><code>## Document-feature matrix of: 35 documents, 1,213 features (76.6% sparse).
##     features
## docs _do_ _got_ _is_ _will_ advantage adventure afraid afternoon age air
##    1    3     1    1      3         2         1      2         1   1   2
##    2    0     0    0      0         0         0      0         1   0   1
##    3    0     0    1      0         0         0      0         0   1   3
##    4    0     0    1      0         0         0      3         0   3   1
##    5    0     0    0      0         0         0      0         0   2   0
##    6    0     0    1      1         0         0      0         0   0   1
## [ reached max_ndoc ... 29 more documents, reached max_nfeat ... 1,203 more features ]</code></pre>
<p>Create a topic model with <code>stm::stm()</code>. Parameter <code>K</code> specifies the number of topics. In LDA I used perplexity to determine the optimal number of clusters, in part because that is the only optimizing metric it <code>LDA()</code> offers. <code>stm</code> offers other metrics, including the held-out likelihood and coherence. Here are models with 5 - 50 topics. This process can take a while, but the <strong>furrr</strong> package and <code>future_map()</code> function leverage parallel processing to make it quicker.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="topic-modeling-chapter.html#cb17-1"></a><span class="kw">library</span>(furrr)</span></code></pre></div>
<pre><code>## Warning: package &#39;furrr&#39; was built under R version 4.0.3</code></pre>
<pre><code>## Loading required package: future</code></pre>
<pre><code>## Warning: package &#39;future&#39; was built under R version 4.0.3</code></pre>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="topic-modeling-chapter.html#cb21-1"></a><span class="kw">plan</span>(multiprocess)</span></code></pre></div>
<pre><code>## Warning: Strategy &#39;multiprocess&#39; is deprecated in future (&gt;= 1.20.0). Instead,
## explicitly specify either &#39;multisession&#39; or &#39;multicore&#39;. In the current R
## session, &#39;multiprocess&#39; equals &#39;multisession&#39;.</code></pre>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="topic-modeling-chapter.html#cb23-1"></a>sawyer_stm_mdls &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">K =</span> <span class="kw">seq</span>(<span class="dt">from =</span> <span class="dv">5</span>, <span class="dt">to =</span> <span class="dv">50</span>, <span class="dt">by =</span> <span class="dv">5</span>)) <span class="op">%&gt;%</span></span>
<span id="cb23-2"><a href="topic-modeling-chapter.html#cb23-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">mdl =</span> <span class="kw">future_map</span>(K, <span class="op">~</span><span class="kw">stm</span>(sawyer_dfm, <span class="dt">K =</span> ., <span class="dt">verbose =</span> <span class="ot">FALSE</span>),</span>
<span id="cb23-3"><a href="topic-modeling-chapter.html#cb23-3"></a>                          <span class="dt">.options =</span> <span class="kw">furrr_options</span>(<span class="dt">seed =</span> <span class="dv">123</span>)))</span></code></pre></div>
<pre><code>## Warning: `data_frame()` is deprecated as of tibble 1.1.0.
## Please use `tibble()` instead.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_warnings()` to see where this warning was generated.</code></pre>
<pre><code>## Loading required package: quanteda</code></pre>
<pre><code>## Warning: Problem with `mutate()` input `mdl`.
## i package &#39;quanteda&#39; was built under R version 4.0.2
## i Input `mdl` is `future_map(K, ~stm(sawyer_dfm, K = ., verbose = FALSE), .options = furrr_options(seed = 123))`.</code></pre>
<pre><code>## Warning: package &#39;quanteda&#39; was built under R version 4.0.2</code></pre>
<pre><code>## Package version: 2.1.2</code></pre>
<pre><code>## Parallel computing: 2 of 8 threads used.</code></pre>
<pre><code>## See https://quanteda.io for tutorials and examples.</code></pre>
<pre><code>## 
## Attaching package: &#39;quanteda&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:utils&#39;:
## 
##     View</code></pre>
<pre><code>## Loading required package: quanteda</code></pre>
<pre><code>## Warning: Problem with `mutate()` input `mdl`.
## i package &#39;quanteda&#39; was built under R version 4.0.2
## i Input `mdl` is `future_map(K, ~stm(sawyer_dfm, K = ., verbose = FALSE), .options = furrr_options(seed = 123))`.

## Warning: package &#39;quanteda&#39; was built under R version 4.0.2</code></pre>
<pre><code>## Package version: 2.1.2</code></pre>
<pre><code>## Parallel computing: 2 of 8 threads used.</code></pre>
<pre><code>## See https://quanteda.io for tutorials and examples.</code></pre>
<pre><code>## 
## Attaching package: &#39;quanteda&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:utils&#39;:
## 
##     View</code></pre>
<pre><code>## Loading required package: quanteda</code></pre>
<pre><code>## Warning: Problem with `mutate()` input `mdl`.
## i package &#39;quanteda&#39; was built under R version 4.0.2
## i Input `mdl` is `future_map(K, ~stm(sawyer_dfm, K = ., verbose = FALSE), .options = furrr_options(seed = 123))`.

## Warning: package &#39;quanteda&#39; was built under R version 4.0.2</code></pre>
<pre><code>## Package version: 2.1.2</code></pre>
<pre><code>## Parallel computing: 2 of 8 threads used.</code></pre>
<pre><code>## See https://quanteda.io for tutorials and examples.</code></pre>
<pre><code>## 
## Attaching package: &#39;quanteda&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:utils&#39;:
## 
##     View</code></pre>
<pre><code>## Loading required package: quanteda</code></pre>
<pre><code>## Warning: Problem with `mutate()` input `mdl`.
## i package &#39;quanteda&#39; was built under R version 4.0.2
## i Input `mdl` is `future_map(K, ~stm(sawyer_dfm, K = ., verbose = FALSE), .options = furrr_options(seed = 123))`.

## Warning: package &#39;quanteda&#39; was built under R version 4.0.2</code></pre>
<pre><code>## Package version: 2.1.2</code></pre>
<pre><code>## Parallel computing: 2 of 8 threads used.</code></pre>
<pre><code>## See https://quanteda.io for tutorials and examples.</code></pre>
<pre><code>## 
## Attaching package: &#39;quanteda&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:utils&#39;:
## 
##     View</code></pre>
<pre><code>## Loading required package: quanteda</code></pre>
<pre><code>## Warning: Problem with `mutate()` input `mdl`.
## i package &#39;quanteda&#39; was built under R version 4.0.2
## i Input `mdl` is `future_map(K, ~stm(sawyer_dfm, K = ., verbose = FALSE), .options = furrr_options(seed = 123))`.

## Warning: package &#39;quanteda&#39; was built under R version 4.0.2</code></pre>
<pre><code>## Package version: 2.1.2</code></pre>
<pre><code>## Parallel computing: 2 of 8 threads used.</code></pre>
<pre><code>## See https://quanteda.io for tutorials and examples.</code></pre>
<pre><code>## 
## Attaching package: &#39;quanteda&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:utils&#39;:
## 
##     View</code></pre>
<pre><code>## Loading required package: quanteda</code></pre>
<pre><code>## Warning: Problem with `mutate()` input `mdl`.
## i package &#39;quanteda&#39; was built under R version 4.0.2
## i Input `mdl` is `future_map(K, ~stm(sawyer_dfm, K = ., verbose = FALSE), .options = furrr_options(seed = 123))`.

## Warning: package &#39;quanteda&#39; was built under R version 4.0.2</code></pre>
<pre><code>## Package version: 2.1.2</code></pre>
<pre><code>## Parallel computing: 2 of 8 threads used.</code></pre>
<pre><code>## See https://quanteda.io for tutorials and examples.</code></pre>
<pre><code>## 
## Attaching package: &#39;quanteda&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:utils&#39;:
## 
##     View</code></pre>
<pre><code>## Loading required package: quanteda</code></pre>
<pre><code>## Warning: Problem with `mutate()` input `mdl`.
## i package &#39;quanteda&#39; was built under R version 4.0.2
## i Input `mdl` is `future_map(K, ~stm(sawyer_dfm, K = ., verbose = FALSE), .options = furrr_options(seed = 123))`.

## Warning: package &#39;quanteda&#39; was built under R version 4.0.2</code></pre>
<pre><code>## Package version: 2.1.2</code></pre>
<pre><code>## Parallel computing: 2 of 8 threads used.</code></pre>
<pre><code>## See https://quanteda.io for tutorials and examples.</code></pre>
<pre><code>## 
## Attaching package: &#39;quanteda&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:utils&#39;:
## 
##     View</code></pre>
<pre><code>## Loading required package: quanteda</code></pre>
<pre><code>## Warning: Problem with `mutate()` input `mdl`.
## i package &#39;quanteda&#39; was built under R version 4.0.2
## i Input `mdl` is `future_map(K, ~stm(sawyer_dfm, K = ., verbose = FALSE), .options = furrr_options(seed = 123))`.

## Warning: package &#39;quanteda&#39; was built under R version 4.0.2</code></pre>
<pre><code>## Package version: 2.1.2</code></pre>
<pre><code>## Parallel computing: 2 of 8 threads used.</code></pre>
<pre><code>## See https://quanteda.io for tutorials and examples.</code></pre>
<pre><code>## 
## Attaching package: &#39;quanteda&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:utils&#39;:
## 
##     View</code></pre>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="topic-modeling-chapter.html#cb82-1"></a>heldout &lt;-<span class="st"> </span><span class="kw">make.heldout</span>(sawyer_dfm)</span>
<span id="cb82-2"><a href="topic-modeling-chapter.html#cb82-2"></a></span>
<span id="cb82-3"><a href="topic-modeling-chapter.html#cb82-3"></a>k_result &lt;-<span class="st"> </span>sawyer_stm_mdls <span class="op">%&gt;%</span></span>
<span id="cb82-4"><a href="topic-modeling-chapter.html#cb82-4"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">exclusivity        =</span> <span class="kw">map</span>(mdl, exclusivity),</span>
<span id="cb82-5"><a href="topic-modeling-chapter.html#cb82-5"></a>         <span class="dt">semantic_coherence =</span> <span class="kw">map</span>(mdl, semanticCoherence, sawyer_dfm),</span>
<span id="cb82-6"><a href="topic-modeling-chapter.html#cb82-6"></a>         <span class="dt">eval_heldout       =</span> <span class="kw">map</span>(mdl, eval.heldout, heldout<span class="op">$</span>missing),</span>
<span id="cb82-7"><a href="topic-modeling-chapter.html#cb82-7"></a>         <span class="dt">residual           =</span> <span class="kw">map</span>(mdl, checkResiduals, sawyer_dfm),</span>
<span id="cb82-8"><a href="topic-modeling-chapter.html#cb82-8"></a>         <span class="dt">bound              =</span> <span class="kw">map_dbl</span>(mdl, <span class="op">~</span><span class="kw">max</span>(.<span class="op">$</span>convergence<span class="op">$</span>bound)),</span>
<span id="cb82-9"><a href="topic-modeling-chapter.html#cb82-9"></a>         <span class="dt">lfact              =</span> <span class="kw">map_dbl</span>(mdl, <span class="op">~</span><span class="kw">lfactorial</span>(.<span class="op">$</span>settings<span class="op">$</span>dim<span class="op">$</span>K)),</span>
<span id="cb82-10"><a href="topic-modeling-chapter.html#cb82-10"></a>         <span class="dt">lbound             =</span> bound <span class="op">+</span><span class="st"> </span>lfact,</span>
<span id="cb82-11"><a href="topic-modeling-chapter.html#cb82-11"></a>         <span class="dt">iterations         =</span> <span class="kw">map_dbl</span>(mdl, <span class="op">~</span><span class="kw">length</span>(.<span class="op">$</span>convergence<span class="op">$</span>bound)))</span>
<span id="cb82-12"><a href="topic-modeling-chapter.html#cb82-12"></a></span>
<span id="cb82-13"><a href="topic-modeling-chapter.html#cb82-13"></a>k_result <span class="op">%&gt;%</span></span>
<span id="cb82-14"><a href="topic-modeling-chapter.html#cb82-14"></a><span class="st">  </span><span class="kw">transmute</span>(K,</span>
<span id="cb82-15"><a href="topic-modeling-chapter.html#cb82-15"></a>            <span class="st">`</span><span class="dt">Lower bound</span><span class="st">`</span> =<span class="st"> </span>lbound,</span>
<span id="cb82-16"><a href="topic-modeling-chapter.html#cb82-16"></a>            <span class="dt">Residuals =</span> <span class="kw">map_dbl</span>(residual, <span class="st">&quot;dispersion&quot;</span>),</span>
<span id="cb82-17"><a href="topic-modeling-chapter.html#cb82-17"></a>            <span class="st">`</span><span class="dt">Semantic coherence</span><span class="st">`</span> =<span class="st"> </span><span class="kw">map_dbl</span>(semantic_coherence, mean),</span>
<span id="cb82-18"><a href="topic-modeling-chapter.html#cb82-18"></a>            <span class="st">`</span><span class="dt">Held-out likelihood</span><span class="st">`</span> =<span class="st"> </span><span class="kw">map_dbl</span>(eval_heldout, <span class="st">&quot;expected.heldout&quot;</span>)) <span class="op">%&gt;%</span></span>
<span id="cb82-19"><a href="topic-modeling-chapter.html#cb82-19"></a><span class="st">  </span><span class="kw">gather</span>(Metric, Value, <span class="op">-</span>K) <span class="op">%&gt;%</span></span>
<span id="cb82-20"><a href="topic-modeling-chapter.html#cb82-20"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(K, Value, <span class="dt">color =</span> Metric)) <span class="op">+</span></span>
<span id="cb82-21"><a href="topic-modeling-chapter.html#cb82-21"></a><span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">size =</span> <span class="fl">1.5</span>, <span class="dt">alpha =</span> <span class="fl">0.7</span>, <span class="dt">show.legend =</span> <span class="ot">FALSE</span>) <span class="op">+</span></span>
<span id="cb82-22"><a href="topic-modeling-chapter.html#cb82-22"></a><span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>Metric, <span class="dt">scales =</span> <span class="st">&quot;free_y&quot;</span>) <span class="op">+</span></span>
<span id="cb82-23"><a href="topic-modeling-chapter.html#cb82-23"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;K (number of topics)&quot;</span>,</span>
<span id="cb82-24"><a href="topic-modeling-chapter.html#cb82-24"></a>       <span class="dt">y =</span> <span class="ot">NULL</span>,</span>
<span id="cb82-25"><a href="topic-modeling-chapter.html#cb82-25"></a>       <span class="dt">title =</span> <span class="st">&quot;Model diagnostics by number of topics&quot;</span>,</span>
<span id="cb82-26"><a href="topic-modeling-chapter.html#cb82-26"></a>       <span class="dt">subtitle =</span> <span class="st">&quot;These diagnostics indicate that a good number of topics would be around 60&quot;</span>)</span></code></pre></div>
<p><img src="text-book_files/figure-html/unnamed-chunk-15-1.png" width="672" />
The held-out likelihood is highest between 30 and 50, and the residuals are lowest at 20, so 30 might be the right number. Semantic coherence is maximized when the most probable words in a given topic frequently co-occur together. Coherence tends to fall as exclusivity increases. You’ll want the topic size that balances the trade-off.</p>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb83-1"><a href="topic-modeling-chapter.html#cb83-1"></a>k_result <span class="op">%&gt;%</span></span>
<span id="cb83-2"><a href="topic-modeling-chapter.html#cb83-2"></a><span class="st">  </span><span class="kw">select</span>(K, exclusivity, semantic_coherence) <span class="op">%&gt;%</span></span>
<span id="cb83-3"><a href="topic-modeling-chapter.html#cb83-3"></a><span class="st">  </span><span class="kw">filter</span>(K <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">20</span>, <span class="dv">25</span>, <span class="dv">30</span>)) <span class="op">%&gt;%</span></span>
<span id="cb83-4"><a href="topic-modeling-chapter.html#cb83-4"></a><span class="st">  </span><span class="kw">unnest</span>(<span class="dt">cols =</span> <span class="kw">c</span>(exclusivity, semantic_coherence)) <span class="op">%&gt;%</span></span>
<span id="cb83-5"><a href="topic-modeling-chapter.html#cb83-5"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">K =</span> <span class="kw">as.factor</span>(K)) <span class="op">%&gt;%</span></span>
<span id="cb83-6"><a href="topic-modeling-chapter.html#cb83-6"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(semantic_coherence, exclusivity, <span class="dt">color =</span> K)) <span class="op">+</span></span>
<span id="cb83-7"><a href="topic-modeling-chapter.html#cb83-7"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size =</span> <span class="dv">2</span>, <span class="dt">alpha =</span> <span class="fl">0.7</span>) <span class="op">+</span></span>
<span id="cb83-8"><a href="topic-modeling-chapter.html#cb83-8"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Semantic coherence&quot;</span>,</span>
<span id="cb83-9"><a href="topic-modeling-chapter.html#cb83-9"></a>       <span class="dt">y =</span> <span class="st">&quot;Exclusivity&quot;</span>,</span>
<span id="cb83-10"><a href="topic-modeling-chapter.html#cb83-10"></a>       <span class="dt">title =</span> <span class="st">&quot;Comparing exclusivity and semantic coherence&quot;</span>,</span>
<span id="cb83-11"><a href="topic-modeling-chapter.html#cb83-11"></a>       <span class="dt">subtitle =</span> <span class="st">&quot;Models with fewer topics have higher semantic coherence for more topics, but lower exclusivity&quot;</span>)</span></code></pre></div>
<p><img src="text-book_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>It looks like <em>k</em> = 30 may be optimal.</p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="topic-modeling-chapter.html#cb84-1"></a>sawyer_stm &lt;-<span class="st"> </span>sawyer_stm_mdls <span class="op">%&gt;%</span></span>
<span id="cb84-2"><a href="topic-modeling-chapter.html#cb84-2"></a><span class="st">  </span><span class="kw">filter</span>(K <span class="op">==</span><span class="st"> </span><span class="dv">30</span>) <span class="op">%&gt;%</span></span>
<span id="cb84-3"><a href="topic-modeling-chapter.html#cb84-3"></a><span class="st">  </span><span class="kw">pull</span>(mdl) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb84-4"><a href="topic-modeling-chapter.html#cb84-4"></a><span class="st">  </span><span class="kw">pluck</span>(<span class="dv">1</span>)</span></code></pre></div>
<p>Like <code>LDA()</code>, <code>stm()</code> returns two outputs: a “beta” matrix of probabilities of terms belonging to topics; a “gamma” matrix of probabilities of topics contributing to documents. The tidytext package provides a <code>tidy()</code> method for extracting these matrices.</p>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb85-1"><a href="topic-modeling-chapter.html#cb85-1"></a>sawyer_stm_beta &lt;-<span class="st"> </span><span class="kw">tidy</span>(sawyer_stm, <span class="dt">matrix =</span> <span class="st">&quot;beta&quot;</span>) </span>
<span id="cb85-2"><a href="topic-modeling-chapter.html#cb85-2"></a>sawyer_stm_gamma &lt;-<span class="st"> </span><span class="kw">tidy</span>(sawyer_stm, <span class="dt">matrix =</span> <span class="st">&quot;gamma&quot;</span>, <span class="dt">document_names =</span> <span class="kw">rownames</span>(sawyer_dfm)) </span></code></pre></div>
<p>I have 30 topics here, so it would be hard to show the top words per topic, but here are the first six topics.</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="topic-modeling-chapter.html#cb86-1"></a>sawyer_stm_beta <span class="op">%&gt;%</span></span>
<span id="cb86-2"><a href="topic-modeling-chapter.html#cb86-2"></a><span class="st">  </span><span class="kw">filter</span>(topic <span class="op">&lt;=</span><span class="st"> </span><span class="dv">6</span>) <span class="op">%&gt;%</span></span>
<span id="cb86-3"><a href="topic-modeling-chapter.html#cb86-3"></a><span class="st">  </span><span class="kw">group_by</span>(topic) <span class="op">%&gt;%</span></span>
<span id="cb86-4"><a href="topic-modeling-chapter.html#cb86-4"></a><span class="st">  </span><span class="kw">slice_max</span>(<span class="dt">order_by =</span> beta, <span class="dt">n =</span> <span class="dv">10</span>, <span class="dt">with_ties =</span> <span class="ot">FALSE</span>) <span class="op">%&gt;%</span></span>
<span id="cb86-5"><a href="topic-modeling-chapter.html#cb86-5"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">term =</span> <span class="kw">reorder_within</span>(term, beta, topic)) <span class="op">%&gt;%</span></span>
<span id="cb86-6"><a href="topic-modeling-chapter.html#cb86-6"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> term, <span class="dt">y =</span> beta, <span class="dt">fill =</span> <span class="kw">factor</span>(topic))) <span class="op">+</span></span>
<span id="cb86-7"><a href="topic-modeling-chapter.html#cb86-7"></a><span class="st">  </span><span class="kw">geom_col</span>(<span class="dt">show.legend =</span> <span class="ot">FALSE</span>) <span class="op">+</span></span>
<span id="cb86-8"><a href="topic-modeling-chapter.html#cb86-8"></a><span class="st">  </span><span class="kw">scale_fill_manual</span>(<span class="dt">values =</span> RColorBrewer<span class="op">::</span><span class="kw">brewer.pal</span>(<span class="dt">n =</span> <span class="dv">6</span>, <span class="dt">name =</span> <span class="st">&quot;Set2&quot;</span>), <span class="dt">name =</span> <span class="st">&quot;Topic&quot;</span>) <span class="op">+</span></span>
<span id="cb86-9"><a href="topic-modeling-chapter.html#cb86-9"></a><span class="st">  </span><span class="kw">scale_x_reordered</span>() <span class="op">+</span></span>
<span id="cb86-10"><a href="topic-modeling-chapter.html#cb86-10"></a><span class="st">  </span><span class="kw">coord_flip</span>() <span class="op">+</span></span>
<span id="cb86-11"><a href="topic-modeling-chapter.html#cb86-11"></a><span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span><span class="st"> </span>topic, <span class="dt">scales =</span> <span class="st">&quot;free&quot;</span>) <span class="op">+</span></span>
<span id="cb86-12"><a href="topic-modeling-chapter.html#cb86-12"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;STM Beta Matrix for first six topics&quot;</span>,</span>
<span id="cb86-13"><a href="topic-modeling-chapter.html#cb86-13"></a>       <span class="dt">subtitle =</span> <span class="st">&quot;Showing top 10 word probabilities&quot;</span>)</span></code></pre></div>
<p><img src="text-book_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>Here is a Word cloud representation.</p>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="topic-modeling-chapter.html#cb87-1"></a>colors6 &lt;-<span class="st"> </span>RColorBrewer<span class="op">::</span><span class="kw">brewer.pal</span>(<span class="dt">n =</span> <span class="dv">6</span>, <span class="dt">name =</span> <span class="st">&quot;Set2&quot;</span>)</span>
<span id="cb87-2"><a href="topic-modeling-chapter.html#cb87-2"></a>x &lt;-<span class="st"> </span><span class="kw">map</span>(<span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>), <span class="op">~</span><span class="st"> </span><span class="kw">with</span>(sawyer_stm_beta <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(topic <span class="op">==</span><span class="st"> </span>.x), </span>
<span id="cb87-3"><a href="topic-modeling-chapter.html#cb87-3"></a>                        wordcloud<span class="op">::</span><span class="kw">wordcloud</span>(term, beta, <span class="dt">max.words =</span> <span class="dv">20</span>,</span>
<span id="cb87-4"><a href="topic-modeling-chapter.html#cb87-4"></a>                                             <span class="dt">colors =</span> colors6[.x])))</span></code></pre></div>
<p><img src="text-book_files/figure-html/unnamed-chunk-20-1.png" width="672" /><img src="text-book_files/figure-html/unnamed-chunk-20-2.png" width="672" /></p>
<p>And here are the most prevalent topics across chapters.</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb88-1"><a href="topic-modeling-chapter.html#cb88-1"></a>top_terms &lt;-<span class="st"> </span>sawyer_stm_beta <span class="op">%&gt;%</span></span>
<span id="cb88-2"><a href="topic-modeling-chapter.html#cb88-2"></a><span class="st">  </span><span class="kw">group_by</span>(topic) <span class="op">%&gt;%</span></span>
<span id="cb88-3"><a href="topic-modeling-chapter.html#cb88-3"></a><span class="st">  </span><span class="kw">slice_max</span>(<span class="dt">order_by =</span> beta, <span class="dt">n =</span> <span class="dv">7</span>) <span class="op">%&gt;%</span></span>
<span id="cb88-4"><a href="topic-modeling-chapter.html#cb88-4"></a><span class="st">  </span><span class="kw">summarise</span>(<span class="dt">.groups =</span> <span class="st">&quot;drop&quot;</span>, <span class="dt">terms =</span> <span class="kw">list</span>(term)) <span class="op">%&gt;%</span></span>
<span id="cb88-5"><a href="topic-modeling-chapter.html#cb88-5"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">terms =</span> <span class="kw">map</span>(terms, paste, <span class="dt">collapse =</span> <span class="st">&quot;, &quot;</span>)) <span class="op">%&gt;%</span></span>
<span id="cb88-6"><a href="topic-modeling-chapter.html#cb88-6"></a><span class="st">  </span><span class="kw">unnest</span>(terms)</span>
<span id="cb88-7"><a href="topic-modeling-chapter.html#cb88-7"></a></span>
<span id="cb88-8"><a href="topic-modeling-chapter.html#cb88-8"></a>sawyer_stm_gamma <span class="op">%&gt;%</span></span>
<span id="cb88-9"><a href="topic-modeling-chapter.html#cb88-9"></a><span class="st">  </span><span class="kw">group_by</span>(topic) <span class="op">%&gt;%</span></span>
<span id="cb88-10"><a href="topic-modeling-chapter.html#cb88-10"></a><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">.groups =</span> <span class="st">&quot;drop&quot;</span>, <span class="dt">gamma =</span> <span class="kw">mean</span>(gamma)) <span class="op">%&gt;%</span></span>
<span id="cb88-11"><a href="topic-modeling-chapter.html#cb88-11"></a><span class="st">  </span><span class="kw">left_join</span>(top_terms, <span class="dt">by =</span> <span class="st">&quot;topic&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb88-12"><a href="topic-modeling-chapter.html#cb88-12"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">topic =</span> <span class="kw">paste</span>(<span class="st">&quot;topic&quot;</span>, topic),</span>
<span id="cb88-13"><a href="topic-modeling-chapter.html#cb88-13"></a>         <span class="dt">topic =</span> <span class="kw">fct_reorder</span>(topic, gamma)) <span class="op">%&gt;%</span></span>
<span id="cb88-14"><a href="topic-modeling-chapter.html#cb88-14"></a><span class="st">  </span><span class="kw">slice_max</span>(<span class="dt">order_by =</span> gamma, <span class="dt">n =</span> <span class="dv">10</span>) <span class="op">%&gt;%</span></span>
<span id="cb88-15"><a href="topic-modeling-chapter.html#cb88-15"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> topic, <span class="dt">y =</span> gamma, <span class="dt">label =</span> terms)) <span class="op">+</span></span>
<span id="cb88-16"><a href="topic-modeling-chapter.html#cb88-16"></a><span class="st">  </span><span class="kw">geom_col</span>(<span class="dt">fill =</span> <span class="st">&quot;#D8A7B1&quot;</span>, <span class="dt">show.legend =</span> <span class="ot">FALSE</span>) <span class="op">+</span></span>
<span id="cb88-17"><a href="topic-modeling-chapter.html#cb88-17"></a><span class="st">  </span><span class="kw">geom_text</span>(<span class="dt">hjust =</span> <span class="dv">0</span>, <span class="dt">nudge_y =</span> <span class="fl">0.0005</span>, <span class="dt">size =</span> <span class="dv">3</span>) <span class="op">+</span></span>
<span id="cb88-18"><a href="topic-modeling-chapter.html#cb88-18"></a><span class="st">  </span><span class="kw">coord_flip</span>() <span class="op">+</span></span>
<span id="cb88-19"><a href="topic-modeling-chapter.html#cb88-19"></a><span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">expand =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>),</span>
<span id="cb88-20"><a href="topic-modeling-chapter.html#cb88-20"></a>                     <span class="dt">limits =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">0.09</span>),</span>
<span id="cb88-21"><a href="topic-modeling-chapter.html#cb88-21"></a>                     <span class="dt">labels =</span> scales<span class="op">::</span><span class="kw">percent_format</span>()) <span class="op">+</span></span>
<span id="cb88-22"><a href="topic-modeling-chapter.html#cb88-22"></a><span class="st">  </span><span class="kw">theme_minimal</span>() <span class="op">+</span></span>
<span id="cb88-23"><a href="topic-modeling-chapter.html#cb88-23"></a><span class="st">  </span><span class="kw">theme</span>(<span class="dt">panel.grid =</span> <span class="kw">element_blank</span>()) <span class="op">+</span></span>
<span id="cb88-24"><a href="topic-modeling-chapter.html#cb88-24"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="ot">NULL</span>, <span class="dt">y =</span> <span class="kw">expression</span>(gamma),</span>
<span id="cb88-25"><a href="topic-modeling-chapter.html#cb88-25"></a>       <span class="dt">title =</span> <span class="st">&quot;Top 10 STM topics by prevalence in The Adentures of Tom Sawyer&quot;</span>,</span>
<span id="cb88-26"><a href="topic-modeling-chapter.html#cb88-26"></a>       <span class="dt">subtitle =</span> <span class="st">&quot;With top words in each topic&quot;</span>)</span></code></pre></div>
<p><img src="text-book_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>Another way to look at the betas is to identify terms that had the greatest difference in beta between the first and second most probable topic. A good way to do this is with their log ratio, <span class="math inline">\(log_2(\beta_2 / \beta_1)\)</span>. Filter for relatively common words having a beta greater than 1/100 in at least one topic.</p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="topic-modeling-chapter.html#cb89-1"></a>sawyer_stm_beta <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb89-2"><a href="topic-modeling-chapter.html#cb89-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">topic =</span> <span class="kw">paste0</span>(<span class="st">&quot;topic&quot;</span>, topic)) <span class="op">%&gt;%</span></span>
<span id="cb89-3"><a href="topic-modeling-chapter.html#cb89-3"></a><span class="st">  </span><span class="kw">group_by</span>(term) <span class="op">%&gt;%</span></span>
<span id="cb89-4"><a href="topic-modeling-chapter.html#cb89-4"></a><span class="st">  </span><span class="kw">slice_max</span>(<span class="dt">order_by =</span> beta, <span class="dt">n =</span> <span class="dv">2</span>) <span class="op">%&gt;%</span></span>
<span id="cb89-5"><a href="topic-modeling-chapter.html#cb89-5"></a><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">.groups =</span> <span class="st">&quot;drop&quot;</span>, <span class="dt">min_beta =</span> <span class="kw">min</span>(beta)<span class="op">+</span>.<span class="dv">001</span>, <span class="dt">max_beta =</span> <span class="kw">max</span>(beta)<span class="op">+</span>.<span class="dv">001</span>) <span class="op">%&gt;%</span></span>
<span id="cb89-6"><a href="topic-modeling-chapter.html#cb89-6"></a><span class="st">  </span><span class="kw">filter</span>(max_beta <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.01</span>) <span class="op">%&gt;%</span></span>
<span id="cb89-7"><a href="topic-modeling-chapter.html#cb89-7"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">log_ratio =</span> <span class="kw">log2</span>(max_beta <span class="op">/</span><span class="st"> </span>min_beta)) <span class="op">%&gt;%</span></span>
<span id="cb89-8"><a href="topic-modeling-chapter.html#cb89-8"></a><span class="st">  </span><span class="kw">top_n</span>(<span class="dt">n =</span> <span class="dv">20</span>, <span class="dt">w =</span> <span class="kw">abs</span>(log_ratio)) <span class="op">%&gt;%</span></span>
<span id="cb89-9"><a href="topic-modeling-chapter.html#cb89-9"></a><span class="st">  </span><span class="kw">arrange</span>(<span class="op">-</span>log_ratio) <span class="op">%&gt;%</span></span>
<span id="cb89-10"><a href="topic-modeling-chapter.html#cb89-10"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> <span class="kw">fct_rev</span>(<span class="kw">fct_inorder</span>(term)), <span class="dt">y =</span> log_ratio)) <span class="op">+</span></span>
<span id="cb89-11"><a href="topic-modeling-chapter.html#cb89-11"></a><span class="st">  </span><span class="kw">geom_col</span>(<span class="dt">fill =</span> <span class="st">&quot;#D8A7B1&quot;</span>) <span class="op">+</span></span>
<span id="cb89-12"><a href="topic-modeling-chapter.html#cb89-12"></a><span class="st">  </span><span class="kw">theme_minimal</span>() <span class="op">+</span></span>
<span id="cb89-13"><a href="topic-modeling-chapter.html#cb89-13"></a><span class="st">  </span><span class="kw">coord_flip</span>() <span class="op">+</span><span class="st"> </span></span>
<span id="cb89-14"><a href="topic-modeling-chapter.html#cb89-14"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;STM beta matrix log ratios&quot;</span>,</span>
<span id="cb89-15"><a href="topic-modeling-chapter.html#cb89-15"></a>       <span class="dt">subtitle =</span> <span class="st">&quot;showing greatest differences in beta values&quot;</span>,</span>
<span id="cb89-16"><a href="topic-modeling-chapter.html#cb89-16"></a>       <span class="dt">x =</span> <span class="st">&quot;&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;log(beta ratio)&quot;</span>)</span></code></pre></div>
<p><img src="text-book_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
</div>
</div>
<p style="text-align: center;">
<a href="text-mining.html"><button class="btn btn-default">Previous</button></a>
<a href="sentiment-analysis.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>

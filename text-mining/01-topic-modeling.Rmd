# Topic Modeling {#topicmodeling}

```{r include=FALSE}
library(tidyverse)
library(stm)
library(topicmodels)
```


_These notes are primarily compiled from the vignette for the [STM](http://www.structuraltopicmodel.com/) package._

Topic models are unsupervised ML models that identify topics as clusters of words with an associated probability distribution, and a probability distribution of topics within each document. 

There are two commonly used models: LDA and STM. LDA is the simpler model and is implemented in the popular **topicmodels** package. STM incorporates document metadata into the model. It is implemented in the **STM** package. These notes also discuss CTM, also in **topicmodels**, that is somewhere between LDA and STM. I discuss CTM because it is a bridge from LDA to STM.

All three topic models are _generative models of word counts_. That means they assume there is some process that generates text which is a mixture of topics composed of words which occur with varying probabilities. Think of the observed text document as the product of an algorithm that selected each word in two stages: 1) it sampled a topic from a probability distribution, then 2) it sampled a word from the topic's word probability distribution. The object in topic modeling is to tune the hyperparameters that define those probability distributions. In a way, topic models do the opposite of what you might expect. They are _not_ estimating the probability that each document is _one_ of those topics. They assume _all_ topics contribute to each document and instead estimate their relative contributions. More concisely, the models treat documents as a mixture of topics, and the topics as a mixture of words where each word has a probability of belonging to each topic. The sum of topic proportions in document is one; the sum of word probabilities in a topic is one.

This leads to two frameworks for thinking about topics. A topic's _prevalance_ in a document measures the proportion of the document generated by it. The topic's _content_ is the probability distribution of words associated with it. What distinguishes the following following models is how they handle these frameworks. An STM model defines covariates associated with prevalence and content; CTM does not. _I don't think LDA does either. We're kind of at the limit of my understanding here._

## Learning by Example

I will work through the model concepts by example using the `stm::gadarian` data set. This data set has _n_ = 351 comments about immigration in an experimental setting. The test group as specifically instructred to write about what made them anxious about immigration. There is also a variable `pid_rep` for political party.

```{r}
gadarian_dat <- stm::gadarian %>%
  rename(comment = open.ended.response) %>%
  select(-MetaID)
glimpse(gadarian_dat) 
```

## LDA {#lda}

Latent Dirichlet allocation (LDA) is an instance of a general family of mixed membership models that decompose data into latent components. *Latent* refers to _unidentified_ topics and *Dirichlet* refers to the type of distribution followed by the words in the the topics and by the topics in the documents.

#### Algorithm {-}

LDA assumes each document is created by a generative process where topics are included according to probabilities and words are included in the topics according to probabilities. The LDA algorithm determines what those probabilities are. The [algorithm](https://www.mygreatlearning.com/blog/understanding-latent-dirichlet-allocation/) is:

1) For each document $d_i$, randomly assign each word to one of the *K* topics. Note that each $w_j$ may be assigned to a different topic in each documents.

2) For each document, tabulate the number of words in each topic, a $d \times K$ matrix. For each word, tabulate the sum of occurrences across all documents, a $w \times K$ matrix.

3) Resample a single instance of a word from the corpus and remove it from the analysis, decrementing the document's topic count and the word's topic count.

4) Calculate the gamma matrix, $\gamma$, and the beta matrix, $\beta$.
    * the gamma matrix is the probability distribution of topics for each document, $$p(t_k|d_i) = \frac{n_{ik} + \alpha}{N_i + K \alpha}$$ were $n_{ik}$ is the number of words in document $i$ for topic $k$, $N_i$ is the total number of words in $i$, and $\alpha$ is a hyperparameter. For each $d_i$, $\sum_{k \in K} \gamma_{ik} = 1$.
    * the beta matrix is the probability distribution of words for each topic, $$p(w_j|t_k) = \frac{m_{j,k} + \beta}{\sum_{j \in V}m_{j,k} + V\beta}$$ where $m_{j,k}$ is the corpus-wide frequency count of word $w_j$ to topic $k$, $V$ is the number of distinct words in the corpus, and $\beta$ is a hyperparameter. For each $t_k$, $\sum_{j \in V} \beta_{kj} = 1$.
  
6) Perform Gibbs sampling. Calculate the joint probability distribution of words for each document and topic, $p(w_j|t_k,d_i) = p(t_k|d_i)p(w_j|t_k)$. Assign each word, $w_j$, to the topic with the maximum joint probability.

7) Repeat steps 3-6 for all of the words in all of the documents.

8) Repeat steps 3-7 for a pre-determined number of iterations.

LDA thus has 3 hyperparameters: document-topic density factor, $\alpha$, topic-word density factor, $\beta$, and topic count, $K$. $\alpha$ controls the number of topics expected per document (large $\alpha$ = more topics). $\beta$ controls the distribution of words per topic (large $\beta$ = more words). Ideally, you want a few topics per document and a few words per topics, so, $\alpha$ and $\beta$ are typically set below one. $K$ is set using a combination of domain knowledge, *coherence*, and exclusivity.

#### Evaluation {-}

##### Held-out Likelihood {-}

(discussion of hold-out probability) (Wallach et al., 2009).

##### Semantic Coherence {-}

##### Exclusivity {-}

Generally, the greater the number of topics in a model, the lower the quality of the smallest topics. One way around this is simply hiding the low-quality topics. The coherence measure [@10.5555/2145432.2145462] evaluates topics.


## CTM {#ctm}

The Correlated Topic Model (CTM) [@blei2007] builds on the LDA model (chapter \@ref(lda)). 

## STM {#stm}

STM incorporates arbitrary document metadata into the topic model. Without the inclusion of covariates, STM reduces to a logistic-normal topic model, often called the Correlated Topic Model (CTM) (chapter \@ref(ctm)). The goal of STM is to discover topics and estimate their relationship to the metadata.

### Data Preparation {-}

The **stm** package represents a text corpus as an object with three components: a sparse matrix of counts by document and vocabulary word vector index, the vocabulary word vector, and document metadata.

`stm::textProcessor()` is essentially a wrapper around the **tm** package. It:
* converts words to lowercase,
* removes stop words (including custom stop words!), numbers, and punctuation, and
* stems words.

After processing, `stm::prepDocuments()` removes infrequently appearing words, and removes any documents that contain no words after processing and removing words.

```{r}
gadarian_processed <- textProcessor(gadarian_dat$comment, metadata = gadarian_dat)
```

```{r}
plotRemoved(gadarian_processed$documents, lower.thresh = seq(10, 200, by = 10))
```


### Prepare {-}
### Evaluate {-}
### Interpret {-}
### Visualize {-}

## Data Formats

There are five common text mining packages, each with their own format requirements. Whichever package you work in, there is a decent chance you will want to use a function from one of the others, so you need some fluency in them all.

* **tm** works with Corpus objects (raw text with document and corpus metadata). Many **tm** algorithms work with a document-term matrix (DTM), a sparse matrix with one row per document, one column per term, and values equaling the word count or tf-idf. 

* **quanteda** also works with Corpus objects, but has its own implementation. Many **quanteda** algorithms work with a document-feature matrix (DFM), again similar to **tm**'s DTM.

* **tidytext** works with tibbles. Many **tidytext** algorithms work with tibbles with one row per token (usually a word, but possibly a large item of text), a frequency count column, and possibly other metadata columns.

* **qdap** works with text fields in a data frame, so it does not require any particular data structure.

* **sentimentr** is similar to **qdap**.

Let's take the `sawyer_raw` data frame and pre-process it for all three packages.

#### tm {-}

Turn the character vector `sawyer_raw$text` into a text source with `VectorSource()`, then turn the text source into a corpus with `vCorpus()`. Clean the corpus with a series of utility functions. One particularly important function, `removeWords()`, removes stop words, plus any custom stop words. I would normally add "tom" because it is so ubiquitous throughout the text. However, in this case I won't because `stopwords` includes valence shifting words like "very" which are used in polarity scoring. I can remove them later for other exercises.

```{r}
# (sawyer_tm <- VCorpus(VectorSource(sawyer$text)) %>%
#   tm_map(content_transformer(replace_abbreviation)) %>%
#   tm_map(removePunctuation) %>%
#   tm_map(removeNumbers) %>%
#   tm_map(content_transformer(tolower)) %>%
#   tm_map(removeWords, c(stopwords("en"), "tom")) %>%
#   tm_map(stripWhitespace))
```

Each document in the `sawyer_tm` VCorpus is a line of text. Use `DocumentTermMaterix()` to convert the vCorpus into **tm**'s bag-of-words format, DTM.

```{r}
# (sawyer_tm_dtm <- DocumentTermMatrix(sawyer_tm))
```

This is a very sparse (nearly 100% sparse) matrix documents as rows and distinct words as columns.

```{r}
#   group_by(chapter) %>%
#   mutate(text = paste(text, collapse = " ")) %>%
#   slice_head(n = 1) %>%
#   select(chapter, text)
# 
# sawyer_sent <- sawyer %>%
#   sentSplit("text")
# 
# skimr::skim(sawyer)
```

#### quanteda {-}
dafdafd

#### tidytext {-}
dafdafd




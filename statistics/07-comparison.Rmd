# Comparison

```{r include=FALSE}
library(tidyverse)
library(broom)
library(flextable)
library(table1)
```

Comparison tests look for differences among group means. They can be used to test the effect of a categorical variable on the mean value of some other characteristic.

T-tests are used when comparing the means of precisely two groups (e.g. the average heights of men and women). ANOVA and MANOVA tests are used when comparing the means of more than two groups (e.g. the average heights of children, teenagers, and adults).


Quantitative ~ Categorical

## Independent t-Test
## Paired t-Test
## Sign est
## Wilcoxon Rank-Sum Test
## Wilcoxon Signed-Rank Test
## ANOVA

Most of these notes are gleaned from [PSU STAT-502](https://online.stat.psu.edu/stat502) "Analysis of Variance and Design of Experiments" covers ANOVA. [Laerd Statistics](https://statistics.laerd.com/premium/spss/owa/one-way-anova-in-spss.php) is useful for writing up your results for reports.

Classic analysis of variance (ANOVA) compares the mean responses from *experimental* studies. However, ANOVA also compares the mean responses from *observational* studies, but conclusions are just less rigorous. 

### One-Way ANOVA

Use the one-way ANOVA test to compare the mean response of a continuous dependent variable among the levels of a factor variable.

Here is a case study. Researchers compare the plant growth among three fertilizers and a control group. Data set `greenhouse` contains 6 observations per each of the *k* = 4 treatment levels (*N* = 24) - a balanced design.

```{r include=FALSE}
greenhouse <- tribble(
  ~group, ~growth,
"Control",      21,
"Control",      19.5,
"Control",      22.5,
"Control",      21.5,
"Control",      20.5,
"Control",      21,
"F1",      32,
"F1",      30.5,
"F1",      25,
"F1",      27.5,
"F1",      28,
"F1",      28.6,
"F2",      22.5,
"F2",      26,
"F2",      28,
"F2",      27,
"F2",      26.5,
'F2',      25.2,
"F3",      28,
"F3",      27.5,
"F3",      31,
"F3",      29.5,
"F3",      30,
"F3",      29.2
) %>%
  mutate(group = factor(group), id = row_number())

skimr::skim(greenhouse)
```

All three fertilizers produced more growth than the control group. Fertilizers *F1* and *F3* appear to be about tied for most growth, but it is unclear if the fertilizers are significantly different from each other.  

```{r echo=FALSE, fig.height=2.5}
greenhouse %>%
  group_by(group) %>%
  summarize(.groups = "drop",
            mean_growth = mean(growth),
            cl_025 = mean_growth + qnorm(.025) * sd(growth) / sqrt(n()),
            cl_975 = mean_growth + qnorm(.975) * sd(growth) / sqrt(n())) %>%
  ggplot(aes(x = group, y = mean_growth)) +
  geom_col(fill = "snow3", color = "snow3", alpha = 0.6, width = 0.5) +
  geom_errorbar(aes(ymin = cl_025, ymax = cl_975, width = 0.3)) +
  theme_minimal() +
  labs(title = "Mean Growth by Group",
       x = NULL, y = "Growth (cm)",
       caption = "Error Bars: 95% CI")
```

```{r echo=FALSE}
# table1 requires this metadata
label(greenhouse$growth) <- "Growth"
units(greenhouse$growth) <- "cm"

# Quick and good:
# table1(~ growth | group, data = greenhouse)

# Better: prints "Treated" at the top.
strata <- c(split(greenhouse, greenhouse$group),
            list("All treated" = subset(greenhouse, group %in% c("F1", "F2", "F3"))),
            list(Overall = greenhouse))
labels <- list(
  variables = list(growth = render.varlabel(greenhouse$growth)),
  groups = list("", "Treated", "")
)
table1(strata, labels, groupspan = c(1, 4, 1))
```

```{r}
greenhouse_desc <- greenhouse %>% 
  group_by(group) %>% 
  summarize(.groups = "drop", n = n(), mean = mean(growth), sd = sd(growth))
```

Data is presented as mean $\pm$ standard deviation. Plant growth (growth) increased from the control (*n* = `r pull(greenhouse_desc[1, "n"])`, `r pull(greenhouse_desc[1, "mean"])` $\pm$ `r format(pull(greenhouse_desc[1,"sd"]), nsmall = 1)`), to fertilizer 1 (*n* = `r pull(greenhouse_desc[2, "n"])`, `r pull(greenhouse_desc[2, "mean"])` $\pm$ `r format(pull(greenhouse_desc[2,"sd"]), digits = 2, nsmall = 1)`), fertilizer 2 (*n* = `r pull(greenhouse_desc[3, "n"])`, `r pull(greenhouse_desc[3, "mean"])` $\pm$ `r format(pull(greenhouse_desc[3,"sd"]), digits = 2, nsmall = 1)`), and fertilizer 3 (*n* = `r pull(greenhouse_desc[4, "n"])`, `r pull(greenhouse_desc[4, "mean"])` $\pm$ `r format(pull(greenhouse_desc[4,"sd"]), digits = 2, nsmall = 1)`) fertilizer groups.

ANOVA decomposes the deviation of observation $Y_{ij}$ around the overall mean $\bar{Y}_{..}$ into two parts: the deviation of the observations around their treatment means, $SSE$, and the deviation of the treatment means around the overall mean, $SSR$. Their ratio, $F = SSR/SSE$ follows an *F*-distribution with $k-1$ numerator dof and $N-k$ denominator dof. The more observation variance captured by the treatments, the large is $F$, and the less likely that the null hypothesis, $H_0 = \mu_1 = \mu_2 = \cdots = \mu_k$ is true.

```{r echo=FALSE}
tmp <- tribble(
  ~Source, ~SS, ~df, ~MS, ~F,
  "SSR", "$\\sum{n_i(\\bar{Y}_{i.} - \\bar{Y}_{..})^2}$", "$k - 1$", "${SSR}/{(k - 1)}$", "${MSR}/{MSE}$", 
  "SSE", "$\\sum(Y_{ij} - \\bar{Y}_{i.})^2$", "$N - k$", "${SSE}/{(N - k)}$", "",
  "SST", "$\\sum(Y_{ij} - \\bar{Y}_{..})^2$", "$N - 1$", "", ""
)

tmp %>% 
  knitr::kable(format = "html", caption = "ANOVA Table") %>%
  kableExtra::kable_styling(full_width = TRUE) %>%
  kableExtra::row_spec(row = 0, align = "c") #%>%
  # kableExtra::footnote(
  #   general_title = "Note.",
  #   general = "Compare the *F*-statistic to the *F*-distribution with $k-1$ numerator degrees of freedom and $N-k$ denominator degrees of freedom",
  #  footnote_as_chunk = TRUE
  #   )
```

Run an ANOVA test in R like this:

```{r}
greenhouse_aov <- aov(growth ~ group, data = greenhouse)
greenhouse_anova <- anova(greenhouse_aov)

greenhouse_anova %>% 
  tidy() %>%
  flextable() %>%
  set_table_properties(width = 0.8, layout = "autofit") %>%
  colformat_num(j = c(3, 4, 5), digits = 1) %>%
  colformat_num(j = 6, digits = 4) %>%
  set_caption("Results of ANOVA for Growth vs Fertilizer Group")
```

The one-way ANOVA indicates amount of growth was statistically significantly different for different levels of fertilizer group, *F*(3, 20) = 27.5, *p* < .0001.

BTW, it is worth noting the relationship with linear regression. The regression model intercept is the overall mean and the coefficient estimators indirectly indicate the group means. The analysis of variance table in a regression model shows how much of the overall variance is explained by those coefficient estimators. It's the same thing.

You may also want to report the $\omega^2$ effect size, 

$$\omega^2 = \frac{SSR - df_R \cdot MSE}{MSE + SST}$$
```{r}
greenhouse_omega <- sjstats::anova_stats(greenhouse_anova) %>% 
  filter(term == "group") %>%
  pull(omegasq)
```
$\omega^2$ ranges from -1 to +1. In this example, $\omega^2$ is `r greenhouse_omega`.

#### ANOVA Conditions

The ANOVA test applies when the dependent variable is continuous, the independent variable is categorical, and the observations are independent *within* groups. Independent means the observations should be from a random sample, or from an experiment using random assignment.  Each group's size should be less than 10% of its population size. The groups must also be independent of each other (non-paired, and non-repeated measures). Additionally, there are three conditions related to the data distribution. If any condition does not hold, and the suggested work-arounds do not work switch to the non-parametric [Kruskal-Wallis Test].

1. **No outliers**. There should be no significant outliers in the groups. Outliers exert a large influence on the mean and standard deviation. Test with a box plot. If there are outliers, you might be able to drop them or transform the data.
2. **Normality**.  Each group's values should be *nearly* normally distributed ("nearly" because ANOVA is considered robust to the normality assumption). This condition is especially important with small sample sizes. Test with the Q-Q plots or the Shapiro-Wilk test for normality. If the data is very non-normal, you might be able to transform your response variable.
3. **Equal Variances**.  The group variances should be roughly equal. This condition is especially important when sample sizes differ. Test with a box plot, rule of thumb, or one of the formal [homogeneity of variance](http://www.cookbook-r.com/Statistical_analysis/Homogeneity_of_variance/) (external) tests such as Bartlett, and Levene. If the variances are very different, use a Games-Howell post hoc test instead of the Tukey post hoc test.

##### Outliers {-}

Assess outliers with a box plot. Box plot whiskers extend up to 1.5\*IQR from the upper and lower hinges and outliers (beyond the whiskers) are are plotted individually. Our example includes an outlier in fertilizer group *F2*.

```{r echo=FALSE, fig.height=2.5}
greenhouse %>%
  ggplot(aes(x = group, y = growth)) +
  geom_boxplot(fill = "snow3", color = "snow4", alpha = 0.6, width = 0.5, 
               outlier.color = "goldenrod", outlier.size = 2) +
  theme_minimal() +
  labs(title = "Boxplot of Growth vs Fertilizer Group",
       y = "Growth (cm)", x = "Fertilizer Group")
```

Outliers might occur from data entry errors or measurement errors, so investigate and fix or throw them out. However, if the outlier is a genuine extreme value, you still have a couple options before reverting to Kruskal-Wallis.

* Transform the dependent variable. Don't do this unless the data is also non-normal. It also has the downside of making interpretation more difficult.
* Leave it in if it doesn't affect the conclusion (compared to taking it out).

Lets try removing the outlier (*id#* 13). 

```{r}
greenhouse_aov2 <- aov(growth ~ group, data = greenhouse %>% filter(!id == 13))
greenhouse_anova2 <- anova(greenhouse_aov2)
```

```{r echo=FALSE}
greenhouse_anova2 %>% 
  tidy() %>%
  flextable() %>%
  set_table_properties(width = 0.8, layout = "autofit") %>%
  colformat_num(j = c(3, 4, 5), digits = 1) %>%
  colformat_num(j = 6, digits = 4) %>%
  set_caption("Results of ANOVA for Growth vs Fertilizer Group") %>%
  footnote(i = 1, j = 1,
           value = as_paragraph("Note: One outlier in group F2 removed."),
           ref_symbols = c(""),
           part = "header", inline = TRUE)

```

The conclusion is the same, so leaving it in is fine!

##### Normality {-}

You can assume the populations are normally distributed if $n_j >= 30$. Otherwise, try the Q-Q plot, or skewness and kurtosis values, or histograms. If you still don't feel confident about normality, run a [Shapiro-Wilk Test] or Kolmogorov-Smirnov Test. If $n_j >= 50$, stick with graphical methods because at larger sample sizes Shapiro-Wilk flags even minor deviations from normality.

The QQ plots below appear to be approximately normal.

```{r}
greenhouse %>%
  ggplot(aes(sample = growth)) +
  stat_qq() +
  stat_qq_line(col = "goldenrod") +
  facet_wrap(~group) +
  theme_minimal() +
  labs(title = "Normal Q-Q Plot")
```

The Shapiro-Wilk test corroborates this conclusion - it fails to reject the null hypothesis of normally distributed populations.

```{r}
x <- by(greenhouse, greenhouse$group, function(x) shapiro.test(x$growth) %>% tidy())

x[1:4] %>%
  bind_rows() %>%
  mutate(group = names(x)) %>%
  dplyr::select(group, everything(), - method) %>%
  flextable() %>% 
  set_table_properties(width = 0.6, layout = "autofit") %>%
  set_caption("Shapiro-Wilk Normality Test")
```

If the data is not normally distributed, you still have a couple options before reverting to Kruskal-Wallis.

* Transform the dependent variable. Transformations will generally only work when the distribution of scores in all groups are the same shape. They also have the drawback of making the data less interpretable.
* carry on regardless. One-way ANOVA is fairly robust to deviations from normality, particularly if the sample sizes are nearly equal.

##### Equal Variances {-}

The equality of sample variances condition is less critical when sample sizes are similar among the groups. One rule of thumb is that no group's standard deviation should be more than double that of any other.  In this case `F1` is more than double `Control`.

```{r echo=FALSE}
greenhouse %>% 
  group_by(group) %>% 
  summarize(.groups = "drop", sd = sd(growth)) %>%
  flextable() %>%
  set_table_properties(width = 0.5, layout = "autofit")
```

There are two other common tests, Bartlett and Levene. NIST has a good write-up for [Levene](https://www.itl.nist.gov/div898/handbook/eda/section3/eda35a.htm) and for [Bartlett](https://www.itl.nist.gov/div898/handbook/eda/section3/eda357.htm). Levene is less sensitive than Bartlett to departures from normality, so if you know your data is normally distributed, then use Bartlett.

Levene's test fails to reject the null hypothesis of equality of variance.

```{r}
greenhouse_levene <- car::leveneTest(growth ~ group, data = greenhouse) 
greenhouse_levene %>% 
  tidy() %>%
  flextable() %>%
  set_table_properties(width = 0.6, layout = "autofit") %>% 
  set_caption("Levene's Test for Homogeneity of Variance")
```

So does Bartlett.  

```{r}
bartlett.test(growth ~ group, data = greenhouse) %>% 
  tidy() %>%
  dplyr::select(-method) %>%
  flextable() %>%
  set_table_properties(width = 0.6, layout = "autofit") %>% 
  set_caption("Bartlett's Test for Homogeneity of Variance")
```

Heterogeneity is a common problem in ANOVA. Transforming the response variable can often remove the heterogeneity. The Box-Cox procedure can help find a good transformation. The MASS::boxcox() function calculates a profile of log-likelihoods for a power transformation of the response variable $Y^\lambda$.

|$\lambda$ | $Y^\lambda$ | Transformation |
|---|---|---|
|2 | $Y^2$ | Square |
|1 | $Y^1$ | (no transformation) |
|.5 | $Y^{.5}$ | Square Root |
|0 | $\ln(Y)$ | Log |
|-.5 | $Y^{-.5}$ | Inverse Square Root |
|-1 | $Y^{-1}$ | Inverse|

The Box-Cox procedure does not recommend any particular transformation of the data in this case.

```{r message=FALSE}
MASS::boxcox(greenhouse_aov, plotit = TRUE)
```

#### Custom Contrasts

Taking this route is appropriate if you have specific hypotheses about the differences between the groups of your independent variable (e.g., a hypothesis that there is a difference between Group D and Group B, or a hypothesis that there is a difference between Groups D and C combined when compared to Groups D and A combined). 

```{r}
greenhouse_aov <- aov(growth ~ group, data = greenhouse)
anova(greenhouse_aov)

greenhouse_glm <- glm(growth ~ group, data = greenhouse, family = "gaussian")
anova(greenhouse_glm)

greenhouse_lm <- lm(growth ~ group, data = greenhouse)
anova(greenhouse_lm)
```

#### Tukey Post Hoc Test

The *F* test does not indicate which populations cause the rejection of $H_0$. For this, use one of the post-hoc tests: Tukey, Fisher's Least Significant Difference (LSD), Bonferroni, Scheffe, or Dunnett. Post hoc tests are appropriate if you are investigating all possible pairwise comparisons with no specific hypotheses about specific groups differing from others.

Here is the Tukey test. As expected, all three fertilizer factor levels differ from the control.  `F3` differed from `F2`, but `F1` was not significantly different from either `F2` or `F3`.

```{r}
greenhouse_tukey <- TukeyHSD(greenhouse_aov)
greenhouse_tukey %>% 
  tidy() %>%
  flextable() %>%
  set_table_properties(width = 0.8, layout = "autofit") %>% 
  colformat_num(j = c(4:6), digits = 1) %>%
  colformat_num(j = 7, digits = 3) %>%
  set_caption("Tukey multiple comparisons of means") %>%
  footnote(i = 1, j = c(1),
            value = as_paragraph(
              paste0("95% family-wise confidence level\n",
                "Fit: aov(formula = growth ~ group, data = greenhouse)")),
            ref_symbols = c(""),
            part = "header")
```

Data are mean $\pm$ standard deviation. There was an increase in growth from `r pull(greenhouse_desc[1, "mean"])` $\pm$ `r pull(greenhouse_desc[1, "sd"])` in the control group to `r pull(greenhouse_desc[2, "mean"])` $\pm$ `r format(pull(greenhouse_desc[2, "sd"]), digits = 2, nsmall = 1)` in the group with fertilizer F1, an increase of `r greenhouse_tukey$group[1, "diff"]` (95% CI, `r format(greenhouse_tukey$group[1, "lwr"], digits = 2, nsmall = 1)` to `r format(greenhouse_tukey$group[1, "upr"], digits = 2, nsmall = 1)`), which was statistically significant (*p* < .0001)... etc.

#### Reporting a One-Way ANOVA

Report like this.

> A one-way ANOVA was conducted to determine if plant growth was different for groups with different fertilizer types. Plants were classified into four groups: control (*n* = 6), fertilizer 1 (*n* = 6), fertilizer 2 (*n* = 6), and fertilizer 3 (n = 6). There was a single outlier, as assessed by boxplot, and was retained because it did not change the conclusions; data was normally distributed for each group, as assessed by Shapiro-Wilk test (*p* > .05); and there was homogeneity of variances, as assessed by Levene's test of homogeneity of variances (*p* = `r round(greenhouse_levene[1, 3], 3)`). Data is presented as mean $\pm$ standard deviation. Plant growth was statistically significantly different between different fertilizer groups, *F*(`r greenhouse_anova$Df[1]`, `r greenhouse_anova$Df[2]`) = `r round(greenhouse_anova[1, 4], 3)`, p < .0005, $\omega^2$ = `r greenhouse_omega`. Plant growth increased from the control (`r pull(greenhouse_desc[1, "mean"])` $\pm$ `r format(pull(greenhouse_desc[1,"sd"]), nsmall = 1)`), to fertilizer F1 (`r pull(greenhouse_desc[2, "mean"])` $\pm$ `r format(pull(greenhouse_desc[2,"sd"]), digits = 2, nsmall = 1)`), fertilizer F2 (`r format(pull(greenhouse_desc[3, "mean"]), digits = 3, nsmall = 1)` $\pm$ `r format(pull(greenhouse_desc[3,"sd"]), digits = 2, nsmall = 1)`), and fertilizer F3 (`r pull(greenhouse_desc[4, "mean"])` $\pm$ `r format(pull(greenhouse_desc[4,"sd"]), digits = 2, nsmall = 1)`) fertilizer groups. Tukey post hoc analysis revealed statistically significant increases from control to F1 (`r format(greenhouse_tukey$group[1, 1], digits=2, nsmall = 1)`, 95% CI (`r format(greenhouse_tukey$group[1, 2], digits=2, nsmall = 1)` to `r format(greenhouse_tukey$group[1, 3], digits=2, nsmall = 1)`), *p* = `r format(greenhouse_tukey$group[1, 4], digits=2, nsmall = 3)`), control to F2 (`r format(greenhouse_tukey$group[2, 1], digits=2, nsmall = 1)`, 95% CI (`r format(greenhouse_tukey$group[2, 2], digits=2, nsmall = 1)` to `r format(greenhouse_tukey$group[2, 3], digits=2, nsmall = 1)`), *p* = `r format(greenhouse_tukey$group[2, 4], digits=2, nsmall = 3)`), and control to F3 (`r format(greenhouse_tukey$group[3, 1], digits=2, nsmall = 1)`, 95% CI (`r format(greenhouse_tukey$group[3, 2], digits=2, nsmall = 1)` to `r format(greenhouse_tukey$group[3, 3], digits=2, nsmall = 1)`), *p* = `r format(greenhouse_tukey$group[3, 4], digits=2, nsmall = 3)`), as well as the increase from F2 to F3 (`r format(greenhouse_tukey$group[6, 1], digits=2, nsmall = 1)`, 95% CI (`r format(greenhouse_tukey$group[6, 2], digits=2, nsmall = 1)` to `r format(greenhouse_tukey$group[6, 3], digits=2, nsmall = 1)`), *p* = `r format(greenhouse_tukey$group[6, 4], digits=2, nsmall = 3)`), but there were no statistically significant group differences between F1 and F2 or F1 and F3.


### Welch's ANOVA w/Games-Howell

Welch's ANOVA test is an alternative to the one-way ANOVA test in cases where the equality of variances assumption is violated.

Here is a case study. Researchers compare the force (in newtons) generated in three steps. Data set `newton` contains 30 observations per each of the *k* = 3 step levels (*N* = 90) - a balanced design.

```{r echo=FALSE, fig.height=2.5}
newton <- data.frame(
  newtons = c(400.95, 445.58, 499.7, 344.62, 397.19, 424.1, 692.2, 436.68, 541.27, 
              518.01, 618.62, 493.19, 368.08, 405.96, 391.81, 305.59, 345.88, 458.15, 
              320.92, 371.11, 373.94, 434.02, 484.49, 360.24, 495.93, 464.1, 455.07, 
              343.19, 358.93, 325.86, 524.63, 584.21, 592.76, 567.12, 716.15, 481.64, 
              518.3, 759, 419.33, 641.5, 713.04, 657.78, 469.29, 420.89, 440.02, 417.39, 
              489.02, 586.63, 477.39, 461.69, 564.12, 565.96, 427.77, 503.48, 494.44, 
              577.08, 425.99, 428.64, 461.9, 417.68, 820.65, 788.84, 644.03, 779.9, 
              892.91, 571.47, 625.09, 751.91, 582.32, 870.45, 886.16, 938.5, 605.69, 
              572.6, 643.57, 770.42, 660.53, 576.62, 437.25,593.72, 441.11, 591.43, 
              749.08, 670.02, 475.11, 455.26, 526.59, 560.3, 474.65, 511.5),
  step = factor(c(rep("A", 30), rep("B", 30), rep("C", 30)))
)

newton %>%
  group_by(step) %>%
  summarize(.groups = "drop",
            mean = mean(newtons),
            cl_025 = mean + qnorm(.025) * sd(newtons) / sqrt(n()),
            cl_975 = mean + qnorm(.975) * sd(newtons) / sqrt(n())) %>%
  ggplot(aes(x = step, y = mean)) +
  geom_col(fill = "snow3", color = "snow3", alpha = 0.6, width = 0.5) +
  geom_errorbar(aes(ymin = cl_025, ymax = cl_975, width = 0.3)) +
  theme_minimal() +
  labs(title = "Mean Newtons by Step",
       x = "Step", y = "Newtons",
       caption = "Error Bars: 95% CI")
```

```{r echo=FALSE}
# table1 requires this metadata
label(newton$newtons) <- "Force"
units(newton$newtons) <- "newtons"

table1(~ newtons | step, data = newton)
```

```{r}
newton_desc <- newton %>% 
  group_by(step) %>% 
  summarize(.groups = "drop", n = n(), mean = mean(newtons), sd = sd(newtons))
```

Data is presented as mean $\pm$ standard deviation. Force (newtons) increased from step 1 (*n* = `r pull(newton_desc[1, "n"])`, `r pull(newton_desc[1, "mean"]) %>% format(digits = 3, nsmall = 0)` $\pm$ `r pull(newton_desc[1,"sd"]) %>% format(digits = 3, nsmall = 0)`), to step 2 (*n* = `r pull(newton_desc[2, "n"])`, `r pull(newton_desc[2, "mean"]) %>% format(digits = 3, nsmall = 0)` $\pm$ `r pull(newton_desc[2,"sd"]) %>% format(digits = 3, nsmall = 0)`), to step 3 (*n* = `r pull(newton_desc[3, "n"])`, `r pull(newton_desc[3, "mean"]) %>% format(digits = 3, nsmall = 0)` $\pm$ `r pull(newton_desc[3,"sd"]) %>% format(digits = 3, nsmall = 0)`).

Start by running the standard ANOVA test:

```{r}
newton_aov <- aov(newtons ~ step, data = newton)
newton_anova <- anova(newton_aov)

newton_anova %>% 
  tidy() %>%
  flextable() %>%
  set_table_properties(width = 0.8, layout = "autofit") %>%
  colformat_num(j = c(3, 4, 5), digits = 1) %>%
  colformat_num(j = 6, digits = 4) %>%
  set_caption("Results of ANOVA for Force vs Step")
```

The one-way ANOVA indicates amount of force was statistically significantly different for different levels of step, *F*(2, 87) = 28.4, *p* < .0001.

#### ANOVA Conditions

Check the three ANOVA conditions: no outliers, normality, and equal variances.

##### Outliers {-}

Assess outliers with a box plot. Our example includes an outlier in step *A*.

```{r echo=FALSE, fig.height=2.5}
newton %>%
  ggplot(aes(x = step, y = newtons)) +
  geom_boxplot(fill = "snow3", color = "snow4", alpha = 0.6, width = 0.5, 
               outlier.color = "goldenrod", outlier.size = 2) +
  theme_minimal() +
  labs(title = "Boxplot of Force vs Step",
       y = "Force (newtons)", x = "Step")
```

You can either transform the dependent variable, see if taking it out changes your conclusion, or use a non-parametric test. Let's try removing the outlier (*id#* 13). 

```{r}
newton2 <- newton %>% mutate(id = row_number())
newton_aov2 <- aov(newtons ~ step, data = newton2 %>% filter(!id == 7))
newton_anova2 <- anova(newton_aov2)
```

```{r echo=FALSE}
newton_anova2 %>% 
  tidy() %>%
  flextable() %>%
  set_table_properties(width = 0.8, layout = "autofit") %>%
  colformat_num(j = c(3, 4, 5), digits = 1) %>%
  colformat_num(j = 6, digits = 4) %>%
  set_caption("Results of ANOVA for Force vs Step") %>%
  footnote(i = 1, j = 1,
           value = as_paragraph("Note: One outlier in step A removed."),
           ref_symbols = c(""),
           part = "header", inline = TRUE)

```

The conclusion is the same, so leaving it in is fine!

##### Normality {-}

You can assume the populations are normally distributed if $n_j >= 30$, but I'll examine the Q-Q plot and run a [Shapiro-Wilk Test] anyway.

The QQ plots below appear to be approximately normal...

```{r}
newton %>%
  ggplot(aes(sample = newtons)) +
  stat_qq() +
  stat_qq_line(col = "goldenrod") +
  facet_wrap(~step) +
  theme_minimal() +
  labs(title = "Normal Q-Q Plot")
```

...but the Shapiro-Wilk test fails for step *A* and *B* -- evidence of its sensitivity for large *n*. I will ignore this violation.

```{r}
x <- by(newton, newton$step, function(x) shapiro.test(x$newtons) %>% tidy())

x[1:3] %>%
  bind_rows() %>%
  mutate(group = names(x)) %>%
  dplyr::select(group, everything(), - method) %>%
  flextable() %>% 
  set_table_properties(width = 0.6, layout = "autofit") %>%
  set_caption("Shapiro-Wilk Normality Test")
```

##### Equal Variances {-}

The equality of sample variances condition is less critical when sample sizes are similar among the groups. Following the rule of thumb that no group's standard deviation be more than double that of any other, we look okay.

```{r echo=FALSE}
newton %>% 
  group_by(step) %>% 
  summarize(.groups = "drop", sd = sd(newtons)) %>%
  flextable() %>%
  set_table_properties(width = 0.5, layout = "autofit")
```

However, Levene's test rejects the null hypothesis of equality of variance.

```{r}
newton_levene <- car::leveneTest(newtons ~ step, data = newton) 
newton_levene %>% 
  tidy() %>%
  flextable() %>%
  set_table_properties(width = 0.6, layout = "autofit") %>% 
  set_caption("Levene's Test for Homogeneity of Variance")
```

So does Bartlett.  

```{r}
bartlett.test(newtons ~ step, data = newton) %>% 
  tidy() %>%
  dplyr::select(-method) %>%
  flextable() %>%
  set_table_properties(width = 0.6, layout = "autofit") %>% 
  set_caption("Bartlett's Test for Homogeneity of Variance")
```

We could transform the response variable to remove the heterogeneity. The Box-Cox procedure suggests an inverse square root transformation.

|$\lambda$ | $Y^\lambda$ | Transformation |
|---|---|---|
|2 | $Y^2$ | Square |
|1 | $Y^1$ | (no transformation) |
|.5 | $Y^{.5}$ | Square Root |
|0 | $\ln(Y)$ | Log |
|-.5 | $Y^{-.5}$ | Inverse Square Root |
|-1 | $Y^{-1}$ | Inverse|

The Box-Cox procedure does not recommend any particular transformation of the data in this case.

```{r message=FALSE}
MASS::boxcox(newton_aov, plotit = TRUE)
```

```{r}
newton3 <- newton %>%
  mutate(newtons_isr = newtons^(-0.5))
newton_levene3 <- car::leveneTest(newtons_isr ~ step, data = newton3) 
newton_levene3 %>% 
  tidy() %>%
  flextable() %>%
  set_table_properties(width = 0.6, layout = "autofit") %>% 
  set_caption("Levene's Test for Homogeneity of Variance")
```

Huzzah - it worked! Before we continue on, we should backtrack and re-test the outliers and normality conditions. However, because the point of this section is to try Welch's ANOVA, I'm going use it instead of transforming the response variable. Use `oneway.test(..., var.equal = FALSE)` to run a Welch's ANOVA.

```{r}
newton_anova <- oneway.test(newtons ~ step, data = newton, var.equal = FALSE)
newton_anova
```

Welch's ANOVA indicates amount of force was statistically significantly different for different steps, *F*(2, 56.2) = 26.2, *p* < .0001.

I don't think you can calculate $\omega^2$ for a Welch's ANOVA object.

#### Games-Howell Post Hoc Test

Use the `PMCMRplus::gamesHowellTest()` to run the Games-Howell post hoc test. As expected, the three steps differ from each other.

```{r message=FALSE}
newton_gameshowell <- PMCMRplus::gamesHowellTest(newton_aov)
summary(newton_gameshowell)
```

Data are mean $\pm$ standard deviation. There was an increase in growth from `r pull(greenhouse_desc[1, "mean"])` $\pm$ `r pull(greenhouse_desc[1, "sd"])` in the control group to `r pull(greenhouse_desc[2, "mean"])` $\pm$ `r format(pull(greenhouse_desc[2, "sd"]), digits = 2, nsmall = 1)` in the group with fertilizer F1, an increase of `r greenhouse_tukey$group[1, "diff"]` (95% CI, `r format(greenhouse_tukey$group[1, "lwr"], digits = 2, nsmall = 1)` to `r format(greenhouse_tukey$group[1, "upr"], digits = 2, nsmall = 1)`), which was statistically significant (*p* < .0001)... etc.

#### Reporting a Welch's ANOVA

> A Welch's ANOVA was conducted to determine if force was different for different steps. Measurements were classified into three groups: A (*n* = 30), B (*n* = 30), and C (n = 30). There was a single outlier, as assessed by boxplot, and was retained because it did not change the conclusions; data was normally distributed for each group, as assessed by Q-Q plot. ; Homogeneity of variances was violated, as assessed by Levene's Test of Homogeneity of Variance (*p* = `r round(newton_levene[1, 3], 3)`). Data is presented as mean $\pm$ standard deviation. Force was statistically significantly different between different steps, *F*(`r newton_anova$parameter[1]`, `r newton_anova$parameter[2]`) = `r round(newton_anova$statistic, 3)`, p < .0005. Force increased from A (`r pull(newton_desc[1, "mean"])` $\pm$ `r format(pull(newton_desc[1,"sd"]), nsmall = 1)`), to B (`r pull(newton_desc[2, "mean"])` $\pm$ `r format(pull(newton_desc[2,"sd"]), digits = 2, nsmall = 1)`), to C (`r pull(newton_desc[3, "mean"])` $\pm$ `r format(pull(newton_desc[3,"sd"]), digits = 2, nsmall = 1)`). Games-Howell post hoc analysis revealed statistically significant increases from A to B (`r format(greenhouse_tukey$group[1, 1], digits=2, nsmall = 1)`, 95% CI (`r format(greenhouse_tukey$group[1, 2], digits=2, nsmall = 1)` to `r format(greenhouse_tukey$group[1, 3], digits=2, nsmall = 1)`), *p* = `r format(greenhouse_tukey$group[1, 4], digits=2, nsmall = 3)`), control to F2 (`r format(greenhouse_tukey$group[2, 1], digits=2, nsmall = 1)`, 95% CI (`r format(greenhouse_tukey$group[2, 2], digits=2, nsmall = 1)` to `r format(greenhouse_tukey$group[2, 3], digits=2, nsmall = 1)`), *p* = `r format(greenhouse_tukey$group[2, 4], digits=2, nsmall = 3)`), and control to F3 (`r format(greenhouse_tukey$group[3, 1], digits=2, nsmall = 1)`, 95% CI (`r format(greenhouse_tukey$group[3, 2], digits=2, nsmall = 1)` to `r format(greenhouse_tukey$group[3, 3], digits=2, nsmall = 1)`), *p* = `r format(greenhouse_tukey$group[3, 4], digits=2, nsmall = 3)`), as well as the increase from F2 to F3 (`r format(greenhouse_tukey$group[6, 1], digits=2, nsmall = 1)`, 95% CI (`r format(greenhouse_tukey$group[6, 2], digits=2, nsmall = 1)` to `r format(greenhouse_tukey$group[6, 3], digits=2, nsmall = 1)`), *p* = `r format(greenhouse_tukey$group[6, 4], digits=2, nsmall = 3)`), but there were no statistically significant group differences between F1 and F2 or F1 and F3.

> Games-Howell post hoc analysis revealed that the increase from sedentary to moderate (2.97, 95% CI (1.07 to 4.88)) was statistically significant (p = .003), as well as the increase from sedentary to high (3.35, 95% CI (1.66 to 5.05), p = .001).


### MANOVA

Multi-factor ANOVA (MANOVA) is a method to compare mean responses by treatment factor level of two or more treatments applied in combination. The null hypotheses are $H_0: \mu_{1.} = \mu_{2.} = \dots = \mu_{a.}$ for the $a$ levels of factor 1, $H_0: \mu_{.1} = \mu_{.2} = \dots = \mu_{.b}$ for the $b$ levels of factor 2, etc. for all the factors in the experiment, and $H_0: $ no interaction for all the factor interactions.

There are two equivalent ways to state the MANOVA model:

$$Y_{ijk} = \mu_{ij} + \epsilon_{ijk}$$

In this notation $Y_{ijk}$ refers to the $k^{th}$ observation in the $j^{th}$ level of factor two and the $i^{th}$ level of factor 1.  Potentially there could be additional factors.  This model formulation decomposes the response into a cell mean and an error term.  The second makes the factor effect more explicit and is thus more common:

$$Y_{ijk} = \mu + \alpha_i + \beta_j + (\alpha\beta)_{ij} +  \epsilon_{ijk}$$

### Multiple Variance Comparison F Test


### Example
*A study investigates the relationship between oxygen update and two explanatory variables: smoking, and type of stress test.  A sample of* $n = 27$ *persons, 9 non-smoking, 9 moderately-smoking, and 9 heavy-smoking are divided into three stress tests, bicycle, treadmill, and steps and their oxygen uptake was measured.  Is oxygen uptake related to smoking status and type of stress test?  Is there an interaction effect between smoking status and type of stress test?*
```{r message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2)
library(nortest)  # for Anderson-Darling test
library(stats)  # for anova

smoker <- c(1, 1, 1, 1, 1, 1, 1, 1, 1, 
            2, 2, 2, 2, 2, 2, 2, 2, 2, 
            3, 3, 3, 3, 3, 3, 3, 3, 3)
stress <- c(1, 1, 1, 2, 2, 2, 3, 3, 3,
            1, 1, 1, 2, 2, 2, 3, 3, 3,
            1, 1, 1, 2, 2, 2, 3, 3, 3)
oxytime <- c(12.8, 13.5, 11.2, 16.2, 18.1, 17.8, 22.6, 19.3, 18.9,
             10.9, 11.1, 9.8, 15.5, 13.8, 16.2, 20.1, 21.0, 15.9,
             8.7, 9.2, 7.5, 14.7, 13.2, 8.1, 16.2, 16.1, 17.8)
oxy <- data.frame(oxytime, smoker, stress)
oxy$smoker <- ordered(oxy$smoker,
                      levels = c(1, 2, 3),
                      labels = c("non-smoker", "moderate", "heavy"))
oxy$stress <- factor(oxy$stress,
                     labels = c("bicycle", "treadmill", "steps"))

lm_oxy <- lm(oxytime~smoker+stress+smoker*stress, data = oxy)
anova(lm_oxy)
```



[SFU BIO710](http://online.sfsu.edu/efc/classes/biol710/manova/MANOVAnewest.pdf)



### Repeated Measures
### Two-Way
### Randomized Blocks
### ANCOVA
## Kruskalâ€“Wallis Test



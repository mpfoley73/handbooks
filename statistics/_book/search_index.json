[["index.html", "Statistical Inference Data Analyst Handbook Preface", " Statistical Inference Data Analyst Handbook Michael Foley 2022-11-13 Preface These are notes from books, classes, tutorials, vignettes, etc. They contain mistakes, are poorly organized, and are sloppy on fundamentals. They should improve over time, but that’s all I can say for it. Use at your own risk. The focus of this handbook is statistical inference, including population estimates, group comparisons, and regression modeling. Not included here: probability, machine learning, text mining, survey analysis, or survival analysis. These subjects frequently arise at work, but are distinct enough and large enough to warrant separate handbooks. "],["statistical-inference.html", "Statistical Inference", " Statistical Inference Statistical inference is the use of a sample’s distribution to describe the population distribution. Hypothesis tests, confidence intervals, and effect size estimates are all examples of statistical inference. We wary of published study results. Identical studies might produce significant and non-significant results, yet only the significant result is likely to reach publication (publication bias). The researcher may have tortured the data until they found a statistically significant result. The study might suffer from low statistical power. Applying principles of inference can mitigate these problems. There are at least three approaches to establishing statistical inference: frequentist, likelihood, and Bayesian. Think of them philosophically. The frequentist approach is the path of action. It rejects a null hypothesis if the p-value is low because repeated sample analyses are likely to agree. The likelihood apprach is the path of knowledge. It compares the observed summary measure to the likelihoods of the other possible realities. The Bayesian approach is the path of belief. It uses a summary measure to update the prior belief. "],["frequentist-statistics.html", "Chapter 1 Frequentist Statistics", " Chapter 1 Frequentist Statistics P-values express how surprising the summary measure is given the null hypothesis (H0). Suppose you hypothesize that IQs have increased from the established mean of \\(\\mu_0\\) = 100. H0 is \\(\\mu\\) = 100 and the alternative hypothesis, H1, is \\(\\mu\\) &gt; 100. Also suppose you are right and the population mean IQ is actually 106. Finally, assume the “population” is some very big number - 1,000,000 for convenience. mu_0 &lt;- 100 mu &lt;- 106 sigma &lt;- 15 N &lt;- 1000000 pop_100 &lt;- data.frame(person = seq(1, N), iq = rnorm(N, mu_0, sigma)) pop_106 &lt;- data.frame(person = seq(1, N), iq = rnorm(N, mu, sigma)) Here is what the distribution of IQs might look like. You take a random sample of n = 30 IQs from the population. n &lt;- 30 x &lt;- sample(pop_106$iq, n) (x_bar &lt;- mean(x)) ## [1] 105.5975 You measure \\(\\bar{x}\\) = 105.6, SD = 15.2. How surprising is this result given H0 that \\(\\mu\\) is 100? I.e., what is the probability of observing \\(\\bar{x}\\) this extreme? According to the Central Limit Theorem (CLM) repeated samples of size n from a large population will yield \\(\\bar{x}\\) values that approach a normal distribution centered at \\(\\mu\\) with a standard deviation equal to the population SD (\\(\\sigma\\)) divided by \\(\\sqrt{n}\\). The standard deviation of the sampling distribution of the mean is commonly referred to as the standard error (SE). For a sample size of n = 30, and a \\(\\sigma\\) of 15, you’d expect repeated samples to converge on a mean of 100 with SE = 2.7. You can verify this empirically. sim &lt;- replicate(1000, mean(sample(pop_100$iq, 30))) Here is the distribution of \\(\\bar{x}\\) measurements from 1,000 random samples from the hypothesized population. So what is the probability of measuring \\(\\bar{x}\\) = 106? It is the probability of measuring a value \\(\\frac{\\bar{x} - \\mu_0}{\\sigma / \\sqrt{n}}\\) = 2.0 standard deviations from 100. You can look up the probability from the normal distribution. q &lt;- (x_bar - mu_0) / (sigma / sqrt(n)) pnorm(q, lower.tail = FALSE) ## [1] 0.02048121 You can get a sense of what this probability means by overlaying the sampling distribution of the mean from the presumed (H0) distribution with the sampling distribution of the mean from the actual population. The sampling distribution of the mean is centered at \\(\\bar{x}\\) = 106, and that is well into the \\(\\alpha\\) = .05 level (the yellow-shaded region). The p-value is 0.020 – the probability of measuring a mean IQ of 106 from a sample of size n = 30 when the true population mean is 100. Using an \\(\\alpha\\) level of significance, you will reject H0, which is a true positive given the fact that in this example the true population mean is 106. But you can imagine a stricter threshold level of significance, shrinking the yellow region to the right of the dashed blue line. Then we mistakenly fail to reject H0, a false negative (Type II error). "],["type-i-and-ii-errors.html", "1.1 Type I and II Errors", " 1.1 Type I and II Errors Either H0 or H1 is correct, and you must choose to either reject or not reject H0. That means there are four possible states at the end of your study. If your summary measure is extreme enough for you to declare a “positive” result and reject H0, you are either correct (true positive) or incorrect (false positive). False positives are called Type I errors. Alternatively, if your summary measure is not extreme enough for you to reject H0, you are either correct (true negative) or incorrect (false negative). False negatives are called Type II errors. The probabilities of landing in these four states depend on your chosen significance level, \\(\\alpha\\), and on the statistical power of the study, 1 - \\(\\beta\\). H0 True H1 True Significance test is positive, so you reject H0. False Positive Type I Error Probability = \\(\\alpha\\) True Positive Good Call! Probability = 1 - \\(\\beta\\) Significance test is negative, so you do not reject H0. True Negative Good Call! Probability = (\\(1 - \\alpha\\)) False Negative Type II Error Probability = \\(\\beta\\) \\(\\alpha\\) is the expected rate of Type I errors due to summary measures that are extreme by chance alone. \\(\\beta\\) is the expected rate of Type II errors due to summary measures that are not extreme by chance alone. In the IQ example, if the reality is \\(\\mu\\) = 100, any sample mean over 104.5 would be a false positive under the \\(\\alpha\\) = .05 level of significance. If the reality is \\(\\mu\\) = 106, any sample mean under 104.5 would be a false negative under the \\(1 - \\beta\\) = 0.64 statistical power of the study. "],["statistical-power.html", "1.2 Statistical Power", " 1.2 Statistical Power Type II error rates (\\(\\beta\\)) vary inversely with the power of the study (1 - \\(\\beta\\)). Statistical power is an increasing function of a) sample size, b) effect size, and c) significance level. The positive association with significance level means there is a trade-off between Type I and Type II error rates. A small \\(\\alpha\\) sets a high bar for rejecting H0, but you run the risk of failing to appreciate a real difference. On the other hand, a small \\(\\alpha\\) sets a low bar for rejecting H0, and you run the risk of judging a random difference as a real difference. The 1 - \\(\\beta\\) statistical power threshold is usually set at .80, similar to the \\(\\alpha\\) = .05 level of significance threshold convention. Given a real effect, a study with a statistical power of .80 will only find a positive test result 80% of the time. There may be such a thing as too much power, however. With a large enough sample size, even trivial effect sizes may yield a positive test result. You need to consider both sides of this coin. A power analysis is frequently used to determine the sample size required to detect a threshold effect size given an \\(\\alpha\\) level of significance criteria. A power analysis expresses the relationship among four components. If you know any three, it can return the fourth. The components are a) power (1 - \\(\\beta\\)), b) sample size (n), c) significance (\\(\\alpha\\)), and d) expected effect size (Cohen’s d). Suppose you set power at .80 and significance at .05. You can use the power test to see the relationship between sample size and effect size. Let’s do that with the IQ example. I multiplied Cohen’s d \\((\\bar{x} - \\mu_0)/\\sigma\\) by \\(\\sigma\\) to get a non-normalized effect size. The plot shows a sample size of 30 is required to detect an effect size of 7 at a .05 significance level with 80% probability. If an effect size of 5 is important, then if you want to detect it at a .05 significance level with 80% probability, you will need a sample size of at least 58 (always round up). pwr::pwr.t.test( d = 5 / sigma, sig.level = .05, power = 0.80, type = &quot;one.sample&quot;, alternative = &quot;greater&quot; ) ## ## One-sample t test power calculation ## ## n = 57.02048 ## d = 0.3333333 ## sig.level = 0.05 ## power = 0.8 ## alternative = greater "],["what-p-values-would-you-expect.html", "1.3 What p-values would you expect?", " 1.3 What p-values would you expect? This section is based on ideas I learned from homework assignment 1 in Daniel Lakens’s Coursera class Improving your statistical inferences. What distribution of p-values would you expect if there is a true effect and you repeated the study many times? What if there is no true effect? The answer is completely determined by the statistical power of the study. To see this, run 100,000 simulations of an experiment measuring the average IQ from a sample of size n = 26. The samples will be 26 random values from the normal distribution centered at 106 with a standard deviation of 15. H0 is \\(\\mu\\) = 100. # 100,000 random samples of IQ simulations from a normal distribution where # sigma = 15. True population value is 100, but we&#39;ll try other values. n_sims &lt;- 1E5 mu &lt;- 100 sigma &lt;- 15 run_sim &lt;- function(mu_0 = 106, n = 26) { data.frame(i = 1:n_sims) %&gt;% mutate( x = map(i, ~ rnorm(n = n, mean = mu_0, sd = sigma)), z = map(x, ~ t.test(., mu = mu)), p = map_dbl(z, ~ .x$p.value), x_bar = map_dbl(x, mean) ) %&gt;% select(x_bar, p) } The null hypothesis is that the average IQ is 100. Our rigged simulation finds an average IQ of 106 - an effect size of 6. sim_106_26 &lt;- run_sim(mu_0 = 106, n = 26) glimpse(sim_106_26) ## Rows: 100,000 ## Columns: 2 ## $ x_bar &lt;dbl&gt; 104.0366, 107.4146, 106.3826, 101.9452, 104.9108, 106.0985, 109.… ## $ p &lt;dbl&gt; 0.1764747839, 0.0345864346, 0.0419669200, 0.4522238256, 0.065089… mean(sim_106_26$x_bar) ## [1] 106.0062 The statistical power achieved by the simulations is 50%. That is, the typical simulation detected the effect size of 6 at the .05 significance level about 50% of the time. pwr.t.test( n = 26, d = (106 - 100) / 15, sig.level = .05, type = &quot;one.sample&quot;, alternative = &quot;two.sided&quot; ) ## ## One-sample t test power calculation ## ## n = 26 ## d = 0.4 ## sig.level = 0.05 ## power = 0.5004646 ## alternative = two.sided That means that given a population with an average IQ of 106, a two-sided hypothesis test of H0: \\(\\mu\\) = 100 from a sample of size 26 will measure an \\(\\bar{x}\\) with a p-value under .05 only 50% of the time. You can see that in this histogram of p-values. sim_106_26 %&gt;% plot_sim() Had there been no effect to observe, you’d expect all p-values to be equally likely, so the 20 bins would all have been 5% of the number of simulations – i.e., uniformly distributed under the null. This is called “0 power”, although 5% of the p-values will still be significant at the .05 level. The 5% of p-values &lt; .05 is the Type II error rate - that probability of a positive test result when there is no actual effect to observe. run_sim(mu_0 = 100, n = 26) %&gt;% plot_sim(mu_0 = 100) If you want a higher powered study that would detect the effect at least 80% of the time (the normal standard), you’ll need a higher sample size. How high? Conduct the power analysis again, but specify the power while leaving out the sample size. pwr.t.test( power = 0.80, d = (106 - 100) / 15, sig.level = .05, type = &quot;one.sample&quot;, alternative = &quot;two.sided&quot; ) ## ## One-sample t test power calculation ## ## n = 51.00945 ## d = 0.4 ## sig.level = 0.05 ## power = 0.8 ## alternative = two.sided You need 51 people (technically, you might want to round up to 52). Here’s what that looks like. 80% of p-values are below .05 now. run_sim(mu_0 = 106, n = 51) %&gt;% plot_sim(mu_0 = 106) So far, we’ve discovered that when there is an effect, the probability that the measure p-value is under the \\(\\alpha\\) significance level equals the power of the study, 1 - \\(\\beta\\) - the true positive rate, and \\(\\beta\\) will be above the \\(\\alpha\\) level - the false negative rate. We’ve also discovered that when there is no effect, all p-values are equally likely, so \\(\\alpha\\) of them will be below the \\(alpha\\) level of significance - the false positive rate, and 1 - \\(\\alpha\\) will be above \\(\\alpha\\) - the true negative rate. It’s not the case that all p-values below 0.05 are support for the alternative hypothesis. If the statistical power is high enough, a p-value just under .05 can be even less likely under the null hypothesis. run_sim(mu_0 = 108, n = 51) %&gt;% mutate(bin = case_when(p &lt; .01 ~ &quot;0.00 - 0.01&quot;, p &lt; .02 ~ &quot;0.01 - 0.02&quot;, p &lt; .03 ~ &quot;0.02 - 0.03&quot;, p &lt; .04 ~ &quot;0.03 - 0.04&quot;, p &lt; .05 ~ &quot;0.04 - 0.05&quot;, TRUE ~ &quot;other&quot;) ) %&gt;% janitor::tabyl(bin) ## bin n percent ## 0.00 - 0.01 86618 0.86618 ## 0.01 - 0.02 5011 0.05011 ## 0.02 - 0.03 2353 0.02353 ## 0.03 - 0.04 1353 0.01353 ## 0.04 - 0.05 887 0.00887 ## other 3778 0.03778 (Recall that under H0, all p-values are equally likely, so each of the percentile bins would contain 1% of p-values.) In fact, at best, a p-value between .04 and .05 can only be about four times as likely under the alternative hypothesis as the null hypothesis. If your p-value is just under .05, it is at best weak support for the alternative hypothesis. "],["further-reading.html", "1.4 Further Reading", " 1.4 Further Reading Pritha Bhandari has two nice posts on Type I and Type II errors and Statistical Power. Daniel Lakens’s Coursera class Improving your statistical inferences has a great p-value simulation exercise in Week 1 (assignment) "],["likelihood-statistics.html", "Chapter 2 Likelihood Statistics", " Chapter 2 Likelihood Statistics Likelihood functions are an approach to statistical inference (along with Frequentist and Bayesian). Likelihoods are functions of a data distribution parameter. For example, the binomial likelihood function is \\[L(\\theta) = \\frac{n!}{x!(n-x)!}\\cdot \\theta^x \\cdot (1-\\theta)^{n-x}\\] You can use the binomial likelihood function to assess the likelihoods of various hypothesized population probabilities, \\(\\theta\\). Suppose you sample n = 10 coin flips and observe x = 8 successful events (heads) for an estimated heads probability of .8. The likelihood of a fair coin, \\(\\theta\\) = .05 given the evidence is only 0.044. dbinom(8, 10, .5) ## [1] 0.04394531 You can see from the plot below that the likelihood function is maximized at \\(\\theta\\) = 0.8 (likelihood = 0.302). The actual value of the likelihood is unimportant - it’s a density. You can combine likelihood estimates by multiplying them. Suppose one experiment finds 4 of 10 heads and a second experiment finds 8 of 10 heads. You’d hope two experiments could be combined to achieve the same result as a single experiment with 12 of 20 heads, and that is indeed the case. x &lt;- dbinom(4, 10, seq(0, 1, .1)) y &lt;- dbinom(8, 10, seq(0, 1, .1)) z &lt;- dbinom(12, 20, seq(0, 1, .1)) round((x / max(x)) * (y / max(y)), 3) ## [1] 0.000 0.000 0.000 0.004 0.035 0.119 0.178 0.113 0.022 0.000 0.000 round(z, 3) ## [1] 0.000 0.000 0.000 0.004 0.035 0.120 0.180 0.114 0.022 0.000 0.000 Compare competing estimates of \\(\\theta\\) with the likelihood ratio. The likelihood of \\(\\theta\\) = .8 vs \\(\\theta\\) = .5 (fair coin) is \\(\\frac{L(\\theta = 0.8)}{L(\\theta = 0.5)}\\) = 6.87. A likelihood ratio of &gt;= 8 is moderately strong evidence for an alternative hypothesis. A likelihood ratio of &gt;= 32 is strong evidence for the alternative hypothesis. Keep in mind that likelihood ratios are relative evidence of H1 vs H0 - both hypotheses may be quite unlikely! A set of studies usually include both positive and negative test results. You can see this from the likelihood plots below. These are the likelihood curves produced from x = [0..3] successes in a sample of 3. Think of this as the likelihood of [0..3] positive findings in 3 studies based on an \\(\\alpha\\) = .05 level of significance and a .80 1 - \\(\\beta\\) statistical power of the study. The yellow line at .05 is the likelihood of a Type I error of concluding there is an effect when H1 is false. The yellow line at .80 is the likelihood of a Type II error of concluding there is no effect when H1 is true. The likelihood of 0 of 3 experiments reporting a positive effect under \\(\\alpha\\) = .05, 1 - \\(\\beta\\) = .80 is much higher under H0 (\\(\\theta\\) = .05) than under H1 (\\(\\theta\\) = .80): 0.857 vs 0.008 for a likelihood ratio of 107. The likelihood of 1 of 3 experiments reporting a positive effect is still higher under H0 than under H1: 0.135 vs 0.096 for a likelihood ratio of 1.41. For 2 of 3 experiments reporting a positive effect the likelihood ratio is 0.019, and for 3 of 3 experiments reporting a positive effect the likelihood ratio is 0.00024. The blue lines demarcates the points where mixed results are as likely as unanimous results. A set of studies are likely to produce unanimous results only if the number of studies is fairly high \\((\\gt 1 - n / (n+1))\\) or low \\((&lt; n / (n + 1))\\). "],["maximum-likelihood-estimation.html", "2.1 Maximum Likelihood Estimation", " 2.1 Maximum Likelihood Estimation Suppose a process \\(T\\) is the time to event of a process following an exponential probability distribution (notes), \\(f(T = t; \\lambda) = \\lambda e^{-\\lambda t}\\). Fitting a model to the data means estimating the distribution’s parameter, \\(\\lambda\\). The way this is typically done is by the process of maximum likelihood estimation (MLE). MLE compares the observed outcomes to those produced by the range of possible parameter values within the parameter space \\(\\lambda \\in \\Lambda\\) and chooses the parameter value that maximizes the likelihood of producing the observed outcome, \\(\\hat{\\lambda} = \\underset{\\lambda \\in \\Lambda}{\\arg\\max} \\hat{L}_t(\\lambda, t)\\). For the exponential distribution, the likelihood that \\(\\lambda\\) produces the observed outcomes is the product of the probability densities for each observation because they are a sequence of independent variables. \\[\\begin{eqnarray} L(\\lambda; t_1, t_2, \\dots, t_n) &amp;=&amp; f(t_1; \\lambda) \\cdot f(t_2; \\lambda) \\cdots f(t_n; \\lambda) \\\\ &amp;=&amp; \\Pi_{i=1}^n f(t_i; \\lambda) \\\\ &amp;=&amp; \\Pi_{i=1}^n \\lambda e^{-\\lambda t_i} \\\\ &amp;=&amp; \\lambda^n \\exp \\left(-\\lambda \\sum_{i=1}^n t_i \\right) \\end{eqnarray}\\] That is difficult to optimize, but the log of it is simple. \\[l(\\lambda; t_1, t_2, \\dots, t_n) = n \\ln(\\lambda) - \\lambda \\sum_{i=1}^n t_i\\] Maximize the log-likelihood equation by setting its derivative to zero and solving for \\(\\lambda\\). \\[\\begin{eqnarray} \\frac{d}{d \\lambda} l(\\lambda; t_1, t_2, \\dots, t_n) &amp;=&amp; \\frac{d}{d \\lambda} \\left( n \\ln(\\lambda) - \\lambda \\sum_{i=1}^n t_i \\right) \\\\ 0 &amp;=&amp; \\frac{n}{\\lambda} - \\sum_{i=1}^n t_i \\\\ \\lambda &amp;=&amp; \\frac{n}{\\sum_{i=1}^n t_i} \\end{eqnarray}\\] \\(\\lambda\\) is the reciprocal of the sample mean. "],["bayesian-statistics.html", "Chapter 3 Bayesian Statistics", " Chapter 3 Bayesian Statistics Bayesian inference estimates the probability, \\(\\theta\\), that an hypothesis is true. It differs from Frequentist inference in its insistence that all uncertainties be described by probabilities. Bayesian inference updates the prior probability distribution in light of new information. Bayesian inference builds on Bayes’ Law, so let’s start there. "],["bayes-law.html", "3.1 Bayes’ Law", " 3.1 Bayes’ Law Bayes’ Law is a clever re-ordering of the relationship between joint probability and conditional probability, \\(P(\\theta D) = P(\\theta|D)P(D) = P(D|\\theta)P(\\theta)\\), into \\[P(\\theta|D) = \\frac{P(D|\\theta)P(\\theta)}{P(D)}.\\] \\(P(\\theta)\\) is the strength of your belief in \\(\\theta\\) prior to considering the data \\(D\\). \\(P(D|\\theta)\\) is the likelihood of observing \\(D\\) from a generative model with parameter \\(\\theta\\). Note that likelihood is the probability density, and is not quite the same as probability. For a continuous variable, likelihoods can sum to greater than 1. E.g., dbinom(seq(1, 100, 1), 100, .5) sums to 1, but dnorm(seq(0,50,.001), 10, 10) sums to 841. \\(P(D)\\) is the likelihood of observing \\(D\\) from any prior. It is the marginal distribution, or prior predictive distribution of \\(D\\). The likelihood divided by the marginal distribution is the proportional adjustment made to the prior in light of the data. \\(P(\\theta|D)\\) is the strength of your belief in \\(\\theta\\) posterior to considering \\(D\\). Bayes’ Law is useful for evaluating medical tests. A test’s sensitivity is its probability of yielding a positive result \\(D\\) when condition \\(\\theta\\) exists. \\(P(D|\\theta)\\) is a test sensitivity, \\(\\mathrm{sens}\\). \\(P(\\theta)\\) is the probability of \\(\\theta\\) prior to the test (e.g., the general rate in society), \\(\\mathrm{prior}\\). The numerator of Bayes’ Law, the joint probability \\(P(D \\theta) = P(D|\\theta)P(\\theta)\\), is \\(\\mathrm{sens\\times prior}\\). A test’s specificity is the probability of observing negative test result \\(\\hat{D}\\) when the condition does not exist, \\(\\hat{\\theta}\\). The specificity is the compliment of a false positive test result, \\(P(\\hat{D} | \\hat{\\theta}) = 1 - P(D | \\hat{\\theta})\\). The denominator of Bayes’ Law is the overall probability of a positive test result, \\(P(D) = P(D|\\theta)P(\\theta) + P(D|\\hat\\theta)P(\\hat\\theta)\\) or in terms of sensitivity and specificity, \\(P(D) = \\mathrm{(sens \\times prior) + (1 - spec)(1 - prior)}\\). Example. Suppose E. Coli is typically present in 4.5% of samples, and an E. Coli screen has a sensitivity of 0.95 and a specificity of 0.99. Given a positive test result, what is the probability that E. Coli is actually present? \\[P(\\theta|D) = \\frac{.95\\cdot .045}{.95\\cdot .045 + (1 - .99)(1 - .045)} = \\frac{.04275}{.04275 + .00955} = \\frac{.04275}{.05230} = 81.7\\%.\\] The elements of Bayes’ Law come directly from the contingency table. The first row is the positive test result. The probability of E. Coli is the joint probability of E. coli and a positive test divided by the probability of a positive test E. Coli Safe Total + Test .95 * .045 = 0.04275 .01 * .955 = 0.00955 0.05230 - Test .05 * .045 = 0.00225 .99 * .955 = 0.94545 0.94770 Total 0.04500 0.95500 1.00000 "],["bayesian-inference.html", "3.2 Bayesian Inference", " 3.2 Bayesian Inference Bayesian inference extends the logic of Bayes’ Law by replacing the prior probability estimate that \\(\\theta\\) is true with a prior probability distribution that \\(\\theta\\) is true. Rather than saying, “I am x% certain \\(\\theta\\) is true,” you are saying “I believe the probability that \\(\\theta\\) is true is somewhere in a range that has maximum likelihood at x%”. Let \\(\\Pi(\\theta)\\) be the prior probability function of \\(\\theta\\). \\(\\Pi(\\theta)\\) has a pmf or pdf \\(P(\\theta)\\), and a set of conditional distributions called the generative model for the observed data \\(D\\) given \\(\\theta\\), \\(\\{f_\\theta(D): \\theta \\in \\Omega\\}\\). \\(f_\\theta(D)\\) is the likelihood of observing \\(D\\) given \\(\\theta\\). Their product, \\(f_\\theta(D)P(\\theta)\\), is a joint distribution of \\((D, \\theta)\\). For continuous prior distributions, the marginal distribution for \\(D\\), called the prior predictive distribution, is \\[m(D) = \\int_\\Omega f_\\theta(D)P(\\theta) d\\theta\\] For discrete prior distributions, replace the integral with a sum, \\(m(D) = \\sum\\nolimits_\\Omega f_\\theta(D) P(\\theta)\\). The posterior probability distribution of \\(\\theta\\), conditioned on the observance of \\(D\\), is \\(\\Pi(\\cdot|D)\\). It is the joint density, \\(f_\\theta(D) P(\\theta),\\) divided by the the marginal density, \\(m(D)\\). \\[P(\\theta | D) = \\frac{f_\\theta(D) P(\\theta)}{m(D)}\\] The numerator makes the posterior proportional to the prior. The denominator is a normalizing constant that scales the likelihood into a proper density function (whose values sum to 1). It is helpful to look first at discrete priors, a list of competing priors to see how the observed evidence shifts the probabilities of the priors into their posterior probabilities. From there it is a straight-forward step to the more abstract case of continuous prior and posterior distributions. "],["discrete-cases.html", "3.3 Discrete Cases", " 3.3 Discrete Cases Suppose you have a string of numbers \\([1,1,1,1,0,0,1,1,1,0]\\) (7 ones and 3 zeros) produced by a Bernoulli random number generator. What parameter \\(p\\) was used in the Bernoulli function?1 D &lt;- c(1, 1, 1, 1, 0, 0, 1, 1, 1, 0) Your best guess is \\(p = 0.7\\), but how confident are you? Posit eleven competing priors, \\(\\theta = [.0, .1, \\ldots, 1]\\) with equal prior probabilities, \\(P(\\theta) = [1/11, \\ldots]\\). theta &lt;- seq(0, 1, by = 0.1) prior &lt;- rep(1/11, 11) Using a Bernoulli generative model, the likelihood of observing 7 ones and 3 zeros are \\(P(D|\\theta) = \\theta^7 + (1-\\theta)^3.\\) likelihood &lt;- theta^7 * (1 - theta)^3 data.frame(theta, likelihood) %&gt;% ggplot() + geom_segment(aes(x = theta, xend = theta, y = 0, yend = likelihood), linetype = 2, color = &quot;steelblue&quot;) + geom_point(aes(x = theta, y = likelihood), color = &quot;steelblue&quot;, size = 3) + scale_x_continuous(breaks = theta) + theme_minimal() + theme(panel.grid.minor = element_blank()) + labs(title = expression(paste(&quot;Maximum likelihood of observing D is at &quot;, theta, &quot; = 0.7.&quot;)), x = expression(theta), y = expression(f[theta](D))) The posterior probability is the likelihood divided by the marginal probability of observing \\(D\\) multiplied by the prior, \\(P(\\theta|D) = \\frac{P(D|\\theta)}{P(D)}\\cdot P(\\theta).\\) In this case, the marginal probability is straight-forward to calculate: it is the sum-product of the priors and their associated likelihoods. posterior &lt;- likelihood / sum(likelihood * prior) * prior What would the posterior look like if we started with an educated guess on \\(P(\\theta)\\) that more heavily weights \\(\\theta = 0.7\\)? prior &lt;- c(.05, .05, .05, .05, .05, .10, .15, .20, .15, .10, .05) posterior &lt;- likelihood / sum(likelihood * prior) * prior What if we now employ a larger data set? To see, generate a sample of 100 Bernoulli(.7) observations. D100 &lt;- rbernoulli(100, p = 0.7) %&gt;% as.numeric() likelihood &lt;- theta^sum(D100) * (1 - theta)^(length(D100)-sum(D100)) posterior &lt;- likelihood / sum(likelihood * prior) * prior What would it look like if it also had more competing hypotheses, \\(\\theta \\in (0, .01, .02, \\ldots, 1)\\). theta &lt;- seq(0, 1, by = .01) prior &lt;- rep(1/100, 101) likelihood &lt;- theta^sum(D100) * (1 - theta)^(length(D100)-sum(D100)) posterior &lt;- likelihood / sum(likelihood * prior) * prior Example borrowed from chris’ sandbox↩︎ "],["continuous-cases.html", "3.4 Continuous Cases", " 3.4 Continuous Cases Continuing the example of inferring the parameter \\(p\\) used in the Bernoulli process, what if we considered all values between 0 and 1?2 When prior beliefs are best described in continuous distributions, express them using the beta, gamma, or normal distribution so that the posterior distributions are conjugates of the prior distributions with new parameter values. Otherwise, the marginal distribution is difficult to calculate. In this case, use the beta distribution, described by shape parameters, \\(\\alpha\\) and \\(\\beta\\). \\[P(\\theta|D,\\alpha,\\beta) = \\frac{f_\\theta(D) P(\\theta|\\alpha,\\beta)}{\\int_0^1 f_\\theta(D)P(\\theta|\\alpha, \\beta)d\\theta}\\] As with the discrete case, the numerator is the likelihood of observing \\(D\\) if \\(\\theta\\) is true multiplied by the prior probability, but now the prior is a Beta(\\(\\alpha\\), \\(\\beta\\)) distribution. The denominator, sometimes called the evidence, is the marginal probability of \\(D\\). The likelihood of observing \\(D\\) = \\(a\\) successes and \\(b\\) non-successes given a success probability of \\(p\\) = \\(\\theta\\) is \\[f_\\theta(D) = \\theta^a(1-\\theta)^b\\] The prior distribution is the probability density function of the beta distribution \\[P(\\theta|\\alpha,\\beta) = \\frac{1}{\\mathrm{B}(\\alpha, \\beta)}\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}\\] where \\(\\mathrm{B}(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)}\\) is the beta function. The marginal distribution is \\[\\int_0^1 f_\\theta(D)P(\\theta|\\alpha, \\beta)d\\theta = \\frac{\\mathrm{B}(\\alpha + a, \\beta + b)}{\\mathrm{B}(\\alpha, \\beta)}\\] Putting this all together, the posterior distribution is \\[P(\\theta|D, \\alpha, \\beta) = \\frac{1}{\\mathrm{B}(\\alpha + a, \\beta + b)} \\theta^{\\alpha-1+a}(1-\\theta)^{\\beta-1+b}\\] The posterior equals the prior with shape parameters incremented by the observed counts, \\(a\\) and \\(b.\\) plot_bayes &lt;- function(alpha, beta, a, b) { prior_ev &lt;- (alpha / (alpha + beta)) %&gt;% round(2) posterior_ev &lt;- ((alpha + a) / (alpha + beta + a + b)) %&gt;% round(2) dat &lt;- data.frame(theta = seq(0, 1, by = .01)) %&gt;% mutate(prior = (1 / beta(alpha, beta)) * theta^(alpha-1) * (1-theta)^(beta-1), prior_ci = theta &gt; qbeta(.025, alpha, beta) &amp; theta &lt; qbeta(.975, alpha, beta), likelihood = theta^a * (1-theta)^b, posterior = (1 / beta(alpha + a, beta + b)) * theta^(alpha-1+a) * (1-theta)^(beta-1+b), posterior_ci = theta &gt; qbeta(.025, alpha + a, beta + b) &amp; theta &lt; qbeta(.975, alpha + a, beta + b)) p_prior &lt;- dat %&gt;% ggplot(aes(x = theta, y = prior)) + geom_line(color = &quot;steelblue&quot;) + geom_area(aes(alpha = prior_ci), fill = &quot;steelblue&quot;) + geom_vline(xintercept = prior_ev, color = &quot;steelblue&quot;) + scale_alpha_manual(values = c(.1, .5)) + theme_minimal() + theme(legend.position = &quot;none&quot;) + labs(x = NULL) p_likelihood &lt;- dat %&gt;% ggplot(aes(x = theta, y = likelihood)) + geom_line(color = &quot;steelblue&quot;) + theme_minimal() + theme(legend.position = &quot;none&quot;) + labs(x = NULL) p_posterior &lt;- dat %&gt;% ggplot(aes(x = theta, y = posterior)) + geom_line(color = &quot;steelblue&quot;) + geom_area(aes(alpha = posterior_ci), fill = &quot;steelblue&quot;) + geom_vline(xintercept = posterior_ev, color = &quot;steelblue&quot;) + scale_alpha_manual(values = c(.1, .5)) + theme_minimal() + theme(legend.position = &quot;none&quot;) + labs(x = expression(theta)) out &lt;- p_prior / p_likelihood / p_posterior + plot_annotation( title = glue(&quot;Beta({alpha}, {beta}) prior with observed evidence a = {a} &quot;, &quot;and b = {b}&quot;), subtitle = &quot;with shaded 95% credible interval.&quot;, caption = glue(&quot;Prior expected value = {prior_ev}; Posterior expected &quot;, &quot;value = {posterior_ev}&quot;)) out } Suppose you claim complete ignorance and take a uniform Beta(1, 1) prior. Recall that you observed a = 7 ones and b = 3 zeros. The posterior expected value is still pretty close! plot_bayes(alpha = 10, beta = 10, a = 7, b = 3) Suppose you had prior reason to believe p = 0.7. You would model that as \\(\\alpha\\) = 7, \\(\\beta\\) = 3. The prior probability distribution would be \\(P(\\theta|\\alpha = 7,\\beta = 3) = \\frac{1}{\\mathrm{B}(7, 3)}\\theta^{7-1}(1-\\theta)^{3-1}\\). Then after observing a = 7 ones and b = 3 zeros, the posterior probability distribution would be \\(P(\\theta|\\alpha = 7+7,\\beta = 3+3) = \\frac{1}{\\mathrm{B}(7+7, 3+3)}\\theta^{7+7-1}(1-\\theta)^{3+3-1}\\). plot_bayes(alpha = 7, beta = 3, a = 7, b = 3) Chris’s Sandbox again.↩︎ "],["bayes-factors.html", "3.5 Bayes Factors", " 3.5 Bayes Factors The Bayes Factor (BF) is a measure of the relative evidence of one model over another. Take another look at Bayes’ formula: \\[P(\\theta|D) = \\frac{P(D|\\theta)P(\\theta)}{P(D)}.\\] Suppose you want to compare how two models explain and observed data outcome, \\(D\\). Model \\(M_1:f_1(D|\\theta_1)\\) says the observed data \\(D\\) was produced by a generative model with pdf \\(f_1\\) parameterized by \\(\\theta_2\\). Model \\(M_2:f_2(D|\\theta_2)\\) says it was produced by a generative model with pdf \\(f_2\\) parameterized by \\(\\theta_2\\). In each model you specify a prior probability distribution for the parameter If you take the ratio of the posterior probabilities, the posterior odds, the \\(P(D)\\) terms cancel and you have \\[\\frac{P(\\theta_1|D)}{P(\\theta_2|D)} = \\frac{P(D|\\theta_1)}{P(D|\\theta_2)} \\cdot \\frac{P(\\theta_1)}{P(\\theta_2)}\\] The posterior odds equals the ratio of the likelihoods multiplied by the prior odds. That likelihood ratio is the Bayes Factor (BF). Rearranging, BF is the odds ratio of the posterior and prior odds. \\[BF = \\frac{P(D|\\theta_1)}{P(D|\\theta_2)} = \\mathrm{\\frac{Posterior Odds}{Prior Odds}}\\] Return to the example of observing \\(D\\) = 7 ones and 3 zeros. You can compare an hypothesized \\(\\theta\\) of .5 to a completely agnostic model where \\(\\theta\\) is uniform over [0, 1]. The likelihood of observing \\(D\\) when \\(\\theta\\) = .5 is \\(P(D|\\theta_1) = 5^7(1-.5)^3\\) = 0.117. The likelihood of observing \\(D\\) where \\(\\theta\\) is uniform on [0, 1] is \\(P(D|\\theta_2) = \\int_0^1 \\binom{10}{3}q^7(1-q)^3dq\\) .5^1 * .5^1 ## [1] 0.25 dbinom(1, 1, .5) ## [1] 0.5 dbinom(11, 11, .5) ## [1] 0.0004882812 beta(11, 11) ## [1] 2.577402e-07 with a uniform Beta(1, 1) prior (i.e., complete agnosticism). The Bayes factor at \\(\\theta\\) = .7 quantifies how much the odds of H0: \\(\\theta\\) = .7 over H1: \\(\\hat{\\theta}\\) = .7. prior &lt;- function(theta, alpha, beta) { (1 / beta(alpha, beta)) * theta^(alpha-1) * (1-theta)^(beta-1) } posterior &lt;- function(theta, alpha, beta, a, b) { (1 / beta(alpha + a, beta + b)) * theta^(alpha-1+a) * (1-theta)^(beta-1+b) } prior(.5, 115, 85) ## [1] 1.164377 posterior(.5, 1, 1, 10, 10) ## [1] 3.700138 posterior(.5, 1, 1, 10, 10) / prior(.5, 1, 1) ## [1] 3.700138 1 / beta(115, 85) ## [1] 4.677704e+59 # Posterior Distribution 1/beta(1+10, 1+10) * .5^(1-1+10) * (1-.5)^(1-1+10) ## [1] 3.700138 dbeta(.5, 11, 11) ## [1] 3.700138 # Prior Beta Distributions 1/beta(1, 1) * .5^(1-1) * (1-.5)^(1-1) ## [1] 1 dbeta(.5, 1, 1) ## [1] 1 dbeta(.5, 115, 85) ## [1] 1.164377 The Bayes factor measures how much your prior belief is altered by the evidence. It is the ratio of the likelihoods at some hypothesized value before and after observing the data. In this case, our confidence increased by a factor of… theta &lt;- 0.5 alpha &lt;- 1 beta &lt;- 1 a &lt;- 10 b &lt;- 10 (prior_likelihood &lt;- (1 / beta(alpha, beta)) * theta^(alpha-1) * (1-theta)^(beta-1)) ## [1] 1 (posterior_likelihood &lt;- (1 / beta(alpha + a, beta + b)) * theta^(alpha-1+a) * (1-theta)^(beta-1+b)) ## [1] 3.700138 (bayes_factor &lt;- posterior_likelihood / prior_likelihood) ## [1] 3.700138 # 3.7 on alpha = beta = 1 # 1.91 on alpha = beta = 4 "],["a-gentler-introduction.html", "3.6 A Gentler Introduction", " 3.6 A Gentler Introduction This section is my notes from DataCamp course Fundamentals of Bayesian Data Analysis in R. It is an intuitive approach to Bayesian inference. Suppose you purchase 100 ad impressions on a web site and receive 13 clicks. How would you describe the click rate? The Frequentist approach would be to construct a 95% CI around the click proportion. (ad_prop_test &lt;- prop.test(13, 100)) ## ## 1-sample proportions test with continuity correction ## ## data: 13 out of 100, null probability 0.5 ## X-squared = 53.29, df = 1, p-value = 2.878e-13 ## alternative hypothesis: true p is not equal to 0.5 ## 95 percent confidence interval: ## 0.07376794 0.21560134 ## sample estimates: ## p ## 0.13 How might you model this using Bayesian reasoning? One way is to run 1,000 experiments that sample 100 ad impression events from an rbinom() generative model using a uniform prior distribution of 0-30% click probability. The resulting 1,000 row data set of click probabilities and sampled click counts forms a joint probability distribution. This method of Bayesian analysis is called rejection sampling because you sample across the whole parameter space, then condition on the observed evidence. df_sim &lt;- data.frame(click_prob = runif(1000, 0.0, 0.3)) df_sim$click_n &lt;- rbinom(1000, 100, df_sim$click_prob) Condition the joint probability distribution on the 13 observed clicks to update your prior. The quantile() function returns the median and the .025 and .975 percentile values - the credible interval. # median and credible interval (sim_ci &lt;- df_sim %&gt;% filter(click_n == 13) %&gt;% pull(click_prob) %&gt;% quantile(c(.025, .5, .975))) ## 2.5% 50% 97.5% ## 0.0901092 0.1370293 0.2007142 Your posterior click rate likelihood is 13.7% with 95 credible interval [9.0%, 20.1%]. Here is the density plot of the 43 simulations that produced the 13 clicks. The median and 95% credible interval are marked. That’s pretty close to the frequentist result! Instead of running 1,000 experiments with randomly selected click probabilities and randomly selected click counts based on those probabilities, you could define a discrete set of candidate click probabilities, e.g. values between 0 and 0.3 incremented by .01, and calculate the click probability density for the 100 ad impressions. This method of Bayesian analysis is called grid approximation. df_bayes &lt;- expand.grid( click_prob = seq(0, .3, by = .001), click_n = 0:100 ) %&gt;% mutate( prior = dunif(click_prob, min = 0, max = 0.3), likelihood = dbinom(click_n, 100, click_prob), probability = likelihood * prior / sum(likelihood * prior) ) Condition the joint probability distribution on the 13 observed clicks to update your prior. df_bayes_13 &lt;- df_bayes %&gt;% filter(click_n == 13) %&gt;% mutate(posterior = probability / sum(probability)) Instead of using the quantile() function on these values to measure the median and credible interval, resample the posterior probability to create a distribution. sampling_idx &lt;- sample( 1:nrow(df_bayes_13), size = 10000, replace = TRUE, prob = df_bayes_13$posterior ) sampling_vals &lt;- df_bayes_13[sampling_idx, ] (df_bayes_ci &lt;- quantile(sampling_vals$click_prob, c(.025, .5, .975))) ## 2.5% 50% 97.5% ## 0.078 0.134 0.209 You can use a Bayesian model to estimate multiple parameters. Suppose you want to predict the water temperature in a lake on Jun 1 based on 5 years of prior water temperatures. temp &lt;- c(19, 23, 20, 17, 23) You model the water temperature as a normal distribution, \\(\\mathrm{N}(\\mu, \\sigma^2)\\) with a prior distribution \\(\\mu = \\mathrm{N}(18, 5^2)\\) and \\(\\sigma = \\mathrm{unif}(0, 10)\\) based on past experience. Using the grid approximation approach, construct a grid of candidate \\(\\mu\\) values from 8 to 30 degrees incremented by .5 degrees, and candidate \\(\\sigma\\) values from .1 to 10 incremented by .1 - a 4,500 row data frame. mdl_grid &lt;- expand_grid(mu = seq(8, 30, by = 0.5), sigma = seq(.1, 10, by = 0.1)) For each combination of \\(\\mu\\) and \\(\\sigma\\), the prior probabilities are the densities from \\(\\mu = \\mathrm{N}(18, 5^2)\\) and \\(\\sigma = \\mathrm{unif}(0, 10)\\). The combined prior is their product. The likelihoods are the products of the probabilities of observing each temp given the candidate \\(\\mu\\) and \\(\\sigma\\) values. mdl_grid_2 &lt;- mdl_grid %&gt;% mutate( mu_prior = map_dbl(mu, ~dnorm(., mean = 18, sd = 5)), sigma_prior = map_dbl(sigma, ~dunif(., 0, 10)), prior = mu_prior * sigma_prior, # combined prior, likelihood = map2_dbl(mu, sigma, ~dnorm(temp, .x, .y) %&gt;% prod()), posterior = likelihood * prior / sum(likelihood * prior) ) Calculate a credible interval by drawing 10,000 samples from the grid with sampling probability equal to the calculated posterior probabilities. Use the quantile() function to estimate the median and .025 and .975 quantile values. sampling_idx &lt;- sample(1:nrow(mdl_grid), size = 10000, replace = TRUE, prob = mdl_grid$posterior) ## Warning: Unknown or uninitialised column: `posterior`. sampling_vals &lt;- mdl_grid[sampling_idx, c(&quot;mu&quot;, &quot;sigma&quot;)] mu_ci &lt;- quantile(sampling_vals$mu, c(.025, .5, .975)) sigma_ci &lt;- quantile(sampling_vals$sigma, c(.025, .5, .975)) ci &lt;- qnorm(c(.025, .5, .975), mean = mu_ci[2], sd = sigma_ci[2]) data.frame(temp = seq(0, 30, by = .1)) %&gt;% mutate(prob = map_dbl(temp, ~dnorm(., mean = ci[2], sd = sigma_ci[2])), ci = if_else(temp &gt;= ci[1] &amp; temp &lt;= ci[3], &quot;Y&quot;, &quot;N&quot;)) %&gt;% ggplot(aes(x = temp, y = prob)) + geom_area(aes(y = if_else(ci == &quot;N&quot;, prob, 0)), fill = &quot;firebrick&quot;, show.legend = FALSE) + geom_line() + geom_vline(xintercept = ci[2], linetype = 2) + theme_minimal() + scale_x_continuous(breaks = seq(0, 30, 5)) + theme(panel.grid.minor = element_blank()) + labs(title = &quot;Posterior temperature probability&quot;, subtitle = glue(&quot;mu = {ci[2] %&gt;% scales::number(accuracy = .1)}, 95%-CI (&quot;, &quot;{ci[1] %&gt;% scales::number(accuracy = .1)}, &quot;, &quot;{ci[3] %&gt;% scales::number(accuracy = .1)})&quot;)) What is the probability the temperature is at least 18? pred_temp &lt;- rnorm(1000, mean = sampling_vals$mu, sampling_vals$sigma) scales::percent(sum(pred_temp &gt;= 18) / length(pred_temp)) ## [1] &quot;54%&quot; "],["coursera-notes-inference.html", "3.7 Coursera Notes Inference", " 3.7 Coursera Notes Inference Bayesian inference updates prior beliefs with accumulated evidence. The posterior odds equals the prior odds multiplied by the likelihood ratio of observing evidence under the competing hypotheses. \\[\\frac{P(H1|D)}{P(H0|D)} = \\frac{P(D|H1)}{P(D|H0)} \\times \\frac{P(H1)}{P(H0)}\\] (*This isn’t quite what I expected. Wouldn’t you replace the likelihood ratio, \\(\\frac{P(D|H1)}{P(D|H0)}\\) with the proportional adjustment, \\(\\frac{P(D|H1)}{P(D)}\\)?) Express the prior https://www.bayesrulesbook.com/ "],["one-sample.html", "Chapter 4 One-Sample", " Chapter 4 One-Sample Use one-sample tests to either describe a single variable’s frequency or central tendency, or to compare the frequency or central tendency to a hypothesized distribution or value. If the data generating process produces continuous outcomes (interval or ratio), and the outcomes are symmetrically distributed, the sample mean, \\(\\bar{x}\\), is a random variable centered at the population mean, \\(\\mu\\). You can then use a theoretical distribution (normal or student t) to estimate a 95% confidence interval (CI) around \\(\\mu\\), or compare \\(\\bar{x}\\) to an hypothesized population mean, \\(\\mu_0\\). If you (somehow) know the population variance, or the Central Limit Theorem (CLT) conditions hold, you can assume the random variable is normally distributed and use the z-test, otherwise assume the random variable has student t distribution and use the t-test.3 If the data generating process produces continuous outcomes that are not symmetrically distributed, use a non-parametric test like the Wilcoxon median test. If the data generating process produces discrete outcomes (counts), the sample count, \\(x\\), is a random variable from a Poisson, binomial, normal, or multinomial distribution, or a random variable from a theoretical outcome. For counts over a fixed time or space, treat the count as a random variable from a Poisson distribution with expected value \\(\\lambda\\) and variance \\(\\lambda\\). For counts within a fixed total that are then classified into two levels (usually yes/no), then treat the count as a random variable from a binomial distribution with expected value \\(n\\pi\\) and variance \\(n\\pi(1-\\pi)\\). For binomial distributions where \\(n\\ge30\\) and the frequency counts of both levels is \\(\\ge\\) 5, treat the proportion as a random variable from the normal distribution with expected valued \\(\\pi\\) and variance \\(\\frac{\\pi(1-\\pi)}{n}\\). For counts within a fixed total that are then classified into three or more levels, treat the count as a random variable from the multinomial distribution with expected value \\(n\\pi_j\\) and variance \\(n\\pi_j(1-\\pi_j)\\). Whatever the source of the expected values, you use either the chi-squared goodness-of-fit test or G test to test whether the observed values fit the expected values from the distribution. In the special case of binary outcomes with small (n &lt; 1,000), you can use Fisher’s exact test instead. The discrete variable tests are discussed in PSU STATS 504. The t-test returns nearly the same result as the z-test when the CLT holds, so in practice no one bothers with the z-test except as an aid to teach the t-test.↩︎ "],["one-sample-mean-z-test.html", "4.1 One-Sample Mean z Test", " 4.1 One-Sample Mean z Test The z test is also called the normal approximation z test. It only applies when the sampling distribution of the population mean is normally distributed with known variance, and there are no significant outliers. The sampling distribution is normally distributed when the underlying population is normally distributed, or when the sample size is large \\((n &gt;= 30)\\), as follows from the central limit theorem. The t test returns similar results, plus it is valid when the variance is unknown, and that is pretty much always. For that reason, you probably will never use this test. Under the normal approximation method, the measured mean \\(\\bar{x}\\) approximates the population mean \\(\\mu\\), and the sampling distribution has a normal distribution centered at \\(\\mu\\) with standard error \\(se_\\mu = \\frac{\\sigma}{\\sqrt{n}}\\) where \\(\\sigma\\) is the standard deviation of the underlying population. Define a \\((1 - \\alpha)\\%\\) confidence interval as \\(\\bar{x} \\pm z_{(1 - \\alpha) {/} 2} se_\\mu\\), or test \\(H_0: \\mu = \\mu_0\\) with test statistic \\(Z = \\frac{\\bar{x} - \\mu_0}{se_\\mu}\\). Example The mtcars data set is a sample of n = 32 cars. The mean fuel economy is \\(\\bar{x} \\pm s\\) = 20.1 \\(\\pm\\) 6.0 mpg. The prior measured overall fuel economy for vehicles was \\(\\mu_0 \\pm \\sigma\\) = 18.0 \\(\\pm\\) 6.0 mpg. Has fuel economy improved? The sample size is \\(\\ge\\) 30, so the sampling distribution of the population mean is normally distributed. The population variance is known, so use the z test. \\(H_0: \\mu = 16.0\\), and \\(H_a: \\mu &gt; 16.0\\) - a right-tail test. The test statistic is \\(Z = \\frac{\\bar{x} - \\mu_0}{se_\\mu}=\\) 1.97 where \\(se_{\\mu_0} = \\frac{\\mu_0}{\\sqrt{n}} =\\) 1.06. \\(P(z &gt; Z) =\\) 0.0244, so reject \\(H_0\\) at the \\(\\alpha =\\) 0.05 level of significance. The 95% confidence interval for \\(\\mu\\) is \\(\\bar{x} \\pm z_{(1 - \\alpha){/}2} se_\\mu\\) where \\(z_{(1 - \\alpha){/}2} =\\) 1.96. \\(\\mu =\\) 20.09 \\(\\pm\\) 2.08 (95% CI 18.01 to 22.17). "],["one-sample-mean-t-test.html", "4.2 One-Sample Mean t Test", " 4.2 One-Sample Mean t Test The one-sample t test applies when the sampling distribution of the population mean is normally distributed and there are no significant outliers. Unlike the z test, the population variance can be unknown. The sampling distribution is normally distributed when the underlying population is normally distributed, or when the sample size is large \\((n &gt;= 30)\\), as follows from the central limit theorem. Under the t test method, the measured mean, \\(\\bar{x}\\), approximates the population mean, \\(\\mu\\). The sample standard deviation, \\(s\\), estimates the unknown population standard deviation, \\(\\sigma\\). The resulting sampling distribution has a t distribution centered at \\(\\mu\\) with standard error \\(se_\\bar{x} = \\frac{s}{\\sqrt{n}}\\). Define a \\((1 - \\alpha)\\%\\) confidence interval as \\(\\bar{x} \\pm t_{(1 - \\alpha){/}2} se_\\bar{x}\\) and/or test \\(H_0: \\mu = \\mu_0\\) with test statistic \\(T = \\frac{\\bar{x} - \\mu_0}{se_\\bar{x}}\\). Example A researcher recruits a random sample of n = 40 people to participate in a study about depression intervention. The researcher measures the participants’ depression level prior to the study. The mean depression score (3.72 \\(\\pm\\) 0.74) was lower than the population ‘normal’ depression score of 4.0. The null hypothesis is that the sample is representative of the overall population. Should you reject \\(H_0\\)? dep %&gt;% gtsummary::tbl_summary(statistic = list(all_continuous() ~ &quot;{mean} ({sd})&quot;)) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #gjlaqkjnpf .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #gjlaqkjnpf .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #gjlaqkjnpf .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #gjlaqkjnpf .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #gjlaqkjnpf .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #gjlaqkjnpf .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #gjlaqkjnpf .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #gjlaqkjnpf .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #gjlaqkjnpf .gt_column_spanner_outer:first-child { padding-left: 0; } #gjlaqkjnpf .gt_column_spanner_outer:last-child { padding-right: 0; } #gjlaqkjnpf .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #gjlaqkjnpf .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #gjlaqkjnpf .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #gjlaqkjnpf .gt_from_md > :first-child { margin-top: 0; } #gjlaqkjnpf .gt_from_md > :last-child { margin-bottom: 0; } #gjlaqkjnpf .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #gjlaqkjnpf .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #gjlaqkjnpf .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #gjlaqkjnpf .gt_row_group_first td { border-top-width: 2px; } #gjlaqkjnpf .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #gjlaqkjnpf .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #gjlaqkjnpf .gt_first_summary_row.thick { border-top-width: 2px; } #gjlaqkjnpf .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #gjlaqkjnpf .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #gjlaqkjnpf .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #gjlaqkjnpf .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #gjlaqkjnpf .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #gjlaqkjnpf .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #gjlaqkjnpf .gt_footnote { margin: 0px; font-size: 90%; padding-left: 4px; padding-right: 4px; padding-left: 5px; padding-right: 5px; } #gjlaqkjnpf .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #gjlaqkjnpf .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #gjlaqkjnpf .gt_left { text-align: left; } #gjlaqkjnpf .gt_center { text-align: center; } #gjlaqkjnpf .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #gjlaqkjnpf .gt_font_normal { font-weight: normal; } #gjlaqkjnpf .gt_font_bold { font-weight: bold; } #gjlaqkjnpf .gt_font_italic { font-style: italic; } #gjlaqkjnpf .gt_super { font-size: 65%; } #gjlaqkjnpf .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 75%; vertical-align: 0.4em; } #gjlaqkjnpf .gt_asterisk { font-size: 100%; vertical-align: 0; } #gjlaqkjnpf .gt_indent_1 { text-indent: 5px; } #gjlaqkjnpf .gt_indent_2 { text-indent: 10px; } #gjlaqkjnpf .gt_indent_3 { text-indent: 15px; } #gjlaqkjnpf .gt_indent_4 { text-indent: 20px; } #gjlaqkjnpf .gt_indent_5 { text-indent: 25px; } Characteristic N = 401 dep_score 3.72 (0.74) 1 Mean (SD) Conditions The one-sample t test applies when the variable is continuous and the observations are independent. Additionally, there are two conditions related to the data distribution. If either condition fails, try the suggested work-arounds or use the non-parametric [Wilcoxon 1-Sample Median Test for Numeric Var] instead. Outliers. There should be no significant outliers. Outliers exert a large influence on the mean and standard deviation. Test with a box plot. If there are outliers, you might be able to drop them or transform the data. Normality. Values should be nearly normally distributed (“nearly” because the t-test is robust to the normality assumption). This condition is especially important with small sample sizes. Test with Q-Q plots or the Shapiro-Wilk test for normality. If the data is very non-normal, you might be able to transform the data. Outliers Assess outliers with a box plot. Box plot whiskers extend up to 1.5*IQR from the upper and lower hinges and outliers (beyond the whiskers) are are plotted individually. The boxplot shows no outliers. If the outliers might are data entry errors or measurement errors, fix them or discard them. If the outliers are genuine, you have a couple options before reverting to Wilcoxon. Transform the variable. Don’t do this unless the variable is also non-normal. Transformation also has the downside of making interpretation more difficult. Leave it in if it doesn’t affect the conclusion (compared to taking it out). Normality Assume the population is normally distributed if n \\(\\ge\\) 30. Otherwise, asses a Q-Q plot, skewness and kurtosis values, or a histogram. If you still don’t feel confident about normality, run a [Shapiro-Wilk Test]. The data set has n = 40 observations, so you can assume normality. Here is a QQ plot anyway. The QQ plot indicates normality. dep %&gt;% ggplot(aes(sample = dep_score)) + stat_qq() + stat_qq_line(col = &quot;goldenrod&quot;) + theme_minimal() + labs(title = &quot;Normal Q-Q Plot&quot;) Here is the Shapiro-Wilk normality test. It fails to reject the null hypothesis of a normally distributed population. shapiro.test(dep$dep_score) ## ## Shapiro-Wilk normality test ## ## data: dep$dep_score ## W = 0.98446, p-value = 0.8474 If the data is not normally distributed, you still have a couple options before reverting to Wilcoxon. Transform the dependent variable. Carry on regardless - the one-sample t-test is fairly robust to deviations from normality. Results Conduct the t-test. To get a 95% CI around the difference (instead of around the estimate), run the test using the difference, \\(\\mu_0 - \\bar{x}\\), and leave mu at its default of 0. (dep_95ci &lt;- t.test(x = mu_0 - dep$dep_score, alternative = &quot;two.sided&quot;, conf.level = .95)) ## ## One Sample t-test ## ## data: mu_0 - dep$dep_score ## t = 2.3811, df = 39, p-value = 0.02224 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 0.04176615 0.51323385 ## sample estimates: ## mean of x ## 0.2775 The difference is statistically different from 0 at the p = .05 level. The effect size, called Cohen’s d, is defined as \\(d = |M_D| / s\\), where \\(|M_D| = \\bar{x} - \\mu_0\\), and \\(s\\) is the sample standard deviation. \\(d &lt;.2\\) is considered trivial, \\(.2 \\le d &lt; .5\\) small, and \\(.5 \\le d &lt; .8\\) large. (d &lt;- rstatix::cohens_d(dep, dep_score ~ 1, mu = 4) %&gt;% pull(effsize) %&gt;% abs()) ## Cohen&#39;s d ## 0.3764788 Cohen’s d is 0.38, a small effect. Make a habit of constructing a plot, just to make sure your head is on straight. Now you are ready to report the results. A one-sample t-test was run to determine whether depression score in recruited subjects was different from normal, as defined as a depression score of 4.0. Depression scores were normally distributed, as assessed by Shapiro-Wilk’s test (p &gt; .05) and there were no outliers in the data, as assessed by inspection of a boxplot. Data are mean \\(\\pm\\) standard deviation, unless otherwise stated. Mean depression score (3.72 \\(\\pm\\) 0.74) was lower than the population “normal” depression score of 4.00, a statistically significant difference of 0.28 (95% CI, 0.04 to 0.51), t(39) = 2.38, p = 0.022, d = 0.38. Appendix: Deciding Sample Size Determine the sample size required for a maximum error \\(\\epsilon\\) in the estimate by solving the confidence interval equation, \\(\\bar{x} \\pm t_{(1 - \\alpha){/}2} \\frac{s}{\\sqrt{n}}\\) for \\(n=\\frac{{t_{\\alpha/2,n-1}^2se^2}}{{\\epsilon^2}}\\) . Unfortunately, \\(t_{\\alpha/2,n-1}^2\\) is dependent on \\(n\\), so replace it with \\(z_{\\alpha/2}^2\\). What about \\(s^2\\)? Estimate it from the literature, a pilot study, or using the empirical rule that 95% of the range falls within two standard deviations, \\(s=range / 4\\). For example, if the maximum tolerable error is* \\(\\epsilon\\) = 3, and \\(s\\) is approximately 10, what sample size produces an \\(\\alpha\\) =0.05 confidence level? ceiling(qnorm(.975)^2 * 10^2 / 3^2) ## [1] 43 "],["one-sample-median-wilcoxon-test.html", "4.3 One-Sample Median Wilcoxon Test", " 4.3 One-Sample Median Wilcoxon Test The Wilcoxon one-sample median test (aka Wilcoxon signed rank test) is a non-parametric alternative to the t-test for cases when the the sampling distribution of the population mean is not normally distributed, but is at least symmetric. Under the Wilcoxon test, the measured median, \\(\\eta_x\\), approximates the population median, \\(\\eta\\). The method calculates the difference between each value and the hypothesized median, \\(\\eta_0\\), ranks the difference magnitudes, then sums the ranks for the negative and the positive differences, \\(W+\\) and \\(W-\\). The test compares the smaller of the two sums to a table of critical values. Here is a case study. A store claims their checkout wait times are \\(\\le\\) 4 minutes. You challenge the claim by sampling 6 checkout experiences. The mean wait time was 4.6, but the data may violate normality. data.frame(wait = wait) %&gt;% ggplot(aes(sample = wait)) + stat_qq() + stat_qq_line(col = &quot;goldenrod&quot;) + theme_minimal() + labs(title = &quot;Normal Q-Q Plot&quot;) Shapiro-Wilk rejects the null hypothesis of a normally distributed population. shapiro.test(wait) ## ## Shapiro-Wilk normality test ## ## data: wait ## W = 0.75105, p-value = 0.0204 Use the Wilcoxon test instead. (wt &lt;- wilcox.test(wait, mu = 4, alternative = &quot;greater&quot;)) ## Warning in wilcox.test.default(wait, mu = 4, alternative = &quot;greater&quot;): cannot ## compute exact p-value with ties ## ## Wilcoxon signed rank test with continuity correction ## ## data: wait ## V = 14.5, p-value = 0.2309 ## alternative hypothesis: true location is greater than 4 A Wilcoxon Signed-Ranks Test indicated that wait times were not statistically significantly higher than the 4-minute claim, z = 14.5, p = 0.231. "],["chi-squared-goodness-of-fit-test.html", "4.4 Chi-Squared Goodness-of-Fit Test", " 4.4 Chi-Squared Goodness-of-Fit Test Use the chi-squared goodness-of-fit test to test whether the observed frequency counts, \\(O_j\\), of the \\(J\\) levels of a categorical variable differ from the expected frequency counts, \\(E_j\\). \\(H_0\\) is \\(O_j = E_j\\). You can use this test for dichotomous, nominal, or ordinal variables. There are only two conditions to use this test: the observations are independent, meaning either random assignment or random sampling without replacement from &lt;10% of the population, and the expected frequency in each group is &gt;=5. The Pearson goodness-of-fit test statistic is \\[X^2 = \\sum \\frac{(O_j - E_j)^2}{E_j}\\] where \\(O_j = p_j n\\) and \\(E_j = \\pi_j n\\). The sampling distribution of \\(X^2\\) approaches the \\(\\chi_{J-1}^2\\) as the sample size \\(n \\rightarrow \\infty\\). The assumption that \\(X^2\\) is distributed \\(\\sim \\chi^2\\) is not quite correct, so you will see researchers subtract .5 from the differences to increase the p-value, the so-called Yates Continuity Correction. \\[X^2 = \\sum \\frac{(O_j - E_j - 0.5)^2}{E_j}\\] \\(X^2 \\rightarrow 0\\) as the saturated model (the observed data represent the fit of the saturated model, the most complex model possible with the data) proportions approach the expected proportions, \\(p_j \\rightarrow \\pi_j\\). The chi-squared test calculates the probability of the occurrence of \\(X^2\\) at least as extreme given that it is a chi-squared random variable with degrees of freedom equal to the number of levels of the variable minus one, \\(J-1\\). Example with Theoretical Values A researcher crosses tall cut-leaf tomatoes with dwarf potato-leaf tomatoes, then classifies the n = 1,611 offspring’s phenotype. The four phenotypes should occur with relative frequencies 9:3:3:1. The observed frequencies constitute a one-way table. If you only care about one level (or if the variable is binary) of if, conduct a one-proportion Z-test or an exact binomial test. Otherwise, conduct an exact multinomial test (recommended when n &lt;= 1,000), Pearson’s chi-squared goodness-of-fit test, or a G-test. Conditions This is a randomized experiment. The minimum expected frequency was 100, so the chi-squared test of independence is valid. Had the data violated the \\(\\ge\\) 5 condition, you could run an exact test (like the binomial, or in this case, the multinomial), or lump some factor levels together. Results You can calculate \\(X^2\\) by hand, and find the probability of a test statistic at least as extreme using the \\(\\chi^2\\) distribution with 4-1 = 3 degrees of freedom. (pheno_x2 &lt;- sum((pheno_obs - pheno_exp)^2 / pheno_exp)) ## [1] 9.54652 (pheno_p &lt;- pchisq(q = pheno_x2, df = length(pheno_type) - 1, lower.tail = FALSE)) ## [1] 0.02284158 That is what chisq.test() does. The function applies the Yates continuity correction by default, so I had to specify correct = FALSE to exclude it. In this case, setting it to TRUE has almost no effect because the sample size is large. (pheno_chisq_test &lt;- chisq.test(pheno_obs, p = pheno_pi, correct = FALSE)) ## ## Chi-squared test for given probabilities ## ## data: pheno_obs ## X-squared = 9.5465, df = 3, p-value = 0.02284 As always, plot the distribution. At this point you can report, Of the 1,611 offspring produced from the cross-fertiliation, 956 were tall cut-leaf, 258 were tall potato-leaf, 293 where dwarf cut-leaf, and 104 were dwarf potato-leaf. A chi-square goodness-of-fit test was conducted to determine whether the offspring had the same proportion of phenotypes as the theoretical distribution. The minimum expected frequency was 101. The chi-square goodness-of-fit test indicated that the number of tall cut-leaf, tall potato-leaf, dwarf cut-leaf, and dwarf potato-leaf offspring was statistically significantly different from the proportions expected in the theoretical distribution (\\(X^2\\)(3) = 9.547, p = 0.023). If you reject \\(H_0\\), inspect the residuals to learn which differences contribute most to the rejection. Notice how \\(X^2\\) is a sum of squared standardized cell differences, or “Pearson residuals”, \\[r_i = \\frac{o_j - e_j}{\\sqrt{e_j}}\\] Cells with the largest \\(|r|\\) contribute the most to the total \\(X^2\\). pheno_chisq_test$residuals^2 / pheno_chisq_test$statistic ## tall cut-leaf tall potato-leaf dwarf cut-leaf dwarf potato-leaf ## 0.28682269 0.67328098 0.02848093 0.01141540 The two “tall” cells contributed over 95% of the \\(X^2\\) test statistic, with the tall potato-leaf accounting for 67%. This aligns with what you’d expect from the bar plot. Example with Theoretical Distribution You need to reduce the degrees of freedom (df) in the chi-squared goodness-of-fit test by 1 if you test whether the data conform to a particular distribution instead of a set of theoretical values. j &lt;- c(0:5) o &lt;- c(19, 26, 29, 13, 10, 3) childr_n &lt;- as.character(0:5) Suppose you sample n = 100 families and count the number of children. The count of children is a Poisson random variable, \\(J\\), with maximum likelihood estimate \\(\\hat{\\lambda} = \\sum{j_i O_i} / \\sum{O_i}\\). Test whether the observed values can be described as samples from a Poisson random variable. The probabilities for each possible count are \\[f(j; \\lambda) = \\frac{e^{-\\hat{\\lambda}} \\hat{\\lambda}^j}{j!}.\\] Conditions This is random sampling. The minimum expected frequency was 2, so the data violates the \\(\\ge\\) 5 rule. Lump the last two categories into “4-5”. The minimum expected frequency was 6, so now the chi-squared test of independence is valid. Results Compare the expected values to the observed values with the chi-squared goodness of fit test, but in this case \\(df = 5 - 1 - 1\\) because the estimated parameter \\(\\lambda\\) reduces df by 1. You cannot set df in chisq.test(), so perform the test manually. (X2 &lt;- sum((o - e)^2 / e)) ## [1] 7.092968 (p.value &lt;- pchisq(q = X2, df = length(j) - 1 - 1, lower.tail = FALSE)) ## [1] 0.06899286 At this point you can report, Of the 100 families sampled, 19 had no children, 26 had one child, 29 had two children, 13 had three children, and 13 had 4 or 5 children. A chi-square goodness-of-fit test was conducted to determine whether the observed family sizes follow a Poisson distribution. The minimum expected frequency was 13. The chi-square goodness-of-fit test indicated that the number of children was not statistically significantly different from the proportions expected in the Poisson distribution (\\(X^2\\)(3) = 7.093, p = 0.069). "],["g-test.html", "4.5 G-Test", " 4.5 G-Test The G-test is a likelihood-ratio statistical significance test increasingly used instead of chi-squared tests. The test statistic is defined \\[G^2 = 2 \\sum O_j \\log \\left[ \\frac{O_j}{E_j} \\right]\\] where the 2 multiplier asymptotically aligns with the chi-squared test formula. G is distributed \\(\\sim \\chi^2\\), with the same number of degrees of freedom as in the corresponding chi-squared test. In fact, the chi-squared test statistic is a second order Taylor expansion of the natural logarithm around 1. Returning to the phenotype case study in the chi-squared goodness-of-fit test section, you can calculate the \\(G^2\\) test statistic and probability by hand. (pheno_g2 &lt;- 2 * sum(pheno_obs * log(pheno_obs / pheno_exp))) ## [1] 9.836806 (pchisq(q = pheno_g2, df = length(pheno_type) - 1, lower.tail = FALSE)) ## [1] 0.02000552 This is pretty close to the \\(X^2\\) = 9.547, p = 0.023 using the chi-squared goodness-of-fit test. The DescTools::GTest() function to conducts a G-test. DescTools::GTest(pheno_obs, p = pheno_pi) ## ## Log likelihood ratio (G-test) goodness of fit test ## ## data: pheno_obs ## G = 9.8368, X-squared df = 3, p-value = 0.02001 According to the function documentation, the G-test is not usually used for 2x2 tables. EMT::multinomial.test(o, f, useChisq = TRUE) ## ## The model includes 4598126 different events. ## ## The chosen number of trials is rather low, should be at least 10 times the numver of events. ## ## ## Exact Multinomial Test, Chisquare ## ## Events chi2Obs p.value ## 4598126 7.093 0.1479 chisq.test(o, e) ## Warning in chisq.test(o, e): Chi-squared approximation may be incorrect ## ## Pearson&#39;s Chi-squared test ## ## data: o and e ## X-squared = 15, df = 12, p-value = 0.2414 "],["one-sample-poisson-test.html", "4.6 One-Sample Poisson Test", " 4.6 One-Sample Poisson Test If \\(X\\) is the number of successes in \\(n\\) (many) trials when the probability of success \\(\\lambda / n\\) is small, then \\(X\\) is a random variable with a Poisson distribution, and the probability of observing \\(X = x\\) successes is \\[f(x;\\lambda) = \\frac{e^{-\\lambda} \\lambda^x}{x!} \\hspace{1cm} x \\in (0, 1, ...), \\hspace{2mm} \\lambda &gt; 0\\] with \\(E(X)=\\lambda\\) and \\(Var(X) = \\lambda\\) where \\(\\lambda\\) is estimated by the sample \\(\\hat{\\lambda}\\), \\[\\hat{\\lambda} = \\sum_{i=1}^N x_i / n.\\] Poisson sampling is used to model counts of events that occur randomly over a fixed period of time. You can use the Poisson distribution to perform an exact test on a Poisson random variable. Example You are analyzing goal totals from a sample consisting of the 95 matches in the first round of the 2002 World Cup. The average match produced a mean/sd of 1.38 \\(\\pm\\) 1.28 goals, lower than the 1.5 historical average. Should you reject the null hypothesis that the sample is representative of typical values? Conditions The events must be independent of each other. In this case, the goal-count in one match has no effect on goal-counts in other matches. The expected value of each event must be the same (homogeneity). In this case, the expected goal-count of each match is the same regardless of which teams are playing. This assumption is often dubious, causing the distribution variance to be larger than the mean, a conditional called over-dispersion. You might also check whether the data is consistent with a Poisson model. This is random sampling, but the data violates the \\(\\ge\\) 5 rule because the minimum expected frequency was 0. To comply with the minimum frequency rule, lump the last six categories into “3-8”. The minimum expected frequency was 15, so now the chi-squared test of independence is valid. Compare the expected values to the observed values with the chi-squared goodness of fit test, but in this case \\(df = 4 - 1 - 1\\) because the estimated parameter \\(\\lambda\\) reduces the df by 1. You cannot set df in chisq.test(), so perform the test manually. (X2 &lt;- sum((o - e)^2 / e)) ## [1] 0.8618219 (p.value &lt;- pchisq(q = X2, df = length(j) - 1 - 1, lower.tail = FALSE)) ## [1] 0.6499168 Of the 95 World Cup matches, 23 had no goals, 37 had one goal, 20 had two goals, and 15 had 3-8 goals. A chi-square goodness-of-fit test was conducted to determine whether the observed goal counts follow a Poisson distribution. The minimum expected frequency was 15. The chi-square goodness-of-fit test indicated that the number of goals scored was not statistically significantly different from the frequencies expected from a Poisson distribution (\\(X^2\\)(2) = 0.862, p = 0.650). Results The conditions for the exact Poisson test were met, so go ahead and run the test. (pois_val &lt;- poisson.test( x = sum(dat_pois$goals * dat_pois$freq), T = sum(dat_pois$freq), r = 1.5) ) ## ## Exact Poisson test ## ## data: sum(dat_pois$goals * dat_pois$freq) time base: sum(dat_pois$freq) ## number of events = 131, time base = 95, p-value = 0.3567 ## alternative hypothesis: true event rate is not equal to 1.5 ## 95 percent confidence interval: ## 1.152935 1.636315 ## sample estimates: ## event rate ## 1.378947 Construct a plot showing the 95% CI around the hypothesized value. For a Poisson distribution, I built the distribution around the expected value, \\(n\\lambda\\), not the rate, \\(\\lambda\\). I think you could report these results like this. A one-sample exact Poisson test was run to determine whether the number of goals scored in the first round of the 2002 World Cup was different from past World Cups, 1.5. A chi-square goodness-of-fit test indicated that the number of goals was not statistically significantly different from the counts expected in the Poisson distribution (\\(X^2\\)(2) = 0.862, p = 0.650). Data are mean \\(\\pm\\) standard deviation, unless otherwise stated. Mean goals scored (1.38 \\(\\pm\\) 1.28) was lower than the historical mean of 1.50, but was not statistically significantly different (95% CI, 1.15 to 1.64), p = 0.357. "],["exact-binomial-test.html", "4.7 Exact Binomial Test", " 4.7 Exact Binomial Test The Clopper-Pearson exact binomial test is precise, but theoretically complicated in that it inverts two single-tailed binomial tests (No theory here - I’ll just rely on the software). Use the exact binomial test if you have a small sample size or an extreme success/failure probability that invalidates the chi-square and G tests. The exact binomial also applies when you have a one-tail test. The exact binomial test has two conditions: independence, and at least \\(n\\pi \\ge 5\\) successes or \\(n(1−\\pi)\\ge 5\\) failures. You can use this test for multinomial variables too, but the test only compares a single level’s proportion to a hypothesized value. Example A pharmaceutical company claims its drug reduces fever in &gt;60% of cases. In a random sample of n = 40 cases the drug reduces fever in 20 cases. Do you reject the claim? You are testing \\(P(x \\le 20)\\) in n = 40 trials when p = 60%, a one-tail test. The sample is a random assignment experiment with 20&gt;5 successes and 20&gt;5 failures, so it meets the conditions for the exact binomial test. binom.test(20, 40, p = 0.6, alternative = &quot;greater&quot;) ## ## Exact binomial test ## ## data: 20 and 40 ## number of successes = 20, number of trials = 40, p-value = 0.9256 ## alternative hypothesis: true probability of success is greater than 0.6 ## 95 percent confidence interval: ## 0.3610917 1.0000000 ## sample estimates: ## probability of success ## 0.5 The exact binomial test uses the “method of small p-values”, in which the probability of observing a proportion \\(p\\) as far or further from \\(\\pi_0\\) is the sum of all \\(P(X=p_i)\\) where \\(p_i &lt;= p\\). map_dbl(dbinom(0:20, 40, 0.6), ~if_else(. &lt;= 0.5, ., 0)) %&gt;% sum() ## [1] 0.1297657 That is what pbinom() does. pbinom(q = 20, size = 40, p = 0.6, lower.tail = TRUE) ## [1] 0.1297657 A 95% confidence interval means 95% of confidence intervals constructed from a random sample of the population will contain the true population proportion. There are several methods to calculate a binomial confidence interval4 binom.test() uses the Clopper-Pearson interval. This method calculates lower (\\(P_L\\)) and upper (\\(P_U\\)) limits that satisfy \\[ \\begin{eqnarray} \\sum_{x=n_1}^n \\binom{n}{x} p_L^x(1 - p_L)^{n-x} &amp;=&amp; \\alpha/2\\\\ \\sum_{x=0}^{n_1} \\binom{n}{x} p_U^x(1 - p_U)^{n-x} &amp;=&amp; \\alpha/2 \\end{eqnarray} \\] where \\(n_i\\) is the measured successes in \\(n\\) trials. For a one-tail test, the confidence interval is calculated with the right side equaling 0 and \\(/alpha\\) instead of \\(\\alpha/2\\). A right-tailed 95% confidence interval means 95% of confidence intervals will contain a lower limit that is less than the true population proportion. If you wanted to construct a confidence interval around the population proportion, use a two-sided test. binom.test(20, 40, p = 0.6, alternative = &quot;two.sided&quot;) ## ## Exact binomial test ## ## data: 20 and 40 ## number of successes = 20, number of trials = 40, p-value = 0.2007 ## alternative hypothesis: true probability of success is not equal to 0.6 ## 95 percent confidence interval: ## 0.3380178 0.6619822 ## sample estimates: ## probability of success ## 0.5 If you just wanted to know whether 20 successes in 40 trials is compatible with a population proportion of 60%, then you could use the chi-squared goodness-of-fit test. chisq.test(x = c(20, 20), p = c(0.6, 0.4), correct = FALSE) ## ## Chi-squared test for given probabilities ## ## data: c(20, 20) ## X-squared = 1.6667, df = 1, p-value = 0.1967 Wikipedia.↩︎ "],["one-sample-proportion-z-test.html", "4.8 One-Sample Proportion z Test", " 4.8 One-Sample Proportion z Test The z-test uses the sample proportion of group \\(j\\), \\(p_j\\), as an estimate of the population proportion \\(\\pi_j\\) to evaluate an hypothesized population proportion \\(\\pi_{0j}\\) and/or construct a \\((1−\\alpha)\\%\\) confidence interval around \\(p_j\\) to estimate \\(\\pi_j\\) within a margin of error \\(\\epsilon\\). The z-test is intuitive to learn, but it only applies when the central limit theorem conditions hold: the sample is independently drawn, meaning random assignment (experiments), or random sampling without replacement from &lt;10% of the population (observational studies), there are at least 5 successes and 5 failures, the sample size is &gt;=30, and the expected probability of success is not extreme, between 0.2 and 0.8. If these conditions hold, the sampling distribution of \\(\\pi\\) is normally distributed around \\(p\\) with standard error \\(se_p = \\frac{s_p}{\\sqrt{n}} = \\frac{\\sqrt{p(1−p)}}{\\sqrt{n}}\\). The measured values \\(p\\) and \\(s_p\\) approximate the population values \\(\\pi\\) and \\(\\sigma_\\pi\\). You can define a \\((1 − \\alpha)\\%\\) confidence interval as \\(p \\pm z_{\\alpha / 2}se_p\\). Test the hypothesis of \\(\\pi = \\pi_0\\) with test statistic \\(z = \\frac{p − \\pi_0}{se_{\\pi_0}}\\) where \\(se_{\\pi_0} = \\frac{s_{\\pi_0}}{\\sqrt{n}} = \\frac{\\sqrt{{\\pi_0}(1−{\\pi_0})}}{\\sqrt{n}}\\). Example A machine is supposed to randomly churn out prizes in 60% of boxes. In a random sample of n = 40 boxes there are prizes in 20 boxes. Is the machine flawed? prop.test(20, 40, 0.6, &quot;two.sided&quot;, correct = FALSE) ## ## 1-sample proportions test without continuity correction ## ## data: 20 out of 40, null probability 0.6 ## X-squared = 1.6667, df = 1, p-value = 0.1967 ## alternative hypothesis: true p is not equal to 0.6 ## 95 percent confidence interval: ## 0.3519953 0.6480047 ## sample estimates: ## p ## 0.5 The first thing you’ll notice is that prop.test() performs a chi-squared goodness-of-fit test, not a one-proportion Z-test! chisq.test(c(20, 40-20), p = c(.6, .4), correct = FALSE) ## ## Chi-squared test for given probabilities ## ## data: c(20, 40 - 20) ## X-squared = 1.6667, df = 1, p-value = 0.1967 It turns out \\(P(\\chi^2 &gt; X^2)\\) equals \\(2 \\cdot P(Z &gt; z).\\) Here is the manual calculation of the chi-squared test statistic \\(X^2\\) and resulting p-value on 1 dof. pi_0 &lt;- .6 p &lt;- 20 / 40 observed &lt;- c(p, 1-p) * 40 expected &lt;- c(pi_0, 1-pi_0) * 40 X2 &lt;- sum((observed - expected)^2 / expected) pchisq(X2, 1, lower.tail = FALSE) ## [1] 0.1967056 And here is the manual calculation of the Z-test statistic \\(z\\) and resulting p-value. se &lt;- sqrt(pi_0*(1-pi_0)) / sqrt(40) z &lt;- (p - pi_0) / se pnorm(z, lower.tail = TRUE) * 2 ## [1] 0.1967056 The 95% CI presented by prop.test() is also not the \\(p \\pm z_{\\alpha / 2}se_p\\) Wald interval; it is the Wilson interval! DescTools::BinomCI(20, 40, method = &quot;wilson&quot;) ## est lwr.ci upr.ci ## [1,] 0.5 0.3519953 0.6480047 There are a lot of methods (see ?DescTools::BinomCI), and Wilson is the one Agresti-Coull recommends. If you want Wald, use DescTools::BinomCI() with method = \"wald\". DescTools::BinomCI(20, 40, method = &quot;wald&quot;) ## est lwr.ci upr.ci ## [1,] 0.5 0.3450512 0.6549488 This matches the manual calculation below. z_crit = qnorm(1 - .05/2) se &lt;- sqrt(p*(1-p)) / sqrt(40) (CI &lt;- c(p - z_crit*se, p + z_crit*se)) ## [1] 0.3450512 0.6549488 prop.test() (and chissq.test()) reported a p-value of 0.1967056, so you cannot reject the null hypothesis that \\(\\pi = 0.6\\). It’s good practice to plot this out to make sure your head is on straight. Incidentally, if you have a margin of error requirement, you can back into the required sample size to achieve it. Just solve the margin of error equation \\(\\epsilon = z_{\\alpha/2}^2 = \\sqrt{\\frac{\\pi_0(1-\\pi_0)}{n}}\\) for \\(n = \\frac{z_{\\alpha/2}^2 \\pi_0(1-\\pi_0)}{\\epsilon^2}.\\) "],["sample-t-test-for-categorical-var.html", "4.9 1 sample t Test for Categorical Var", " 4.9 1 sample t Test for Categorical Var This test applies when you do not know the population variance. "],["wilcoxon-1-sample-median-test-for-categorical-var.html", "4.10 Wilcoxon 1-Sample Median Test for Categorical Var", " 4.10 Wilcoxon 1-Sample Median Test for Categorical Var This test applies when the variable is not normally distributed. "],["group-differences.html", "Chapter 5 Group Differences", " Chapter 5 Group Differences This section presents a statistical tests of comparison. Which test to use depends on the structure of the data. Below is a guide to which test to use. Continuous (interval or ratio) and Ordinal Outcomes The Independent samples t-test (5.1) is the main way to compare a continuous dependent variable between the two levels of a binomial independent categorical variable. Revert to the nonparametric Wilcoxon Rank Sum Test (5.2) if the t-test assumptions fail. A special case arises when samples are paired. Paired samples are more like one-sample tests where the dependent variable is the difference between the pairs. Use the Paired Samples t-test (5.3) or the nonparametric Wilcoxon signed-rank test (5.4). If the independent categorical variable is multinomial, conduct an ANOVA (5.5) test or the nonparametric Kruskal-Wallis test (5.6). Discrete (count) Outcomes The Chi-square test of homogeneity (5.8) is the main way to compare a discrete dependent variable among the levels of a binomial or multinomial independent categorical variable. Revert to the nonparametric Fisher’s Exact Test (5.9) if the sample size is small. Handle the special case of paired samples with the Pairwise Prop Test (5.10) or the nonparametric McNemar’s test (5.11). "],["ttest.html", "5.1 Independent Samples t-Test", " 5.1 Independent Samples t-Test If a population measure X is normally distributed with mean \\(\\mu_X\\) and variance \\(\\sigma_X^2\\), and a population measure Y is normally distributed with mean \\(\\mu_Y\\) and variance \\(\\sigma_Y^2\\), then their difference is normally distributed with mean \\(d = \\mu_X - \\mu_Y\\) and variance \\(\\sigma_{XY}^2 = \\sigma_X^2 + \\sigma_Y^2\\). By the CLT, as the sample sizes grow, a non-normally distributed X and Y will approach normality, and so will their difference. The independent samples t-test evaluates an hypothesized difference, \\(d_0\\) (H0: \\(d = d_0\\)), from the difference in sample means \\(\\hat{d} = \\bar{x} - \\bar{y}\\), or constructs a (1 - \\(\\alpha\\))% confidence interval around \\(\\hat{d}\\) to estimate \\(d\\) within a margin of error, \\(\\epsilon\\). In principal, you can evaluate \\(\\hat{d}\\) with either a z-test or a t-test. Both require independent samples and approximately normal sampling distributions. Sampling distributions are normal if the underlying populations are normally distributed, or if the sample sizes are large (\\(n_X\\) and \\(n_Y\\) \\(\\ge\\) 30). However, the z-test additionally requires known sampling distribution variances, \\(\\sigma^2_X\\) and \\(\\sigma^2_Y\\). These variances are never known, so always use the t-test. The z-test assumes \\(d\\) is normally distributed around \\(\\hat{d} = d\\) with standard error \\(SE = \\sqrt{\\frac{\\sigma_X^2}{n_X} + \\frac{\\sigma_Y^2}{n_Y}}.\\) The test statistic for H0: \\(d = d_0\\) is \\(Z = \\frac{\\hat{d} - d_0}{SE}\\). The (1 - \\(\\alpha\\))% CI is \\(d = \\hat{d} \\pm z_{(1 - \\alpha {/} 2)} SE\\). The t-test assumes \\(d\\) has a t-distribution around \\(\\hat{d} = d\\) with standard error \\(SE = \\sqrt{\\frac{s_X^2}{n_X} + \\frac{s_Y^2}{n_Y}}.\\) The test statistic for H0: \\(d = d_0\\) is \\(T = \\frac{\\hat{d} - d_0}{SE}\\). The (1 - \\(\\alpha\\))% CI iss \\(d = \\hat{d} \\pm t_{(1 - \\alpha / 2), (n_X + n_Y - 2)} SE\\). There is a complication with the t-test SE and degrees of freedom. If the sample sizes are small and the standard deviations from each population are similar (the ratios of \\(s_X\\) and \\(s_Y\\) are &lt;2), pool the variances, \\(s_p^2 = \\frac{(n_X - 1) s_X^2 + (n_Y-1) s_Y^2}{n_X + n_Y-2}\\), so that \\(SE = s_p \\sqrt{\\frac{1}{n_X} + \\frac{1}{n_Y}}\\) and the degrees of freedom (df) = \\(n_X + n_Y - 2\\) (the pooled variances t-test). Otherwise, \\(SE = \\sqrt{\\frac{s_X^2}{n_X} + \\frac{s_Y^2}{n_Y}}\\), but you reduce df using the Welch-Satterthwaite correction, \\(df = \\frac{\\left(\\frac{s_X^2}{n_X} + \\frac{s_Y^2}{n_Y}\\right)^2}{\\frac{s_X^4}{n_X^2\\left(N_X-1\\right)} + \\frac{s_Y^4}{n_Y^2\\left(N_Y-1\\right)}}\\) (the separate variance t-test, or Welch’s t-test). "],["wilcoxonranksum.html", "5.2 Wilcoxon Rank Sum Test", " 5.2 Wilcoxon Rank Sum Test The Wilcoxon rank sum test5 is a nonparametric alternative to the independent-samples t-test. Use the the test when the samples are not normally distributed or when the response variables are ordinal rather continuous. In the first case where the normality assumption fails, the test evaluates H0 that the two samples are from the same population distribution. In the second case where the response variables are ordinal, the test evaluates the difference in medians. The Wilcoxon Rank Sum test ranks the response values, then sums the ranks for the reference group, \\(W = \\sum R_1\\). The test statistic is \\(U = W - \\frac{n_2(n_2 + 1)}{2}\\) where \\(n_2\\) is the number of observations in the test group. \\(U\\) will equal 0 if there is complete separation between the groups, and \\(n_1 n_2\\) if there is complete overlap. Reject H0 if \\(U\\) is sufficiently small. The Mann-Whitney U test is also called the Mann-Whitney U test, Wilcoxon-Mann-Whitney test, and the two-sample Wilcoxon test↩︎ "],["case-study-1.html", "Case Study 1", " Case Study 1 A company shows an advertisement to \\(n_M\\) = 20 males and \\(n_F\\) = 20 females, then measures their engagement with a survey. Do the groups’ mean engagement scores differ? Laerd has two data sets for this example. One meets the conditions for a t-test, and the other fails the normality test, forcing you to use the Mann-Whitney U test. The t-test data set has the following summary statistics. (ind_num$t_gt &lt;- ind_num$t_dat %&gt;% gtsummary::tbl_summary( by = c(gender), statistic = list(all_continuous() ~ &quot;{mean} ({sd})&quot;) )) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #yvpkngabka .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #yvpkngabka .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #yvpkngabka .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #yvpkngabka .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #yvpkngabka .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #yvpkngabka .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #yvpkngabka .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #yvpkngabka .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #yvpkngabka .gt_column_spanner_outer:first-child { padding-left: 0; } #yvpkngabka .gt_column_spanner_outer:last-child { padding-right: 0; } #yvpkngabka .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #yvpkngabka .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #yvpkngabka .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #yvpkngabka .gt_from_md > :first-child { margin-top: 0; } #yvpkngabka .gt_from_md > :last-child { margin-bottom: 0; } #yvpkngabka .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #yvpkngabka .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #yvpkngabka .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #yvpkngabka .gt_row_group_first td { border-top-width: 2px; } #yvpkngabka .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #yvpkngabka .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #yvpkngabka .gt_first_summary_row.thick { border-top-width: 2px; } #yvpkngabka .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #yvpkngabka .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #yvpkngabka .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #yvpkngabka .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #yvpkngabka .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #yvpkngabka .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #yvpkngabka .gt_footnote { margin: 0px; font-size: 90%; padding-left: 4px; padding-right: 4px; padding-left: 5px; padding-right: 5px; } #yvpkngabka .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #yvpkngabka .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #yvpkngabka .gt_left { text-align: left; } #yvpkngabka .gt_center { text-align: center; } #yvpkngabka .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #yvpkngabka .gt_font_normal { font-weight: normal; } #yvpkngabka .gt_font_bold { font-weight: bold; } #yvpkngabka .gt_font_italic { font-style: italic; } #yvpkngabka .gt_super { font-size: 65%; } #yvpkngabka .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 75%; vertical-align: 0.4em; } #yvpkngabka .gt_asterisk { font-size: 100%; vertical-align: 0; } #yvpkngabka .gt_indent_1 { text-indent: 5px; } #yvpkngabka .gt_indent_2 { text-indent: 10px; } #yvpkngabka .gt_indent_3 { text-indent: 15px; } #yvpkngabka .gt_indent_4 { text-indent: 20px; } #yvpkngabka .gt_indent_5 { text-indent: 25px; } Characteristic Male, N = 201 Female, N = 201 engagement 5.56 (0.29) 5.30 (0.39) 1 Mean (SD) There were 20 male and 20 female participants. Data are mean \\(\\pm\\) standard deviation, unless otherwise stated. The advertisement was more engaging to male viewers, 5.56 (0.29), than female viewers, 5.30 (0.39). The Mann-Whitney data set has the following summary statistics. (ind_num$mw_gt &lt;- ind_num$mw_dat %&gt;% gtsummary::tbl_summary( by = c(gender), statistic = list(all_continuous() ~ &quot;{mean} ({sd})&quot;) )) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #ceeirqtwyx .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #ceeirqtwyx .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #ceeirqtwyx .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #ceeirqtwyx .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #ceeirqtwyx .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #ceeirqtwyx .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #ceeirqtwyx .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #ceeirqtwyx .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #ceeirqtwyx .gt_column_spanner_outer:first-child { padding-left: 0; } #ceeirqtwyx .gt_column_spanner_outer:last-child { padding-right: 0; } #ceeirqtwyx .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #ceeirqtwyx .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #ceeirqtwyx .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #ceeirqtwyx .gt_from_md > :first-child { margin-top: 0; } #ceeirqtwyx .gt_from_md > :last-child { margin-bottom: 0; } #ceeirqtwyx .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #ceeirqtwyx .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #ceeirqtwyx .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #ceeirqtwyx .gt_row_group_first td { border-top-width: 2px; } #ceeirqtwyx .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #ceeirqtwyx .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #ceeirqtwyx .gt_first_summary_row.thick { border-top-width: 2px; } #ceeirqtwyx .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #ceeirqtwyx .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #ceeirqtwyx .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #ceeirqtwyx .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #ceeirqtwyx .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #ceeirqtwyx .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #ceeirqtwyx .gt_footnote { margin: 0px; font-size: 90%; padding-left: 4px; padding-right: 4px; padding-left: 5px; padding-right: 5px; } #ceeirqtwyx .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #ceeirqtwyx .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #ceeirqtwyx .gt_left { text-align: left; } #ceeirqtwyx .gt_center { text-align: center; } #ceeirqtwyx .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #ceeirqtwyx .gt_font_normal { font-weight: normal; } #ceeirqtwyx .gt_font_bold { font-weight: bold; } #ceeirqtwyx .gt_font_italic { font-style: italic; } #ceeirqtwyx .gt_super { font-size: 65%; } #ceeirqtwyx .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 75%; vertical-align: 0.4em; } #ceeirqtwyx .gt_asterisk { font-size: 100%; vertical-align: 0; } #ceeirqtwyx .gt_indent_1 { text-indent: 5px; } #ceeirqtwyx .gt_indent_2 { text-indent: 10px; } #ceeirqtwyx .gt_indent_3 { text-indent: 15px; } #ceeirqtwyx .gt_indent_4 { text-indent: 20px; } #ceeirqtwyx .gt_indent_5 { text-indent: 25px; } Characteristic Male, N = 201 Female, N = 201 engagement 5.56 (0.35) 5.43 (0.53) 1 Mean (SD) There were 20 male and 20 female participants. Data are mean \\(\\pm\\) standard deviation, unless otherwise stated. The advertisement was more engaging to male viewers, 5.56 (0.35), than female viewers, 5.43 (0.53). Conditions The independent samples t-test and Mann-Whitney U test apply when 1) the response variable is continuous, 2) the independent variable is binomial, and 3) the observations are independent. The decision between the t-test and Mann-Whitney stems from two additional conditions related to the data distribution - if both conditions hold, use the t-test; otherwise use Mann-Whitney. Outliers. There should be no outliers in either group. Outliers exert a large influence on the mean and standard deviation. Test with a box plot. If there are outliers, you might be able to drop them or transform the data. Normality. Values should be nearly normally distributed. The t-test is robust to normality, but this condition is important with small sample sizes. Test with Q-Q plots or the Shapiro-Wilk test for normality. If the data is very non-normal, you might be able to transform the data. If the data passes the two conditions, use the t-test, but now you need to check a third condition related to the variances to determine which flavor of the t-test to use. Homogeneous Variances. Use pooled-variances if the variances are homogeneous; otherwise use the separate variances method. Test with Levene’s test of equality of variances. If the data does not pass the first two conditions, use Mann-Whitney, but now you need to check a third condition here as well. The condition does not affect how to perform the test, but rather how to interpret the results. Distribution shape. If the distributions have the same shape, interpret the Mann-Whitney result as a comparison of the medians; otherwise interpret the result as a comparison of the mean ranks. Checking for Outliers Assess outliers with a box plot. Box plot whiskers extend up to 1.5*IQR from the upper and lower hinges and outliers (beyond the whiskers) are are plotted individually. For the t test data set, There were no outliers in the data, as assessed by inspection of a boxplot. and for the Mann-Whitney data set, There was one outlier in the data, as assessed by inspection of a boxplot. If the outliers are data entry errors or measurement errors, fix or discard them. If the outliers are genuine, you have a couple options before reverting to the Mann-Whitney U test. Leave it in if it doesn’t affect the conclusion (compared to taking it out). Transform the variable. Don’t do this unless the variable is also non-normal. Transformation also has the downside of making interpretation more difficult. Checking for Normality Assume the population is normally distributed if n \\(\\ge\\) 30. Otherwise, assess a Q-Q plot, skewness and kurtosis values, or a histogram. If you still don’t feel confident about normality, run a Shapiro-Wilk test. There are only \\(n_M\\) = 20 male and \\(n_F\\) = 20 female observations, so you need to test normality. The QQ plot indicates normality in the t-test data set, but not in the Mann-Whitney data set. bind_rows( `t-test` = ind_num$t_dat, `Mann-Whitney` = ind_num$mw_dat, .id = &quot;set&quot; ) %&gt;% ggplot(aes(sample = engagement, group = gender, color = fct_rev(gender))) + stat_qq() + stat_qq_line(col = &quot;goldenrod&quot;) + theme_minimal() + theme(legend.position = &quot;top&quot;) + facet_wrap(~fct_rev(set)) + labs(title = &quot;Normal Q-Q Plot&quot;, color = NULL) Run Shapiro-Wilk separately for the males and for the females. Since we are looking at two data sets in tandem, there are four tests below. For the t-test data set, (ind_num$t_shapiro &lt;- split(ind_num$t_dat, ind_num$t_dat$gender) %&gt;% map(~shapiro.test(.$engagement)) ) ## $Male ## ## Shapiro-Wilk normality test ## ## data: .$engagement ## W = 0.98344, p-value = 0.9705 ## ## ## $Female ## ## Shapiro-Wilk normality test ## ## data: .$engagement ## W = 0.96078, p-value = 0.5595 Engagement scores for each level of gender were normally distributed, as assessed by Shapiro-Wilk’s test (p &gt; .05). For the Mann-Whitney data set, (ind_num$mw_shapiro &lt;- split(ind_num$mw_dat, ind_num$mw_dat$gender) %&gt;% map(~shapiro.test(.$engagement)) ) ## $Male ## ## Shapiro-Wilk normality test ## ## data: .$engagement ## W = 0.98807, p-value = 0.9946 ## ## ## $Female ## ## Shapiro-Wilk normality test ## ## data: .$engagement ## W = 0.8354, p-value = 0.003064 Engagement scores for each level of gender were not normally distributed for the Female sample, as assessed by Shapiro-Wilk’s test (p = 0.003). If the data is not normally distributed, you still have a couple options before reverting to the Mann-Whitney U test. Transform the dependent variable. Carry on regardless - the independent samples t-test is fairly robust to deviations from normality. Checking for Homogenous Variances If the data passed the outliers and normality tests, you will use the t-test, so now you need to test the variances to see which version (pooled-variances method if variances are homogeneous; separate variances if variances are heterogeneous). A rule of thumb is that homogeneous variances have a ratio of standard deviations between 0.5 and 2.0: sd(ind_num$t_dat %&gt;% filter(gender == &quot;Male&quot;) %&gt;% pull(engagement)) / sd(ind_num$t_dat %&gt;% filter(gender == &quot;Female&quot;) %&gt;% pull(engagement)) ## [1] 0.7419967 You can also use the F test to compare the ratio of the sample variances \\(\\hat{r} = s_X^2 / s_Y^2\\) to an hypothesized ratio of population variances \\(r_0 = \\sigma_X^2 / \\sigma_Y^2 = 1.\\) var.test(ind_num$t_dat %&gt;% filter(gender == &quot;Female&quot;) %&gt;% pull(engagement), ind_num$t_dat %&gt;% filter(gender == &quot;Male&quot;) %&gt;% pull(engagement)) ## ## F test to compare two variances ## ## data: ind_num$t_dat %&gt;% filter(gender == &quot;Female&quot;) %&gt;% pull(engagement) and ind_num$t_dat %&gt;% filter(gender == &quot;Male&quot;) %&gt;% pull(engagement) ## F = 1.8163, num df = 19, denom df = 19, p-value = 0.2025 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.7189277 4.5888826 ## sample estimates: ## ratio of variances ## 1.816336 Bartlett’s test is another option. bartlett.test(ind_num$t_dat$engagement, ind_num$t_dat$gender) ## ## Bartlett test of homogeneity of variances ## ## data: ind_num$t_dat$engagement and ind_num$t_dat$gender ## Bartlett&#39;s K-squared = 1.6246, df = 1, p-value = 0.2024 Levene’s test is a third option. Levene’s is less sensitive to departures from normality than Bartlett. (ind_num$levene &lt;- with(ind_num$t_dat, car::leveneTest(engagement, gender, center = &quot;mean&quot;)) ) ## Levene&#39;s Test for Homogeneity of Variance (center = &quot;mean&quot;) ## Df F value Pr(&gt;F) ## group 1 1.922 0.1737 ## 38 There was homogeneity of variances for engagement scores for males and females, as assessed by Levene’s test for equality of variances (p = 0.174). Checking for Similar Distributions If the data fail either the outliers or the normality test, use the Mann-Whitney test. The Mann-Whitney data set failed both, so the Mann-Whitney test applies. Now you need to test the distributions to determine how to interpret its results. If the distributions are similarly shaped, interpret the Mann-Whitney U test as inferences about differences in medians between the two groups. If the distributions are dissimilar, interpret the test as inferences about the distributions, lower/higher scores and/or mean ranks. Distributions of the engagement scores for males and females were similar, as assessed by visual inspection. Test Conduct the t-test or the Mann-Whitney U test. t-Test The the t-test data the variances were equal, so the pooled-variances version applies (t.test(var.equal = TRUE)). (ind_num$t_test &lt;- t.test(engagement ~ gender, data = ind_num$t_dat, var.equal = TRUE)) ## ## Two Sample t-test ## ## data: engagement by gender ## t = 2.3645, df = 38, p-value = 0.02327 ## alternative hypothesis: true difference in means between group Male and group Female is not equal to 0 ## 95 percent confidence interval: ## 0.03725546 0.48074454 ## sample estimates: ## mean in group Male mean in group Female ## 5.558875 5.299875 There was a statistically significant difference in mean engagement score between males and females, with males scoring higher than females, 0.26 (95% CI, 0.04 to 0.48), t(38) = 2.365, p = 0.023. The effect size, Cohen’s d, is defined as \\(d = |M_D| / s\\), where \\(|M_D| = \\bar{x} - \\bar{y}\\), and \\(s\\) is the pooled sample standard deviation, \\(s_p = \\sqrt{\\frac{(n_X - 1) s_X^2 + (n_Y-1) s_Y^2}{n_X + n_Y-2}}\\). \\(d &lt;.2\\) is considered trivial, \\(.2 \\le d &lt; .5\\) small, and \\(.5 \\le d &lt; .8\\) large. (d &lt;- effectsize::cohens_d(engagement ~ gender, data = ind_num$t_dat, pooled_sd = TRUE)) ## Cohen&#39;s d | 95% CI ## ------------------------ ## 0.75 | [0.10, 1.39] ## ## - Estimated using pooled SD. There was a large difference in mean engagement score between males and females, Cohen’s d = 0.75 95% CI [0.10, 1.39] Before rejecting the null hypothesis, construct a plot as a sanity check. Wilcoxon Rank Sum test The reference level for the gender variable is males, so the Wilcoxon Rank Sum test statistic is the sum of male ranks minus \\(n_f(n_f + 1) / 2\\) where \\(n_f\\) is the number of females. You can calculate the test statistic by hand. (ind_num$mw_test_manual &lt;- ind_num$mw_dat %&gt;% mutate(R = rank(engagement)) %&gt;% group_by(gender) %&gt;% summarize(.groups = &quot;drop&quot;, n = n(), R = sum(R), meanR = sum(R)/n()) %&gt;% pivot_wider(names_from = gender, values_from = c(n, R, meanR)) %&gt;% mutate(U = R_Male - n_Female * (n_Female + 1) / 2)) ## # A tibble: 1 × 7 ## n_Male n_Female R_Male R_Female meanR_Male meanR_Female U ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 20 20 465 355 23.2 17.8 255 Compare the test statistic to the Wilcoxon rank sum distribution with pwilcox(). pwilcox( q = ind_num$mw_test_manual[1, ]$U - 1, m = ind_num$mw_test_manual[1, ]$n_Male, n = ind_num$mw_test_manual[1, ]$n_Male, lower.tail = FALSE ) * 2 ## [1] 0.141705 There is a function for all this. (ind_num$mw_test &lt;- wilcox.test( engagement ~ gender, data = ind_num$mw_dat, exact = TRUE, correct = FALSE, conf.int = TRUE)) ## ## Wilcoxon rank sum exact test ## ## data: engagement by gender ## W = 255, p-value = 0.1417 ## alternative hypothesis: true location shift is not equal to 0 ## 95 percent confidence interval: ## -0.055 0.420 ## sample estimates: ## difference in location ## 0.1925 Median engagement score was not statistically significantly different between males and females, U = 255, p = 0.142, using an exact sampling distribution for U. Now you are ready to report the results. Here is how you would report the t test. Data are mean \\(\\pm\\) standard deviation, unless otherwise stated. There were 20 male and 20 female participants. An independent-samples t-test was run to determine if there were differences in engagement to an advertisement between males and females. There were no outliers in the data, as assessed by inspection of a boxplot. Engagement scores for each level of gender were normally distributed, as assessed by Shapiro-Wilk’s test (p &gt; .05), and there was homogeneity of variances, as assessed by Levene’s test for equality of variances (p = 0.174). The advertisement was more engaging to male viewers (5.56 \\(\\pm\\) = 0.29) than female viewers (5.30 \\(\\pm\\) = 0.39), a statistically significant difference of 0.26 (95% CI, 0.04 to 0.48), t(38) = 2.365, p = 0.023, d = 0.75. Here is how you would report the Mann-Whitney U-Test. A Mann-Whitney U test was run to determine if there were differences in engagement score between males and females. Distributions of the engagement scores for males and females were similar, as assessed by visual inspection. Median engagement score for males (5.58) and females (5.38) was not statistically significantly different, U = 255, p = 0.142, using an exact sampling distribution for U. Had the distributions differed, you would report the Mann-Whitney like this: A Mann-Whitney U test was run to determine if there were differences in engagement score between males and females. Distributions of the engagement scores for males and females were not similar, as assessed by visual inspection. Engagement scores for males (mean rank = 23.25) and females (mean rank = 17.75) were not statistically significantly different, U = 255, p = 0.142, using an exact sampling distribution for U. "],["pairedttest.html", "5.3 Paired Samples t-Test", " 5.3 Paired Samples t-Test There are two common study designs that employ a paired samples t-test to compare two related groups. One relates the groups as two time points for the same subjects. The second relates the groups as two tests of the same subjects, e.g. comparing reaction time under two lighting conditions. The paired samples t-test uses the mean of sampled paired differences \\(\\bar{d}\\) as an estimate of the mean of the population paired differences \\(\\delta\\) to evaluate an hypothesized mean \\(\\delta_0\\). Test \\(H_0: \\delta = \\delta_0\\) with test statistic \\(T = \\frac{\\bar{d} - \\delta_0}{se}\\), or define a \\((1 - \\alpha)\\%\\) confidence interval as \\(\\delta = \\bar{d} \\pm t_{1 - \\alpha / 2, n - 1} se\\). The paired t-test is really just a one-sample mean t-test operating on variable that is defined as the difference between two variables. The paired samples t test applies when the sampling distribution of the mean of the population paired differences is normally distributed and there are no significant outliers. "],["wilcoxonsignedrank.html", "5.4 Wilcoxon Signed-Rank Test", " 5.4 Wilcoxon Signed-Rank Test The Wilcoxon signed-rank test is a nonparametric alternative to the paired-samples t-test for cases in which the paired differences fails the normality condition, but is at least symmetrically distributed. The test statistic is the sum product of the difference signs (-1, +1) and the rank of the difference absolute values, \\(W = \\sum_{i=1}^n sign (d_i) \\cdot R_i\\). The more differences that are of one sign, or of extreme magnitude, the larger \\(W\\) is likely to be, and the more likely to reject \\(H_0\\) of equality of medians. Sign Test The sign test is an alternative to the Wilcoxon signed-rank test for cases in which the paired differences fails the symmetrical distribution condition. The test statistic is the count of pairs whose difference is positive, \\(W = cnt(d_i &gt; 0)\\). \\(W \\sim b(n, 0.5)\\), so the sign test is really just an exact binomial test (exact sign test), or for large n-size, the normal approximation to the binomial (sign test). "],["case-study-2.html", "Case Study 2", " Case Study 2 \\(n\\) = 20 athletes consume a carb-only or carb+protein drink prior to running as far as possible in 2 hours and a researcher records their distances under each condition. Do the distances differ from 0? Laerd has three data sets for this example. One meets the conditions for a t-test. The second fails the normality condition, but is symmetric and meets the conditions for the Wilcoxon test. The third fails the symmetry condition and requires the sign test. t-test data set (drink$t_gt &lt;- drink$t_dat %&gt;% gtsummary::tbl_summary(statistic = list(all_continuous() ~ &quot;{mean} ({sd})&quot;)) ) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #qkkybgfmvo .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #qkkybgfmvo .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #qkkybgfmvo .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #qkkybgfmvo .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #qkkybgfmvo .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #qkkybgfmvo .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #qkkybgfmvo .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #qkkybgfmvo .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #qkkybgfmvo .gt_column_spanner_outer:first-child { padding-left: 0; } #qkkybgfmvo .gt_column_spanner_outer:last-child { padding-right: 0; } #qkkybgfmvo .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #qkkybgfmvo .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #qkkybgfmvo .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #qkkybgfmvo .gt_from_md > :first-child { margin-top: 0; } #qkkybgfmvo .gt_from_md > :last-child { margin-bottom: 0; } #qkkybgfmvo .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #qkkybgfmvo .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #qkkybgfmvo .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #qkkybgfmvo .gt_row_group_first td { border-top-width: 2px; } #qkkybgfmvo .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #qkkybgfmvo .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #qkkybgfmvo .gt_first_summary_row.thick { border-top-width: 2px; } #qkkybgfmvo .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #qkkybgfmvo .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #qkkybgfmvo .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #qkkybgfmvo .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #qkkybgfmvo .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #qkkybgfmvo .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #qkkybgfmvo .gt_footnote { margin: 0px; font-size: 90%; padding-left: 4px; padding-right: 4px; padding-left: 5px; padding-right: 5px; } #qkkybgfmvo .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #qkkybgfmvo .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #qkkybgfmvo .gt_left { text-align: left; } #qkkybgfmvo .gt_center { text-align: center; } #qkkybgfmvo .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #qkkybgfmvo .gt_font_normal { font-weight: normal; } #qkkybgfmvo .gt_font_bold { font-weight: bold; } #qkkybgfmvo .gt_font_italic { font-style: italic; } #qkkybgfmvo .gt_super { font-size: 65%; } #qkkybgfmvo .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 75%; vertical-align: 0.4em; } #qkkybgfmvo .gt_asterisk { font-size: 100%; vertical-align: 0; } #qkkybgfmvo .gt_indent_1 { text-indent: 5px; } #qkkybgfmvo .gt_indent_2 { text-indent: 10px; } #qkkybgfmvo .gt_indent_3 { text-indent: 15px; } #qkkybgfmvo .gt_indent_4 { text-indent: 20px; } #qkkybgfmvo .gt_indent_5 { text-indent: 25px; } Characteristic N = 201 carb 11.17 (0.73) carb_protein 11.30 (0.71) diff 0.14 (0.10) 1 Mean (SD) There were 20 participants. Data are mean \\(\\pm\\) standard deviation, unless otherwise stated. Participants ran further after consuming the carbohydrate-protein drink, 11.30 (0.71) km, than the carbohydrate-only drink, 11.17 (0.73) km. Wilcoxon data set Once you learn you need Wilcoxon or the sign-test, show the median and IQR summary statistics instead. (drink$wilcoxon_gt &lt;- drink$wilcoxon_dat %&gt;% gtsummary::tbl_summary() ) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #wkrvjkbdzu .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #wkrvjkbdzu .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #wkrvjkbdzu .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #wkrvjkbdzu .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #wkrvjkbdzu .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #wkrvjkbdzu .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #wkrvjkbdzu .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #wkrvjkbdzu .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #wkrvjkbdzu .gt_column_spanner_outer:first-child { padding-left: 0; } #wkrvjkbdzu .gt_column_spanner_outer:last-child { padding-right: 0; } #wkrvjkbdzu .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #wkrvjkbdzu .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #wkrvjkbdzu .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #wkrvjkbdzu .gt_from_md > :first-child { margin-top: 0; } #wkrvjkbdzu .gt_from_md > :last-child { margin-bottom: 0; } #wkrvjkbdzu .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #wkrvjkbdzu .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #wkrvjkbdzu .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #wkrvjkbdzu .gt_row_group_first td { border-top-width: 2px; } #wkrvjkbdzu .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #wkrvjkbdzu .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #wkrvjkbdzu .gt_first_summary_row.thick { border-top-width: 2px; } #wkrvjkbdzu .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #wkrvjkbdzu .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #wkrvjkbdzu .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #wkrvjkbdzu .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #wkrvjkbdzu .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #wkrvjkbdzu .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #wkrvjkbdzu .gt_footnote { margin: 0px; font-size: 90%; padding-left: 4px; padding-right: 4px; padding-left: 5px; padding-right: 5px; } #wkrvjkbdzu .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #wkrvjkbdzu .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #wkrvjkbdzu .gt_left { text-align: left; } #wkrvjkbdzu .gt_center { text-align: center; } #wkrvjkbdzu .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #wkrvjkbdzu .gt_font_normal { font-weight: normal; } #wkrvjkbdzu .gt_font_bold { font-weight: bold; } #wkrvjkbdzu .gt_font_italic { font-style: italic; } #wkrvjkbdzu .gt_super { font-size: 65%; } #wkrvjkbdzu .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 75%; vertical-align: 0.4em; } #wkrvjkbdzu .gt_asterisk { font-size: 100%; vertical-align: 0; } #wkrvjkbdzu .gt_indent_1 { text-indent: 5px; } #wkrvjkbdzu .gt_indent_2 { text-indent: 10px; } #wkrvjkbdzu .gt_indent_3 { text-indent: 15px; } #wkrvjkbdzu .gt_indent_4 { text-indent: 20px; } #wkrvjkbdzu .gt_indent_5 { text-indent: 25px; } Characteristic N = 201 carb 11.28 (10.43, 11.72) carb_protein 11.37 (10.92, 11.81) diff 0.19 (-0.10, 0.47) 1 Median (IQR) There were 20 participants. Data are medians and IQR unless otherwise stated. Participants ran further after consuming the carbohydrate-protein drink, 11.37 (10.92, 11.81) km, than the carbohydrate-only drink, 11.28 (10.43, 11.72) km. Sign data set (drink$sign_gt &lt;- drink$sign_dat %&gt;% gtsummary::tbl_summary() ) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #blsklmwltf .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #blsklmwltf .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #blsklmwltf .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #blsklmwltf .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #blsklmwltf .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #blsklmwltf .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #blsklmwltf .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #blsklmwltf .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #blsklmwltf .gt_column_spanner_outer:first-child { padding-left: 0; } #blsklmwltf .gt_column_spanner_outer:last-child { padding-right: 0; } #blsklmwltf .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #blsklmwltf .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #blsklmwltf .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #blsklmwltf .gt_from_md > :first-child { margin-top: 0; } #blsklmwltf .gt_from_md > :last-child { margin-bottom: 0; } #blsklmwltf .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #blsklmwltf .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #blsklmwltf .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #blsklmwltf .gt_row_group_first td { border-top-width: 2px; } #blsklmwltf .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #blsklmwltf .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #blsklmwltf .gt_first_summary_row.thick { border-top-width: 2px; } #blsklmwltf .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #blsklmwltf .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #blsklmwltf .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #blsklmwltf .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #blsklmwltf .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #blsklmwltf .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #blsklmwltf .gt_footnote { margin: 0px; font-size: 90%; padding-left: 4px; padding-right: 4px; padding-left: 5px; padding-right: 5px; } #blsklmwltf .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #blsklmwltf .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #blsklmwltf .gt_left { text-align: left; } #blsklmwltf .gt_center { text-align: center; } #blsklmwltf .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #blsklmwltf .gt_font_normal { font-weight: normal; } #blsklmwltf .gt_font_bold { font-weight: bold; } #blsklmwltf .gt_font_italic { font-style: italic; } #blsklmwltf .gt_super { font-size: 65%; } #blsklmwltf .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 75%; vertical-align: 0.4em; } #blsklmwltf .gt_asterisk { font-size: 100%; vertical-align: 0; } #blsklmwltf .gt_indent_1 { text-indent: 5px; } #blsklmwltf .gt_indent_2 { text-indent: 10px; } #blsklmwltf .gt_indent_3 { text-indent: 15px; } #blsklmwltf .gt_indent_4 { text-indent: 20px; } #blsklmwltf .gt_indent_5 { text-indent: 25px; } Characteristic N = 201 carb 11.11 (10.43, 11.49) carb_protein 11.37 (10.92, 11.81) diff 0.23 (0.12, 0.55) 1 Median (IQR) There were 20 participants. Data are median and IQR unless otherwise stated. Participants ran further after consuming the carbohydrate-protein drink, 11.37 (10.92, 11.81) km, than the carbohydrate-only drink, 11.11 (10.43, 11.49) km. Conditions The paired samples t test applies when the variable is continuous and partitioned into dependent pairs, Additionally, there are two conditions related to the data distribution. If either condition fails, consider the suggested work-around or move to the nonparametric alternatives. Outliers. There should be no outliers in the differences because they exert a large influence on the mean and standard deviation. Test with a box plot. If there are outliers, you might be able to drop them if they do not affect the conclusion, or you can transform the data. Normality. Differences should be nearly normally distributed (“nearly” because the t-test is robust to the normality assumption). This condition is especially important with small sample sizes. Test with Q-Q plots or the Shapiro-Wilk test for normality. If the data is very non-normal, you might be able to transform the data. Outliers Assess outliers with a box plot. Box plot whiskers extend up to 1.5*IQR from the upper and lower hinges and outliers (beyond the whiskers) are are plotted individually. There were no outliers in the data, as assessed by inspection of a boxplot. Had there been outliers, you might report X outliers were detected. Inspection of their values did not reveal them to be extreme and they were kept in the analysis. If the outliers are data entry errors or measurement errors, fix them or discard them. If the outliers are genuine, you can try leaving them in or transforming the data. Normality Assume the population is normally distributed if n \\(\\ge\\) 30. These data sets have n = 20 observations, so you cannot assume normality. Asses a Q-Q plot, skewness and kurtosis values, histogram, or Shapiro-Wilk test. For the t-test data set, (drink$t_shapiro &lt;- shapiro.test(drink$t_dat$diff)) ## ## Shapiro-Wilk normality test ## ## data: drink$t_dat$diff ## W = 0.97119, p-value = 0.7797 The differences between the distance ran in the carbohydrate-only and carbohydrate-protein trial were normally distributed, as assessed by Shapiro-Wilk’s test (p = 0.780). For the Wilcoxon data set, (drink$wilcoxon_shapiro &lt;- shapiro.test(drink$wilcoxon_dat$diff)) ## ## Shapiro-Wilk normality test ## ## data: drink$wilcoxon_dat$diff ## W = 0.87077, p-value = 0.01212 The differences between the distance ran in the carbohydrate-only and carbohydrate-protein trial were not normally distributed, as assessed by Shapiro-Wilk’s test (p = 0.012). For the sign-test data set, (drink$sign_shapiro &lt;- shapiro.test(drink$sign_dat$diff)) ## ## Shapiro-Wilk normality test ## ## data: drink$sign_dat$diff ## W = 0.8968, p-value = 0.03593 The differences between the distance ran in the carbohydrate-only and carbohydrate-protein trial were not normally distributed, as assessed by Shapiro-Wilk’s test (p = 0.036). If the data is normally distributed, use the t-test. If not, you try transforming the dependent variable, or carrying on regardless since the t-test is fairly robust to deviations from normality. Symmetric Distribution If the data passed the outliers test, but failed the normality test, as the Wilcoxon and sign test data sets above did, you will use the Wilcoxon signed-rank test or sign test. Now you need to test the distribution to determine which test. If the distribution is symmetric, use Wilcoxon; otherwise use the sign test. For the Wilcoxon data set, The distribution of the differences between the carbohydrate-protein drink and the carbohydrate-only was symmetric, as assessed by visual inspection. For the sign data set, The distribution of the differences between the carbohydrate-protein drink and the carbohydrate-only was not asymmetric, as assessed by visual inspection. Test t-test (drink$t_t &lt;- t.test(x = drink$t_dat$carb_protein, y = drink$t_dat$carb, paired = TRUE) ) ## ## Paired t-test ## ## data: drink$t_dat$carb_protein and drink$t_dat$carb ## t = 6.3524, df = 19, p-value = 4.283e-06 ## alternative hypothesis: true mean difference is not equal to 0 ## 95 percent confidence interval: ## 0.09085492 0.18014508 ## sample estimates: ## mean difference ## 0.1355 The carbohydrate-protein drink elicited an increase of 0.135 (95% CI, 0.091 to 0.180) km in the distance run in two hours compared to a carbohydrate-only drink. The effect size, called Cohen’s d, is the number of standard deviations the measured mean difference is from the hypothesized difference, \\((\\bar{d}-d_0) / s\\), where \\(s\\) is the sample standard deviation. .2 is small, .5 is medium, and .8 is large. This one is large. (drink$t_d &lt;- effectsize::cohens_d(drink$t_dat$diff)) ## Cohen&#39;s d | 95% CI ## ------------------------ ## 1.42 | [0.78, 2.04] You are about to reject the null hypothesis. Construct a plot as a sanity check on your reasoning. Report the results. A paired-samples t-test was used to determine the effect of a new formula of sports drink on running performance. Instead of the regular, carbohydrate-only drink, the new sports drink contains a new carbohydrate-protein mixture. Twenty participants were recruited to the study who each performed two trials in which they had to run as far as possible in two hours on a treadmill. In one of the trials they drank the carbohydrate-only drink and in the other trial they drank the carbohydrate-protein drink. The order of the trials was counterbalanced and the distance they ran in both trials was recorded. Two outliers were detected that were more than 1.5 box-lengths from the edge of the box in a boxplot. Inspection of their values did not reveal them to be extreme and they were kept in the analysis. The assumption of normality was not violated, as assessed by Shapiro-Wilk’s test (p = 0.780). Data are mean \\(\\pm\\) standard deviation, unless otherwise stated. Participants ran further after consuming the carbohydrate-protein drink, 11.30 (0.71) km, than the carbohydrate-only drink, 11.17 (0.73) km, a statistically significant increase of 0.135 (95% CI, 0.091 to 0.180) km, t(19) = 6.352, p = 0.0000, d = 1.42. Wilcoxon Signed-Rank Test From the distribution plot, you can see that most of the signs were positive, and the largest absolute difference values were among the positives, so expect a pretty large test statistic. (drink$wilcoxon_test &lt;- wilcox.test(drink$wilcoxon_dat$carb_protein, drink$wilcoxon_dat$carb, paired = TRUE)) ## ## Wilcoxon signed rank exact test ## ## data: drink$wilcoxon_dat$carb_protein and drink$wilcoxon_dat$carb ## V = 162, p-value = 0.03277 ## alternative hypothesis: true location shift is not equal to 0 The carbohydrate-protein drink elicited a statistically significant median increase in distance run in two hours compared to the carbohydrate-only drink, W = 162, p = 0.033. Report the results. A Wilcoxon signed-rank test was conducted to determine the effect of a new formula of sports drink on running performance. Instead of the regular, carbohydrate-only drink, the new sports drink contains a new carbohydrate-protein mixture. Twenty participants were recruited to the study who each performed two trials in which they had to run as far as possible in two hours on a treadmill. In one of the trials they drank the carbohydrate-only drink and in the other trial they drank the carbohydrate-protein drink. The order of the trials was counterbalanced and the distance they ran in both trials was recorded. The difference scores were approximately symmetrically distributed, as assessed by a histogram with superimposed normal curve. Data are medians unless otherwise stated. Of the 20 participants recruited to the study, the carbohydrate-protein drink elicited an increase in the distance run in 17 participants compared to the carbohydrate-only drink, whereas two participants saw no improvement and one participant did not run as far with the carbohydrate-protein drink. There was a statistically significant median increase in distance run (0.2300 km) when subjects imbibed the carbohydrate-protein drink (11.368 km) compared to the carbohydrate-only drink (11.108 km), W = 162, p = 0.0328. Sign Signed-Rank Test Conduct the exact sign test since the n-size is not so large that we need the normal approximation to the binomial. Notice n is the count of non-zero differences. (drink$sign_test &lt;- binom.test(sum(drink$sign_dat$diff &gt; 0), n = sum(drink$sign_dat$diff != 0))) ## ## Exact binomial test ## ## data: sum(drink$sign_dat$diff &gt; 0) and sum(drink$sign_dat$diff != 0) ## number of successes = 18, number of trials = 18, p-value = 7.629e-06 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.814698 1.000000 ## sample estimates: ## probability of success ## 1 The carbohydrate-protein drink elicited a statistically significant median increase in distance run (0.230 km) compared to the carbohydrate-only drink, p = 0.000. Report the results. An exact sign test was conducted to determine the effect of a new formula of sports drink on running performance. Instead of the regular, carbohydrate-only drink, the new sports drink contains a new carbohydrate-protein mixture. Twenty participants were recruited to the study who each performed two trials in which they had to run as far as possible in two hours on a treadmill. In one of the trials they drank the carbohydrate-only drink and in the other trial they drank the carbohydrate-protein drink. The order of the trials was counterbalanced and the distance they ran in both trials was recorded. An exact sign test was used to determine whether there was a statistically significant median difference between the distance ran when participants drank a carbohydrate-protein drink compared to a carbohydrate-only drink. Data are medians unless otherwise stated. Of the 20 participants recruited to the study, the carbohydrate-protein drink elicited an increase in the distance run in 18 participants compared to the carbohydrate-only drink, whereas 0 participants did not run as far and 2 participant saw no improvement with the carbohydrate-protein drink. There was a statistically significant median increase in distance run (0.2300 km) when subjects imbibed the carbohydrate-protein drink (11.368 km) compared to the carbohydrate-only drink (11.108 km), p = 0.0000. "],["onewayanova.html", "5.5 One-way ANOVA", " 5.5 One-way ANOVA Analysis of variance (ANOVA) is a method to compare the mean values of a continuous variable between groups of a categorical independent variable. ANOVA is typically used to analyze the response to a manipulation of the independent variable in a controlled experiment, but it can also be used to analyze the difference in the observed value among groups in a non-experimental setting.6 How it Works ANOVA decomposes the variability around the overall mean \\(Y_{ij} - \\bar{Y}_{..}\\) into two parts: the variability of the factor level means around the overall mean \\(\\bar{Y}_{i.} - \\bar{Y}_{..}\\) (between-group variability) plus the variability of the factor level values around their means \\(Y_{ij} - \\bar{Y}_{i.}\\) (within-group variability). In the table below, the ratio of the treatment mean square and the mean squared error, \\(F = \\frac{MSR}{MSE}\\), follows an F distribution with \\(k-1\\) numerator dof and \\(N-k\\) denominator dof. The more observation variance captured by the treatments, the larger is the between-group variability relative to the within-group variability, and thus the larger is \\(F\\), and the less likely that the null hypothesis, \\(H_0 = \\mu_1 = \\mu_2 = \\cdots = \\mu_k\\) is true. Table 5.1: ANOVA Table Source SS df MS F \\(SSR\\) \\(\\sum{n_i(\\bar{Y}_{i.} - \\bar{Y}_{..})^2}\\) \\(k - 1\\) \\({SSR}/{(k - 1)}\\) \\({MSR}/{MSE}\\) \\(SSE\\) \\(\\sum(Y_{ij} - \\bar{Y}_{i.})^2\\) \\(N - k\\) \\({SSE}/{(N - k)}\\) \\(SST\\) \\(\\sum(Y_{ij} - \\bar{Y}_{..})^2\\) \\(N - 1\\) Assumptions The ANOVA test applies when the independent variable is categorical, and the dependent variable is continuous and independent within groups. Independence means the observations are from a random sample, or from an experiment using random assignment. Each group’s size should be less than 10% of its population size. The groups must also be independent of each other (non-paired, and non-repeated measures). Additionally, there are three assumptions related to the distribution of the dependent variable. If any assumption fails, either try the work-around or revert to the nonparametric Kruskal-Wallis test (Chapter 5.6). No outliers. There should be no significant outliers in the groups. Outliers exert a large influence on the mean and variance. Test with a box plot or residuals vs predicted plot. Work-arounds are dropping the outliers or transforming the dependent variable. Normality. The dependent variable should be nearly normally distributed. ANOVA is robust to this condition, but it important with small sample sizes. Test with the Q-Q plots or the Shapiro-Wilk test for normality. Work-around is transforming the dependent variable. Equal Variances. The group variances should be roughly equal. This condition is especially important with differing sample sizes. Test with a box plot, residuals vs predicted plot, rule of thumb (see case study in Chapter 5.7), or one of the formal homogeneity of variance tests such as Bartlett and Levene (be careful here because the formal tests can be overly sensitive, esp. Bartlett). Work-around is the Games-Howell post hoc test instead of the Tukey post hoc test. Post Hoc Tests If the ANOVA procedure rejects the null hypothesis, use a post hoc procedure to determine which groups differ. The Tukey test is the most common. The test compares the differences in means to Tukey’s \\(w\\), \\(w = q_\\alpha(p, df_{Err}) \\cdot s_\\bar{Y}\\) where \\(q_\\alpha(p, df_{Err})\\) is a lookup table value, and \\(s_\\bar{Y} = \\sqrt{MSE/r}\\) and \\(r\\) is the number of comparisons. Any difference in group means greater than Tukey’s \\(w\\) is statistically significant. The Tukey test is only valid with equal sample sizes. Otherwise, the Tukey–Cramer method calculates the standard deviation for each pairwise comparison separately. There are other post hoc tests. Fisher’s Protected Least Significant Difference (LSD) test is an older approach and less commonly used today. The Bonferroni and Scheffe methods are used for general tests of contrasts, including combinations of groups. The Bonferroni method is better when the number of contrasts is about the same as the number of factor levels. The Scheffe method is better for testing all possible contrasts. Dunnett’s mean comparison method is appropriate for comparisons of treatment levels against a control. ANOVA and OLS ANOVA is related to linear regression. The regression model intercept is the overall mean and the coefficient estimators indirectly indicate the group means. The analysis of variance table in a regression model shows how much of the overall variance is explained by those coefficient estimators. It’s the same thing. These notes are gleaned from PSU STAT-502 “Analysis of Variance and Design of Experiments”, and Laerd Statistics.↩︎ "],["kw.html", "5.6 Kruskal–Wallis Test", " 5.6 Kruskal–Wallis Test The Kruskal-Wallis H test7 measures the difference of a continuous or ordinal dependent variable between groups of a categorical independent variable. It is a rank-based nonparametric alternative to the one-way ANOVA test. Use Kruskal-Wallis if the dependent variable fails ANOVA’s normality or homogeneity conditions, or if it is ordinal. How it Works The Kruskal-Wallis H test ranks the dependent variable irrespective of its group. The test statistic is a function of the averaged square of the rank sum per group: \\[ H = \\left[ \\frac{12}{n(n+1)} \\sum_{j} \\frac{T_j^2}{n_j} \\right] - 3(n + 1) \\] where \\(T_j\\) is the sum of the ranks of group j. The test statistic approximately follows a \\(\\chi^2\\) distribution with k – 1 degrees of freedom, where k is the number of groups of the independent variable. The null hypothesis is that the rank means are equal. If you reject the null hypothesis, run a post hoc test to determine which groups differ. Assumptions Kruskal-Wallis has no assumptions per se, but the test interpretation depends on the distribution of the dependent variable. If its distribution has a similar shape across the groups of the categorical independent variable, then Kruskal-Wallis is a test of differences in their medians. Otherwise, Kruskal-Wallis is a test of differences in their distributions. The Kruskal-Wallis H test is also called the one-way ANOVA on ranks↩︎ "],["groupdiffscs3.html", "5.7 Case Study 3", " 5.7 Case Study 3 This case study uses the data set from Laerd Statistics for ANOVA. cs3 &lt;- list() # Data sets are the same, so just use one. # cs3$kw_dat &lt;- read.spss(&quot;./input/kruskal-wallis-h-test.sav&quot;, to.data.frame = TRUE) # cs3$anova_dat &lt;- read.spss(&quot;./input/one-way-anova.sav&quot;, to.data.frame = TRUE) cs3$dat &lt;- read.spss(&quot;./input/kruskal-wallis-h-test.sav&quot;, to.data.frame = TRUE) A study tests whether physically active individuals are better able to cope with workplace stress. The study categorizes \\(n\\) = 31 participants by physical activity level (“Sedentary”, “Low”, “Moderate”, and “High”) and measures their ability to cope with workplace-related stress (CWWS) as the average score of a series of Likert items on a questionnaire (higher scores indicating a greater CWWS ability). The means plot8 and summary table are an initial look at the data. cs3$dat %&gt;% group_by(group) %&gt;% summarize( .groups = &quot;drop&quot;, mean_coping_stress = mean(coping_stress), cl_025 = mean_coping_stress + qnorm(.025) * sd(coping_stress) / sqrt(n()), cl_975 = mean_coping_stress + qnorm(.975) * sd(coping_stress) / sqrt(n()), n = n() ) %&gt;% ggplot(aes(x = group, y = mean_coping_stress)) + geom_point(shape = 21, fill = &quot;gray80&quot;, color = &quot;black&quot;, size = 3) + geom_errorbar(aes(ymin = cl_025, ymax = cl_975, width = 0.1)) + geom_text(aes(y = 2, label = glue(&quot;n = {n}&quot;)), size = 3) + labs(title = &quot;Distribution of CWWS by Physical Activity Level Group&quot;, x = NULL, y = &quot;Score&quot;, caption = &quot;Means plot with 95% CI&quot;) (cs3$gt &lt;- cs3$dat %&gt;% tbl_summary( by = group, label = list(coping_stress = &quot;CWWR&quot;), type = coping_stress ~ &quot;continuous2&quot;, statistic = coping_stress ~ c(&quot;{median} ({p25}, {p75})&quot;, &quot;{mean}, {sd}&quot;) ) %&gt;% add_n()) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #onrccbtthz .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #onrccbtthz .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #onrccbtthz .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #onrccbtthz .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #onrccbtthz .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #onrccbtthz .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #onrccbtthz .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #onrccbtthz .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #onrccbtthz .gt_column_spanner_outer:first-child { padding-left: 0; } #onrccbtthz .gt_column_spanner_outer:last-child { padding-right: 0; } #onrccbtthz .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #onrccbtthz .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #onrccbtthz .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #onrccbtthz .gt_from_md > :first-child { margin-top: 0; } #onrccbtthz .gt_from_md > :last-child { margin-bottom: 0; } #onrccbtthz .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #onrccbtthz .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #onrccbtthz .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #onrccbtthz .gt_row_group_first td { border-top-width: 2px; } #onrccbtthz .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #onrccbtthz .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #onrccbtthz .gt_first_summary_row.thick { border-top-width: 2px; } #onrccbtthz .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #onrccbtthz .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #onrccbtthz .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #onrccbtthz .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #onrccbtthz .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #onrccbtthz .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #onrccbtthz .gt_footnote { margin: 0px; font-size: 90%; padding-left: 4px; padding-right: 4px; padding-left: 5px; padding-right: 5px; } #onrccbtthz .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #onrccbtthz .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #onrccbtthz .gt_left { text-align: left; } #onrccbtthz .gt_center { text-align: center; } #onrccbtthz .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #onrccbtthz .gt_font_normal { font-weight: normal; } #onrccbtthz .gt_font_bold { font-weight: bold; } #onrccbtthz .gt_font_italic { font-style: italic; } #onrccbtthz .gt_super { font-size: 65%; } #onrccbtthz .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 75%; vertical-align: 0.4em; } #onrccbtthz .gt_asterisk { font-size: 100%; vertical-align: 0; } #onrccbtthz .gt_indent_1 { text-indent: 5px; } #onrccbtthz .gt_indent_2 { text-indent: 10px; } #onrccbtthz .gt_indent_3 { text-indent: 15px; } #onrccbtthz .gt_indent_4 { text-indent: 20px; } #onrccbtthz .gt_indent_5 { text-indent: 25px; } Characteristic N Sedentary, N = 7 Low, N = 9 Moderate, N = 8 High, N = 7 CWWR 31     Median (IQR) 4.12 (3.55, 4.77) 5.50 (4.45, 7.85) 7.10 (6.03, 8.11) 7.47 (6.97, 8.19)     Mean, SD 4.15, 0.77 5.88, 1.69 7.12, 1.57 7.51, 1.24 CWWS score increased from the sedentary (M = 4.15, SD = 0.77), to low (M = 5.88, SD = 1.69), to moderate (M = 7.12, SD = 1.57) to high (M = 7.51, SD = 1.24) physical activity groups, in that order. 5.7.1 Assumptions Recall that the one-way ANOVA test is valid under three assumptions. One, there are no significant outliers that influence the group mean. Two, the dependent variable is at least approximately (ANOVA is robust to this assumption) normally distributed for each group if the sample size is small (for large sample sizes the Central Limit Theorem shows normality is unnecessary). Three, the dependent variable should have equal variances across groups. ANOVA is only sensitive to this condition if the group sample sizes are not similar. Kruskal-Wallis has no assumptions per se, but the interpretation of its results depend on the distribution of the dependent variable. If the distributions are similar, then the test results tell you whether the medians differ. Otherwise, the test results tell you whether the distributions differ. Use a boxplot to assess outliers for ANOVA and the data distribution (if you revert to Kruskal-Wallis). Values greater than 1.5 IQR from the hinges (values beyond the whiskers) are outliers. Outliers might occur from data entry errors or measurement errors, so investigate and fix or throw them out. If the outlier is a genuinely extreme, you still have a couple options before reverting to Kruskal-Wallis. You can transform the dependent variable, but don’t do this unless the data is also non-normal. Transforming the variable also has the downside of making interpretation more difficult. You can also leave the outlier(s) in if it doesn’t affect the conclusion. There are no outliers here. cs3$dat %&gt;% ggplot(aes(x = group, y = coping_stress)) + geom_boxplot(outlier.color = &quot;goldenrod&quot;, outlier.size = 2) + labs(title = &quot;Boxplot of CWWR vs Group&quot;, y = &quot;Score&quot;, x = &quot;Group&quot;) There is no accepted practice for determining whether distributions are similar. The boxplot reveals a wider range of values for “Low” group, but this is close enough to conclude the distributions are similar. You can assume the populations are normally distributed if \\(n_j &gt;= 30\\). Otherwise, try the Q-Q plot, or skewness and kurtosis values, or histograms. If you still don’t feel confident about normality, run the Shapiro-Wilk test of normality or the Kolmogorov-Smirnov test. Definitely do not use Shapiro-Wilk for \\(n_j &gt;= 30\\) because it is too sensitive. The Normal Q-Q plot below looks good for all groups except perhaps the “Low” group. The Shapiro-Wilk test confirms this, with all p-values over .05. cs3$dat %&gt;% ggplot(aes(sample = coping_stress)) + stat_qq() + stat_qq_line(col = &quot;goldenrod&quot;) + facet_wrap(facets = vars(group)) + labs(title = &quot;Q-Q Plot&quot;, x = &quot;Theoretical&quot;, y = &quot;Sample&quot;) with(cs3$dat, by(coping_stress, group, shapiro.test)) %&gt;% map(tidy) %&gt;% bind_rows(.id = &quot;group&quot;) ## # A tibble: 4 × 4 ## group statistic p.value method ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Sedentary 0.928 0.538 Shapiro-Wilk normality test ## 2 Low 0.841 0.0589 Shapiro-Wilk normality test ## 3 Moderate 0.976 0.940 Shapiro-Wilk normality test ## 4 High 0.944 0.671 Shapiro-Wilk normality test Had the data failed the normality test, you could probably carry on anyway since the test is fairly robust to deviations from normality, particularly if the sample sizes are nearly equal. You can also try transforming the dependent variable. Transformations will generally only work when the distribution of scores in all groups are the same shape. Otherwise, revert to the Kruskal-Wallis H test. ANOVA’s equality of sample variances condition is less critical when sample sizes are similar among the groups (as they are here). A rule of thumb is that no group’s standard deviation should be more than double that of any other. In this case, “Moderate” and “Low” are more than double “Sedentary”. ## # A tibble: 4 × 4 ## group n sd multiple ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Sedentary 7 0.771 1 ## 2 High 7 1.24 1.61 ## 3 Moderate 8 1.57 2.04 ## 4 Low 9 1.69 2.19 There are two common tests, Bartlett and Levene9. Levene is less sensitive to departures from normality. Neither test rejects the null hypothesis of equality of variance here. (cs3$levene &lt;- car::leveneTest(coping_stress ~ group, data = cs3$dat, center = &quot;mean&quot;)) ## Levene&#39;s Test for Homogeneity of Variance (center = &quot;mean&quot;) ## Df F value Pr(&gt;F) ## group 3 2.129 0.1199 ## 27 (cs3$bartlet &lt;- bartlett.test(coping_stress ~ group, data = cs3$dat)) ## ## Bartlett test of homogeneity of variances ## ## data: coping_stress by group ## Bartlett&#39;s K-squared = 3.7489, df = 3, p-value = 0.2899 The residuals vs fitted values plot is included in the set of diagnostic plots that are produced in the base R plot.lm() function. aov(coping_stress ~ group, data = cs3$dat) %&gt;% plot(which = 1) Heterogeneity of variances is a common problem in ANOVA. The Box-Cox procedure can help find a good transformation to remove heterogeneity. MASS::boxcox() calculates a profile of log-likelihoods for a power transformation of the dependent variable \\(Y^\\lambda\\). \\(\\lambda\\) \\(Y^\\lambda\\) Transformation 2 \\(Y^2\\) Square 1 \\(Y^1\\) (no transformation) .5 \\(Y^{.5}\\) Square Root 0 \\(\\ln(Y)\\) Log -.5 \\(Y^{-.5}\\) Inverse Square Root -1 \\(Y^{-1}\\) Inverse The Box-Cox procedure does not recommend any particular transformation of the data in this case. MASS::boxcox(aov(coping_stress ~ group, data = cs3$dat), plotit = TRUE) Had the data failed the homogeneity assumption, you could use a modified version of ANOVA called Welch’s ANOVA and the Games-Howell post hoc test, or you could revert to the nonparametric Kruskal-Wallis test. 5.7.2 ANOVA If the dependent variable conforms to the three ANOVA assumptions of no outliers, normality, and homogeneity, then you can run a one-way ANOVA with aov(). If the dependent variable only violates the homegeneity assumption, you can run Welch’s ANOVA with oneway.test(..., var.equal = FALSE) cs3$aov &lt;- aov(coping_stress ~ group, data = cs3$dat) (cs3$anova &lt;- anova(cs3$aov)) ## Analysis of Variance Table ## ## Response: coping_stress ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## group 3 49.033 16.3443 8.316 0.0004454 *** ## Residuals 27 53.066 1.9654 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (cs3$welch &lt;- oneway.test(coping_stress ~ group, data = cs3$dat, var.equal = FALSE)) ## ## One-way analysis of means (not assuming equal variances) ## ## data: coping_stress and group ## F = 14.821, num df = 3.000, denom df = 14.574, p-value = 0.0001058 The ability to cope with workplace-related stress (CWWS score) was statistically significantly different for different levels of physical activity group, F(3, 27) = 8.3, p = 0.0004. tibble( f_stat = seq(0, 10, .01), d_val = df(f_stat, 3, 14.574), p_f = pf(f_stat, cs3$anova$Df[1], cs3$anova$Df[2], lower.tail = FALSE), region = if_else(p_f &lt; .05, &quot;reject&quot;, &quot;accept&quot;) ) %&gt;% ggplot(aes(x = f_stat, y = d_val)) + geom_area(aes(fill = region), show.legend = FALSE) + geom_line() + geom_vline(xintercept = cs3$anova$&quot;F value&quot;[1], linetype = 2, color = &quot;firebrick&quot;) + scale_fill_manual(values = c(reject = &quot;firebrick&quot;, accept = &quot;white&quot;)) + labs( title = glue::glue(&quot;F({paste(cs3$anova$Df, collapse = &#39;, &#39;)}) = &quot;, &quot;{comma(cs3$anova$&#39;F value&#39;[1], .1)}, p = &quot;, &quot;{comma(cs3$anova$&#39;Pr(&gt;F)&#39;[1], .0001)}&quot;), x = &quot;F&quot;, y = &quot;P(F)&quot; ) The F test does not indicate which populations cause the rejection of \\(H_0\\). Conduct a Tukey post hoc test if you have no specific hypothesis about two groups differing or want to see all group differences.10 If you want to compare two groups or set of groups, then use a custom contrast. Tukey is valid for balanced designs. If you have different sample sizes per group, use the Tukey-Kramer post hoc test. If the dependent variable failed the homogeneity of variances assumption, you would run the Games-Howell post hoc test instead. (cs3$tukey &lt;- TukeyHSD(cs3$aov)) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = coping_stress ~ group, data = cs3$dat) ## ## $group ## diff lwr upr p adj ## Low-Sedentary 1.7276175 -0.2057757 3.661011 0.0923527 ## Moderate-Sedentary 2.9715262 0.9859704 4.957082 0.0018413 ## High-Sedentary 3.3540854 1.3034122 5.404759 0.0006806 ## Moderate-Low 1.2439086 -0.6202750 3.108092 0.2835038 ## High-Low 1.6264679 -0.3069254 3.559861 0.1226045 ## High-Moderate 0.3825593 -1.6029965 2.368115 0.9517285 (cs3$games_howell &lt;- rstatix::games_howell_test(cs3$dat, coping_stress ~ group)) ## # A tibble: 6 × 8 ## .y. group1 group2 estimate conf.low conf.high p.adj p.adj.…¹ ## * &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 coping_stress Sedentary Low 1.73 -0.163 3.62 0.077 ns ## 2 coping_stress Sedentary Moderate 2.97 1.07 4.88 0.003 ** ## 3 coping_stress Sedentary High 3.35 1.66 5.05 0.000584 *** ## 4 coping_stress Low Moderate 1.24 -1.04 3.53 0.423 ns ## 5 coping_stress Low High 1.63 -0.508 3.76 0.167 ns ## 6 coping_stress Moderate High 0.383 -1.76 2.52 0.951 ns ## # … with abbreviated variable name ¹​p.adj.signif Tukey post hoc analysis revealed that the increase from sedentary to moderate (2.97, 95% CI (0.99 to 4.96)) was statistically significant (p = 0.002), as well as the increase from sedentary to high (3.35, 95% CI (1.30 to 5.40)) was statistically significant (p = 0.001), but no other group differences were statistically significant. cs3$tukey %&gt;% tidy() %&gt;% ggplot(aes(y = contrast, x = estimate)) + geom_point(shape = 3) + geom_errorbar(aes(xmin = conf.low, xmax = conf.high), width = .2) + geom_vline(aes(xintercept = 0), linetype = 2) + labs(x = NULL, y = NULL, title = &quot;95% family-wise confidence level&quot;) Games-Howell post hoc analysis revealed that the increase from sedentary to moderate (2.97, 95% CI (1.07 to 4.88)) was statistically significant (p =0.003), as well as the increase from sedentary to high (3.35, 95% CI (1.66 to 5.05, p = 0.001). If you have specific hypotheses about the differences between the groups of your independent variable, e.g., whether the mean CWWS differs between the “Low” and “Sedentary” groups, \\(H_0: \\sum_i^K{c_i u_i} = 0\\) where \\(c_i = (1, -1, 0, 0)\\) or “Sedentary” and average of all others, \\(c_i = (1, -1/3, -1/3, -1/3)\\), set up a contrast using the multcomp package. cs3$glht_1 &lt;- multcomp::glht(cs3$aov, linfct = multcomp::mcp(group = c(-1, 1, 0, 0))) summary(cs3$glht_1) ## ## Simultaneous Tests for General Linear Hypotheses ## ## Multiple Comparisons of Means: User-defined Contrasts ## ## ## Fit: aov(formula = coping_stress ~ group, data = cs3$dat) ## ## Linear Hypotheses: ## Estimate Std. Error t value Pr(&gt;|t|) ## 1 == 0 1.7276 0.7065 2.445 0.0213 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## (Adjusted p values reported -- single-step method) cs3$glht_2 &lt;- multcomp::glht(cs3$aov, linfct = multcomp::mcp(group = c(-1, 1/3, 1/3, 1/3))) summary(cs3$glht_2) ## ## Simultaneous Tests for General Linear Hypotheses ## ## Multiple Comparisons of Means: User-defined Contrasts ## ## ## Fit: aov(formula = coping_stress ~ group, data = cs3$dat) ## ## Linear Hypotheses: ## Estimate Std. Error t value Pr(&gt;|t|) ## 1 == 0 2.6844 0.6029 4.452 0.000133 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## (Adjusted p values reported -- single-step method) There are three groups, so you need to adjust the p-value and 95% CI for them. confint(cs3$glht_1, level = 1-.05/3) ## ## Simultaneous Confidence Intervals ## ## Multiple Comparisons of Means: User-defined Contrasts ## ## ## Fit: aov(formula = coping_stress ~ group, data = cs3$dat) ## ## Quantile = 2.5525 ## 98.3333333333333% family-wise confidence level ## ## ## Linear Hypotheses: ## Estimate lwr upr ## 1 == 0 1.7276 -0.0757 3.5309 confint(cs3$glht_2, level = 1-.05/3) ## ## Simultaneous Confidence Intervals ## ## Multiple Comparisons of Means: User-defined Contrasts ## ## ## Fit: aov(formula = coping_stress ~ group, data = cs3$dat) ## ## Quantile = 2.5525 ## 98.3333333333333% family-wise confidence level ## ## ## Linear Hypotheses: ## Estimate lwr upr ## 1 == 0 2.6844 1.1454 4.2234 There was no statistically significant increase in CWWS score from the sedentary group (4.15, 0.77) to the group performing a low level of physical activity (5.88, 1.69), a mean increase of 1.7 (95% CI, -0.08, 3.53), p = 0.021. CWWS score was statistically significantly higher in the non-sedentary groups (M = 6.8) compared to the sedentary group (4.15, 0.77), a mean increase of 1.7 (95% CI, 1.15, 4.22), p = 0.000. You may also want to report the \\(\\omega^2\\) effect size, \\[\\omega^2 = \\frac{SSR - df_R \\cdot MSE}{MSE + SST}\\] where SSR is the between groups sum of squares, 49.0, MSE is the within groups mean square, 1.97, and SST is the total sum of squares, 102.1. (cs3$anova_stats &lt;- sjstats::anova_stats(cs3$aov)) ## term | df | sumsq | meansq | statistic | p.value | etasq | partial.etasq | omegasq | partial.omegasq | epsilonsq | cohens.f | power ## ----------------------------------------------------------------------------------------------------------------------------------------- ## group | 3 | 49.033 | 16.344 | 8.316 | &lt; .001 | 0.480 | 0.480 | 0.415 | 0.415 | 0.423 | 0.961 | 0.993 ## Residuals | 27 | 53.066 | 1.965 | | | | | | | | | \\(\\omega^2\\) estimates the population effect size. It \\(\\omega^2\\) ranges from -1 to +1. Here, \\(\\omega^2\\) is 0.415. comma(cs3$anova$`Sum Sq`[1], .1) ## [1] &quot;49.0&quot; Alternatively, the partial eta squared statistic, \\(\\eta^2\\), measures the effect size in the sample. Here \\(\\eta^2\\) is 0.48. Now you can report your results. A one-way ANOVA was conducted to determine if the ability to cope with workplace-related stress (CWWS score) was different for groups with different physical activity levels. Participants were classified into four groups: sedentary (n = 7), low (n = 9), moderate (n = 8) and high levels of physical activity (n = 7). There were no outliers, as assessed by boxplot; data was normally distributed for each group, as assessed by Shapiro-Wilk test (p &gt; .05); and there was homogeneity of variances, as assessed by Levene’s test of homogeneity of variances (p = 0.120). CWWS score was statistically significantly different between different physical activity groups, F(3, 27) = 8.3, p = 0.0004, \\(\\omega^2\\) = 0.415. CWWS score increased from the sedentary (M = 4.15, SD = 0.77), to low (M = 5.88, SD = 1.69), to moderate (M = 7.12, SD = 1.57) to high (M = 7.51, SD = 1.24) physical activity groups, in that order. Tukey post hoc analysis revealed that the mean increase from sedentary to moderate (2.97, 95% CI [0.99 to 4.96]) was statistically significant (p = 0.002), as well as the increase from sedentary to high (3.35, 95% CI [1.30 to 5.40], p = 0.001), but no other group differences were statistically significant. Had the dependent variable failed the homogeneity of variances assumption, you would report the results from Welch’s ANOVA, The ability to cope with workplace-related stress (CWWS score) was statistically significantly different for different levels of physical activity group, Welch’s F(3, 14.6) = 14.8, p &lt; .0005. and the Games-Howell post hoc test, Games-Howell post hoc analysis revealed that the increase from sedentary to moderate (2.97, 95% CI [1.07 to 4.88]) was statistically significant (p =0.003), as well as the increase from sedentary to high (3.35, 95% CI [1.66 to 5.05], p = 0.001). Power Analysis If you run an ANOVA and do not reject the null hypothesis, you may want to run a power analysis to make sure the power of the test was not very low. Power is the ability to reject the null when the null is really false. Power is affected by sample size, effect size, variability of the experiment, and the significance of the type 1 error. You typically want power to be at 80%, meaning 80% of the time your test rejects the null when it should, and 20% of the time your test does not reject the null when it should. power.anova.test( groups = cs3$aov$rank, n = cs3$aov$model %&gt;% count(group) %&gt;% pull(n) %&gt;% min(), between.var = cs3$dat %&gt;% group_by(group) %&gt;% summarize(M = mean(coping_stress)) %&gt;% pull(M) %&gt;% var(), within.var= cs3$anova$`Mean Sq`[2], sig.level = 0.05 ) ## ## Balanced one-way analysis of variance power calculation ## ## groups = 4 ## n = 7 ## between.var = 2.283631 ## within.var = 1.965395 ## sig.level = 0.05 ## power = 0.9787783 ## ## NOTE: n is number in each group 5.7.3 Kruskal-Wallis Test Run a Kruskal-Wallis H test with kruskal.test(). (cs3$kruskal &lt;- kruskal.test(coping_stress ~ group, data = cs3$dat)) ## ## Kruskal-Wallis rank sum test ## ## data: coping_stress by group ## Kruskal-Wallis chi-squared = 14.468, df = 3, p-value = 0.002332 The dependent variable has similarly shaped distributions for all groups of the independent variable, so you can conclude the median CWWS scores were statistically significantly different between groups, \\(\\chi^2\\)(3) = 14.5, p = 0.0023. Otherwise you would conclude the distributions differ. You rejected the null hypothesis, so continue on with a post hoc test to determine which medians (similar distributions) or mean ranks (dissimilar distributions) differ with the Dunn procedure using a Bonferroni correction for multiple comparisons. (cs3$dunn &lt;- FSA::dunnTest(coping_stress ~ group, data = cs3$dat, method = &quot;bonferroni&quot;)) ## Registered S3 methods overwritten by &#39;FSA&#39;: ## method from ## confint.boot car ## hist.boot car ## Dunn (1964) Kruskal-Wallis multiple comparison ## p-values adjusted with the Bonferroni method. ## Comparison Z P.unadj P.adj ## 1 High - Low 1.6801430 0.0929294869 0.557576921 ## 2 High - Moderate 0.2163067 0.8287486760 1.000000000 ## 3 Low - Moderate -1.5121301 0.1305007724 0.783004635 ## 4 High - Sedentary 3.3216144 0.0008949829 0.005369897 ## 5 Low - Sedentary 1.8429610 0.0653346998 0.392008199 ## 6 Moderate - Sedentary 3.2142419 0.0013078945 0.007847367 P.adj equals P.unadj multiplied by the number of comparisons (6). You could report the adjusted p or the unadjusted p with a note that you accepted statistical significance at the p &lt; .05 / 6 = 0.0083 level. Now you can report your results. A Kruskal-Wallis test was conducted to determine if there were differences in CWWS scores between groups that differed in their level of physical activity: the “sedentary” (n = 7), “low” (n = 9), “moderate” (n = 8) and “high” (n = 7) physical activity level groups. Distributions of CWWS scores were similar for all groups, as assessed by visual inspection of a boxplot. Median CWWS scores were statistically significantly different between the different levels of physical activity group, \\(\\chi^2\\)(3) = 14.5, p = 0.0023. Subsequently, pairwise comparisons were performed using Dunn’s (1964) procedure with a Bonferroni correction for multiple comparisons. Adjusted p-values are presented. This post hoc analysis revealed statistically significant differences in CWWS scores between the sedentary (Mdn = 4.12) and moderate (Mdn = 7.10) (p = 0.0078) and sedentary and high (Mdn = 7.47) (p = 0.0054) physical activity groups, but not between the low physical activity group (Mdn = 5.50) or any other group combination. Had the distributions been different, you would report “CWWS scores” instead of “Median CWWS scores” and report the mean ranks instead of Mdn. Unfortunately, you cannot retrieve those ranks from the test object, so you would have to calculate them yourself. Trying APA style guidelines.↩︎ NIST has a good write-up for Bartlett and Levene↩︎ There are other options for post-hoc tests not discussed here: Fisher’s Least Significant Difference (LSD), Bonferroni, Scheffe, and Dunnett.↩︎ "],["chisqhomogeneity.html", "5.8 Chi-Square Test of Homogeneity", " 5.8 Chi-Square Test of Homogeneity The chi-square test of homogeneity tests whether frequency counts of the R levels of a categorical variable are distributed identically across the C populations. It tests whether observed joint frequency counts \\(O_{ij}\\) differ from expected frequency counts \\(E_{ij}\\) under the independence model (the model of independent explanatory variables, \\(\\pi_{ij} = \\pi_{i+} \\pi_{+j}\\). \\(H_0\\) is \\(O_{ij} = E_{ij}\\). The chi-square homogeneity test can be extended to cases where \\(I\\) and/or \\(J\\) is greater than 2. There are two possible test statistics for this test, Pearson \\(X^2 = \\sum \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\), and deviance \\(G^2 = 2 \\sum_{ij} O_{ij} \\log \\left( \\frac{O_{ij}}{E_{ij}} \\right)\\). Side note: z-Test of Two Proportions The z-test and chi-square test produce the same statistical significance result because they are algebraically identical. The z-test uses the difference in sample proportions \\(\\hat{d} = p_1 - p_2\\) as an estimate of the difference in population proportions \\(\\delta = \\pi_1 - \\pi_2\\) to evaluate an hypothesized difference in population proportions \\(d_0 = \\pi_0 - \\pi_1\\) and/or construct a \\((1−\\alpha)\\%\\) confidence interval around \\(\\hat{d}\\) to estimate \\(\\delta\\) within a margin of error \\(\\epsilon\\). The z-test applies when the central limit theorem conditions hold so that the normal distribution approximates the binomial distribution. the sample is independently drawn, meaning random assignment (experiments) or random sampling without replacement from \\(n &lt; 10\\%\\) of the population (observational studies), there are at least \\(n_i p_i &gt;= 5\\) successes and \\(n_i (1 - p_i) &gt;= 5\\) failures for each group \\(i\\), the sample sizes are both \\(n_i &gt;= 30\\), and the probability of success for each group is not extreme, \\(0.2 &lt; \\pi_i &lt; 0.8\\). If these conditions hold, the sampling distribution of \\(\\delta\\) is normally distributed around \\(\\hat{d}\\) with standard error \\(se_\\hat{d} = \\sqrt{\\frac{p_1(1 - p_1)}{n_1} + \\frac{p_2(1 − p_2)}{n_2}}\\). The measured values \\(\\hat{d}\\) and \\(se_\\hat{d}\\) approximate the population values \\(\\delta\\) and \\(se_\\delta\\). Define a \\((1 − \\alpha)\\%\\) confidence interval as \\(\\hat{d} \\pm z_{\\alpha / 2}se_\\hat{d}\\) or test the hypothesis of \\(d = d_0\\) with test statistic \\(z = \\frac{\\hat{d} − d_0}{se_{d_0}}\\) where \\(se_{d_0} = \\sqrt{p^*(1 - p^*) \\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}\\) and \\(p^*\\) is the overall success probability. "],["fisherexact.html", "5.9 Fisher’s Exact Test", " 5.9 Fisher’s Exact Test Fisher’s exact test is an “exact test” in that the p-value is calculated exactly from the hypergeometric distribution rather than relying on the approximation that the test statistic distribution approaches \\(\\chi^2\\) as \\(n \\rightarrow \\infty\\). The test is applicable in situations where the row totals \\(n_{i+}\\) and the column totals \\(n_+j\\) are fixed by study design (rarely applies), and the expected values of &gt;20% of cells (at least 1 cell in a 2x2 table) have expected cell counts &gt;5, and no expected cell count is &lt;1. The p-value from the test is computed as if the margins of the table are fixed. This leads under a null hypothesis of independence to a hypergeometric distribution of the numbers in the cells of the table (Wikipedia). Fisher’s exact test is useful for small n-size samples where the chi-squared distribution assumption of the chi-squared and G-test tests fails. Fisher’s exact test is overly conservative (p values too high) for large n-sizes. The Hypergeometric density function is \\[f_X(k|N, K, n) = \\frac{{{K}\\choose{k}}{{N-K}\\choose{n-k}}}{{N}\\choose{n}}.\\] The density is the exact hypergeometric probability of observing this particular arrangement of the data, assuming the given marginal totals, on the null hypothesis that the conditional probabilities are equal. "],["case-study-4.html", "Case Study 4", " Case Study 4 The case study below uses a data set from Laerd and a second modified version. The first data set passes the chi-square test of homogeneity requirements. The second (in parentheses), fails the n-sizes test. A researcher recruits 100 (50) patients who have a “high” classification of cholesterol and who currently have a poor lifestyle. The researcher randomly assigns 50 (25) of them to a drug intervention and 50 (25) to a lifestyle intervention. After six months, a doctor reclassifies the patients as either still having a “high” classification of cholesterol or now having a “normal” classification of cholesterol. The chi-sq data set has the following summary statistics. .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; border-color: transparent; caption-side: top; } .tabwid-caption-bottom table{ caption-side: bottom; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .katex-display { margin: 0 0 !important; } .cl-eedf6590{}.cl-eed40ae2{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-eed7f6f2{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-eed8168c{width:0.948in;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-eed81696{width:1.075in;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-eed81697{width:1.033in;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-eed816a0{width:0.948in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-eed816a1{width:1.075in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-eed816aa{width:1.033in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-eed816ab{width:0.948in;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-eed816ac{width:1.075in;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-eed816b4{width:1.033in;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-eed816b5{width:0.948in;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-eed816be{width:1.075in;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-eed816bf{width:1.033in;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-eeaab57a{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;} Table 5.2: Chisq Data Set: Observed vs Expected DatainterventionHighNormalTotalObservedDrug18 (36%)32 (64%) 50 (100%)Lifestyle33 (66%)17 (34%) 50 (100%)Total51 (51%)49 (49%)100 (100%)ExpectedDrug25.5 (51%)24.5 (49%) 50 (100%)Lifestyle25.5 (51%)24.5 (49%) 50 (100%)Total51.0 (51%)49.0 (49%)100 (100%) The Fisher data set has the following summary statistics. .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; border-color: transparent; caption-side: top; } .tabwid-caption-bottom table{ caption-side: bottom; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .katex-display { margin: 0 0 !important; } .cl-ef391f36{}.cl-ef2d80c2{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-ef31fd64{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-ef3216fa{width:0.948in;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ef32170e{width:1.075in;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ef32170f{width:1.033in;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ef321710{width:0.99in;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ef321718{width:0.948in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ef321719{width:1.075in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ef32171a{width:1.033in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ef321722{width:0.99in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ef32172c{width:0.948in;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ef32172d{width:1.075in;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ef321736{width:1.033in;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ef321737{width:0.99in;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ef321740{width:0.948in;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ef321741{width:1.075in;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ef32174a{width:1.033in;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ef32174b{width:0.99in;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ef287a82{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;} Table 5.3: Fisher Data Set: Observed vs Expected DatainterventionHighNormalTotalObservedDrug 9 (36%)16 (64%)25 (100%)Lifestyle16 (64%) 9 (36%)25 (100%)Total25 (50%)25 (50%)50 (100%)ExpectedDrug12.5 (50%)12.5 (50%)25 (100%)Lifestyle12.5 (50%)12.5 (50%)25 (100%)Total25.0 (50%)25.0 (50%)50 (100%) Conditions n-Size The chi-square test of homogeneity applies with the CLT conditions hold. the sample is independently drawn, there are at least 5 successes (Normal) and failures (High) for each group \\(i\\), the sample sizes for both groups are &gt;=30, and the probability of success for each group is not extreme, \\(0.2 &lt; \\pi_i &lt; 0.8\\). The conditions hold for the chi-sq data set, but not for the Fisher data set. Test Chi-Square (ind_discrete$chisq_test &lt;- ind_discrete$chisq_dat %&gt;% tabyl(intervention, risk_level) %&gt;% chisq.test(correct = FALSE)) ## ## Pearson&#39;s Chi-squared test ## ## data: . ## X-squared = 9.0036, df = 1, p-value = 0.002694 100 patients with a high cholesterol classification were randomly assigned to either a drug or lifestyle intervention, 50 in each intervention. The test of two proportions used was the chi-square test of homogeneity. At the conclusion of the drug intervention, 32 patients (64%) had improved their cholesterol classification from high to normal compared to 17 patients (34%) in the lifestyle intervention, a difference in proportions of 0.30, p = 0.0027. Fisher (ind_discrete$fisher_test &lt;- ind_discrete$fisher_dat %&gt;% tabyl(intervention, risk_level) %&gt;% fisher.test()) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: . ## p-value = 0.08874 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 0.08497039 1.15362199 ## sample estimates: ## odds ratio ## 0.3241952 50 patients with a high cholesterol classification were randomly assigned to either a drug or lifestyle intervention, 25 in each intervention. At the conclusion of the drug intervention, 16 patients (64%) had improved their cholesterol classification from high to normal compared to 9 patients (36%) in the lifestyle intervention. Due to small sample sizes, Fisher’s exact test was run. There was a non-statistically significant difference in proportions of 0.28, p = 0.0887. "],["pairwiseproptest.html", "5.10 Pairwise Prop Test", " 5.10 Pairwise Prop Test library(tidyverse) M &lt;- 3573 F &lt;- 4177 dat &lt;- tribble( ~gender, ~src, ~Y, ~N, &quot;Male&quot;, &quot;Indeed&quot;, 1699, M-1699, &quot;Male&quot;, &quot;LinkedIn&quot;, 1755, M-1755, &quot;Male&quot;, &quot;Google&quot;, 1578, M-1578, &quot;Female&quot;, &quot;Indeed&quot;, 2554, F-2554, &quot;Female&quot;, &quot;LinkedIn&quot;, 1914, F-1914, &quot;Female&quot;, &quot;Google&quot;, 1694, F-1694 ) prop.test(x = dat$Y, n = dat$Y + dat$N) ## ## 6-sample test for equality of proportions without continuity correction ## ## data: dat$Y out of dat$Y + dat$N ## X-squared = 412.66, df = 5, p-value &lt; 2.2e-16 ## alternative hypothesis: two.sided ## sample estimates: ## prop 1 prop 2 prop 3 prop 4 prop 5 prop 6 ## 0.4755108 0.4911839 0.4416457 0.6114436 0.4582236 0.4055542 pairwise.prop.test(x = dat$Y, n = dat$Y + dat$N) ## ## Pairwise comparisons using Pairwise comparison of proportions ## ## data: dat$Y out of dat$Y + dat$N ## ## 1 2 3 4 5 ## 2 0.40250 - - - - ## 3 0.02026 0.00021 - - - ## 4 &lt; 2e-16 &lt; 2e-16 &lt; 2e-16 - - ## 5 0.40250 0.02026 0.40250 &lt; 2e-16 - ## 6 6.3e-09 4.8e-13 0.00873 &lt; 2e-16 1.1e-05 ## ## P value adjustment method: holm "],["mcnemar.html", "5.11 McNemar’s Test", " 5.11 McNemar’s Test This test applies when you have paired samples. Wilcoxon Paired-Sample applies when the variable distributions are non-normally distributed and samples are paired. 5.11.1 MANOVA Multi-factor ANOVA (MANOVA) is a method to compare mean responses by treatment factor level of two or more treatments applied in combination. The null hypotheses are \\(H_0: \\mu_{1.} = \\mu_{2.} = \\dots = \\mu_{a.}\\) for the \\(a\\) levels of factor 1, \\(H_0: \\mu_{.1} = \\mu_{.2} = \\dots = \\mu_{.b}\\) for the \\(b\\) levels of factor 2, etc. for all the factors in the experiment, and $H_0: $ no interaction for all the factor interactions. There are two equivalent ways to state the MANOVA model: \\[Y_{ijk} = \\mu_{ij} + \\epsilon_{ijk}\\] In this notation \\(Y_{ijk}\\) refers to the \\(k^{th}\\) observation in the \\(j^{th}\\) level of factor two and the \\(i^{th}\\) level of factor 1. Potentially there could be additional factors. This model formulation decomposes the response into a cell mean and an error term. The second makes the factor effect more explicit and is thus more common: \\[Y_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\epsilon_{ijk}\\] 5.11.2 Multiple Variance Comparison F Test 5.11.3 Example A study investigates the relationship between oxygen update and two explanatory variables: smoking, and type of stress test. A sample of \\(n = 27\\) persons, 9 non-smoking, 9 moderately-smoking, and 9 heavy-smoking are divided into three stress tests, bicycle, treadmill, and steps and their oxygen uptake was measured. Is oxygen uptake related to smoking status and type of stress test? Is there an interaction effect between smoking status and type of stress test? library(dplyr) library(ggplot2) library(nortest) # for Anderson-Darling test library(stats) # for anova smoker &lt;- c(1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3) stress &lt;- c(1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3) oxytime &lt;- c(12.8, 13.5, 11.2, 16.2, 18.1, 17.8, 22.6, 19.3, 18.9, 10.9, 11.1, 9.8, 15.5, 13.8, 16.2, 20.1, 21.0, 15.9, 8.7, 9.2, 7.5, 14.7, 13.2, 8.1, 16.2, 16.1, 17.8) oxy &lt;- data.frame(oxytime, smoker, stress) oxy$smoker &lt;- ordered(oxy$smoker, levels = c(1, 2, 3), labels = c(&quot;non-smoker&quot;, &quot;moderate&quot;, &quot;heavy&quot;)) oxy$stress &lt;- factor(oxy$stress, labels = c(&quot;bicycle&quot;, &quot;treadmill&quot;, &quot;steps&quot;)) lm_oxy &lt;- lm(oxytime~smoker+stress+smoker*stress, data = oxy) anova(lm_oxy) ## Analysis of Variance Table ## ## Response: oxytime ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## smoker 2 84.899 42.449 12.8967 0.0003348 *** ## stress 2 298.072 149.036 45.2793 9.473e-08 *** ## smoker:stress 4 2.815 0.704 0.2138 0.9273412 ## Residuals 18 59.247 3.291 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 SFU BIO710 "],["association.html", "Chapter 6 Association", " Chapter 6 Association Tests of association assess the strength of association between two variables. There are many variations on this theme. Pearson’s correlation assesses the strength of a linear relationship between two continuous variables. It applies when the relationship is linear with no outliers and the variables are bi-variate normal. There are two less restrictive alternatives, Spearman’s rho and Kendall’s tau, that assess the strength and direction of association. A two-way frequency table is a frequency table for two categorical variables. You usually construct a two-way table to test whether the frequency counts in one categorical variable differ from the other categorical variable using the chi-square test for independence or G-test, or Fisher’s exact test. If there is a significant difference (i.e., the variables are related), then describe the relationship with an analysis of the residuals, calculations of measures of association (difference in proportions, Phi and Cramer’s V, Relative risks, or Odds Ratio), and partition tests. If one of the variables is bivariate categorical, use point-biserial correlation, a special case of Pearson’s correlation. Pearson’s partial correlation controls for one or more variables - linear regression? If both variables are ordinal, use Goodman and Kruskal’s gamma. Somers’ d is an alternative if you want to distinguish between a dependent and independent variable (instead of linear regression?). The Mantel-Haenszel test of trend is used to determine whether there is a linear trend (i.e., a linear relationship/association) between two ordinal variables that are represented in a contingency table. The Cochran-Armitage test of trend is used to determine whether there is a linear trend (i.e., a linear relationship/association) between an ordinal independent variable and a dichotomous dependent variable. Goodman and Kruskal’s λ (the Greek symbol, λ, is pronounced “lambda”) is also referred to as Goodman and Kruskal’s lambda. It is a nonparametric measure of the strength of association between two nominal variables where a distinction is made between a dependent and independent variable Loglinear analysis is used to understand (and model) associations between two or more categorical variables (i.e., nominal or ordinal variables). However, loglinear analysis is usually employed when dealing with three or more categorical variables, as opposed to two variables, where a chi-square test for association is usually conducted instead. There are three common correlation tests for categorical variables11: Tetrachoric correlation for binary categorical variables; polychoric correlation for ordinal categorical variables; and Cramer’s V for nominal categorical variables. See https://www.statology.org/correlation-between-categorical-variables/↩︎ "],["pearson.html", "6.1 Pearson’s Correlation", " 6.1 Pearson’s Correlation The Pearson product-moment correlation measures the strength and direction of a linear relationship between two continuous variables, x and y. The Pearson correlation coefficient, r, ranges from -1 (perfect negative linear relationship) to +1 (perfect positive linear relationship). A value of 0 indicates no relationship between two variables. \\[r_{x,y} = \\frac{cov(x,y)}{\\sigma(x) \\sigma(y)}\\] The statistic can be used as an estimate of the population correlation, \\(\\rho\\), in a test of statistical significance from 0 (H0: \\(\\rho\\) = 0). \\[\\rho_{X,Y} = \\frac{cov(X,Y)}{\\sigma(X) \\sigma(Y)}\\] A rule of thumb interpretation is that \\(|r|\\) under .1 is “no correlation”, .1 - .3 is “small”, .3 - .5 “medium/moderate”, and .5 - 1.0 “large/strong”. The test statistic below follows a t-distribution with n − 2 degrees of freedom. \\[t = r_{x,y} \\sqrt{\\frac{n - 2}{1 - r^2_{x,y}}}\\] "],["spearman.html", "6.2 Spearman’s Rho", " 6.2 Spearman’s Rho Spearman’s ranked correlation (Spearman’s rho) is a measure of the strength and direction of a monotonic relationship between two variables that are at least ordinal. Spearman’s correlation is a non-parametric alternative to Pearson when one or more of its conditions are violated. Unlike Pearson, the relationship need not be linear (it only needs to be monotonic), and has no outliers or bivariate normality conditions. Spearman’s correlation is Pearson’s correlation applied to the ranks of variables (for ordinal variables, their value already is a rank). However, there is also a second definition that gives the same result, at least when there are no ties in the ranks: \\[\\rho = 1 - \\frac{6 \\sum_i d^2_i}{n(n^2 - 1)}\\] where \\(d_i\\) is the difference in ranks of observation \\(i\\). "],["kendall.html", "6.3 Kendal’s Tau", " 6.3 Kendal’s Tau Kendal’s tau is a second alternative to Pearson and is identical to Spearman’s rho with regard to assumptions. Kendal’s tau only differs from Spearman’s rho in how it measures the relationship. Whereas Spearman measures the correlation of the ranks, Kendal’s tau is a function of concordant (C), discordant (D) and tied (Tx and Ty) pairs of observations. Concordant means both X and Y in one observation of a pair are larger than in the other. Discordant means X is larger in one observation than the other while Y is smaller. Tied mean either both observations have the same X or both have the same Y. \\[\\mathrm{Kendall&#39;s} \\space \\tau_b = \\frac{C - D}{\\sqrt{(C + D + T_x) \\times (C + D + T_Y)}}\\] "],["case-study-pearson-spearman-kendall.html", "6.4 Case Study: Pearson, Spearman, Kendall", " 6.4 Case Study: Pearson, Spearman, Kendall This case study works with two data sets. dat1 is composed of two continuous variables; dat2 is composed of two ordinal variables. A researcher investigates the relationship between cholesterol concentration and time spent watching TV, time_tv, in n = 100 otherwise healthy 45 to 65 year old men (dat1). This data set will meet the conditions for Pearson, so we try all three tests on it. A researcher investigates the relationship between the level of agreement with the statement “Taxes are too high” (tax_too_high, four level ordinal) and participant income level (three level ordinal) (dat2). The ordinal variables rule out Pearson, leaving Spearman and Kendall. Conditions Pearson’s correlation applies when X and Y are continuous (interval or ratio) paired variables with 1) a linear relationship that 2) has no significant outliers, and 3) are bivariate normal. Spearman’s rho and Kendall’s tau only require that X and Y be at least ordinal with 1) a monotonically increasing or decreasing relationship. Linearity and Monotonicity. A visual inspection of a scatterplot should find a linear relationship (Pearson) or monotonic relationship (Spearman and Kendall). Pearson’s correlation additionall requires No Outliers. Identify outliers with the scatterplot. Normality. Bivariate normality is difficult to assess. Instead, check that each variable is individually normally distributed. Use the Shapiro-Wilk test. Linearity / Monotonicity Assess linearity and monotonicity with a scatter plot. dat1 is plotted on the left in Figure 6.1. A second version that fails the linearity test is shown to the right. If the linear relationship assumption fails, consider transforming the variable instead of reverting to Spearman or Kendall. Figure 6.1: The left scatter plot is dat1. It meets Pearson’s linearity condition. A second version at right illustrates what a failure might look like. The ordinal variable data set dat2 is plotted in Figure 6.2. Figure 6.2: dat2 meets the mononicity assumption for Spearman’s rho and Kendall’s tau. No Outliers Pearson’s correlation requires no outliers. Both plots in Figure 6.1 are free of outliers. If there were outliers, check whether they are data entry errors or measurement errors and fix or discard them. If the outliers are genuine, leave them in if they do not affect the conclusion. You can also try tranforming the variable. Failing all that, revert to the Spearman’s rho or Kendall’s tau. Bivariate Normality Bivariate normality is difficult to assess. If two variables are bivariate normal, they will each be individually normal as well. That’s the best you can hope to check for. Use the Shapiro-Wilk test. shapiro.test(cs$dat1$time_tv) ## ## Shapiro-Wilk normality test ## ## data: cs$dat1$time_tv ## W = 0.97989, p-value = 0.1304 shapiro.test(cs$dat1$cholesterol) ## ## Shapiro-Wilk normality test ## ## data: cs$dat1$cholesterol ## W = 0.97594, p-value = 0.06387 If a variable is not normally distributed, you can transform it, carry on regardless since the Pearson correlation is fairly robust to deviations from normality, or revert to Spearman and Kendall. Test Calculate Pearson’s correlation, Spearman’s rho, or Kendall’s tau. dat meets the assumptions for Pearson’s correlation, but try Spearman’s rho and Kendall’s tau too, just to see how close they come to Pearson. dat2 only meets the assumptions for Spearman and Kendall. Pearson’s Correlation dat1 met the conditions for Pearson’s correlation. (cs$cc_pearson &lt;- cor.test(cs$dat1$cholesterol, cs$dat1$time_tv, method = &quot;pearson&quot;) ) ## ## Pearson&#39;s product-moment correlation ## ## data: cs$dat1$cholesterol and cs$dat1$time_tv ## t = 3.9542, df = 98, p-value = 0.0001451 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.1882387 0.5288295 ## sample estimates: ## cor ## 0.3709418 r = 0.37 falls in the range of a “moderate” linear relationship. \\(r^2\\) = 14% is the coefficient of determination. Interpret it as the percent of variability in one variable that is explained by the other. If you are not testing a hypothesis (HO: \\(\\rho \\ne 0\\)), you can report just report r. Otherwise, include the p-value. Report your results like this: A Pearson’s product-moment correlation was run to assess the relationship between cholesterol concentration and daily time spent watching TV in males aged 45 to 65 years. One hundred participants were recruited. Preliminary analyses showed the relationship to be linear with both variables normally distributed, as assessed by Shapiro-Wilk’s test (p &gt; .05), and there were no outliers. There was a statistically significant, moderate positive correlation between daily time spent watching TV and cholesterol concentration, r(98) = 0.37, p &lt; .0005, with time spent watching TV explaining 14% of the variation in cholesterol concentration. You wouldn’t use Spearman’s rho or Kendall’s tau here since the more precise Pearson’s correlation is available. But just out of curiosity, here are the correlations using those two measures. cor(cs$dat1$cholesterol, cs$dat1$time_tv, method = &quot;kendall&quot;) ## [1] 0.2383971 cor(cs$dat1$cholesterol, cs$dat1$time_tv, method = &quot;spearman&quot;) ## [1] 0.3322122 Both Kendall and Spearman produced more conservative estimates of the strength of the relationship - Kendall especially so. You probably wouldn’t use linear regression here either because it describes the linear relationship between a response variable and changes to an independent explanatory variable. Even though we are reluctant to interpret a regression model in terms of causality, that is what is implied the formulation y ~ x and independence assumption in of X. Nevertheless, correlation and regression are related. The slope term in a simple linear regression of the normalized values equals the Pearson correlation. lm( y ~ x, data = cs$dat1 %&gt;% mutate(y = scale(cholesterol), x = scale(time_tv)) ) ## ## Call: ## lm(formula = y ~ x, data = cs$dat1 %&gt;% mutate(y = scale(cholesterol), ## x = scale(time_tv))) ## ## Coefficients: ## (Intercept) x ## -3.092e-16 3.709e-01 Spearman’s Rho Data set dat2 did not meet the conditions for Pearson’s correlation, so use Spearman’s rho and/or Kendall’s tau. Start with Spearman’s rho. Recall that Spearman’s rho is just the Pearson correlation applied to the ranks. Recall also that the Pearson’s correlation is just the covariance divided by the product of the standard deviations. You can quickly calculate it by hand. cov(rank(cs$dat2$income), rank(cs$dat2$tax_too_high)) / (sd(rank(cs$dat2$income)) * sd(rank(cs$dat2$tax_too_high))) ## [1] 0.6024641 Use the function though. I don’t get why cor.test requires x and y be numeric. (cs$spearman &lt;- cor.test( as.numeric(cs$dat2$tax_too_high), as.numeric(cs$dat2$income), method = &quot;spearman&quot;) ) ## Warning in cor.test.default(as.numeric(cs$dat2$tax_too_high), ## as.numeric(cs$dat2$income), : Cannot compute exact p-value with ties ## ## Spearman&#39;s rank correlation rho ## ## data: as.numeric(cs$dat2$tax_too_high) and as.numeric(cs$dat2$income) ## S = 914.33, p-value = 0.001837 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.6024641 Interpret the statistic using the same rule of thumb as for Pearson’s correlation. A rho over .5 is a strong correlation. A Spearman’s rank-order correlation was run to assess the relationship between income level and views towards income taxes in 24 participants. Preliminary analysis showed the relationship to be monotonic, as assessed by visual inspection of a scatterplot. There was a statistically significant, strong positive correlation between income level and views towards income taxes, \\(r_s\\) = 0.602, p = 0.002. Kendall’s Tau Recall that Kendall’s tau is a function of concordant (C), discordant (D) and tied (Tx and Ty) pairs of observations. The manual calculation is a little more involved. Here is the function instead. (cs$kendall &lt;- cor.test( as.numeric(cs$dat2$tax_too_high), as.numeric(cs$dat2$income), method = &quot;kendall&quot;) ) ## Warning in cor.test.default(as.numeric(cs$dat2$tax_too_high), ## as.numeric(cs$dat2$income), : Cannot compute exact p-value with ties ## ## Kendall&#39;s rank correlation tau ## ## data: as.numeric(cs$dat2$tax_too_high) and as.numeric(cs$dat2$income) ## z = 2.9686, p-value = 0.002991 ## alternative hypothesis: true tau is not equal to 0 ## sample estimates: ## tau ## 0.5345225 As with the case study on cholesterol and television, Kendall’s tau was more conservative than Spearman’s rho. \\(\\tau_b\\) = 0.535 is still in the “strong” range, though just barely. A Kendall’s tau-b correlation was run to assess the relationship between income level and views towards income taxes amongst 24 participants. Preliminary analysis showed the relationship to be monotonic, as assessed by visual inspection of a scatterplot. There was a statistically significant, strong positive correlation between income level and the view that taxes were too high, \\(\\tau_b\\) = 0.535, p = 0.003. "],["chisq-independence.html", "6.5 Chi-Square Test of Independence", " 6.5 Chi-Square Test of Independence The chi-square test of independence tests whether two categorical variables are associated, or are instead independent12. It tests whether the observed joint frequency counts \\(O_{ij}\\) differ from expected frequency counts \\(E_{ij}\\) under the independence model (the model of independent explanatory variables, \\(\\pi_{ij} = \\pi_{i+} \\pi_{+j}\\). The null hypothesis is \\(O_{ij} = E_{ij}\\). The test assumes the two variables are independent13 and that all cell counts are at least 5. Choose from two test statistics, Pearson \\(X^2\\) (and the continuity adjusted \\(X^2\\)), and deviance G. As \\(n \\rightarrow \\infty\\) their sampling distributions approach \\(\\chi^2(df)\\) with degrees of freedom (df) equal to the saturated model df \\(I \\times J - 1\\) minus the independence model df \\((I - 1) + (J - 1)\\), which you can algebraically solve for \\(df = (I - 1)(J - 1)\\). The Pearson goodness-of-fit statistic is \\[X^2 = \\sum \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\] where \\(O_{ij}\\) is the observed count, and \\(E_{ij}\\) is the product of the row and column marginal probabilities. The deviance statistic is \\[G = 2 \\sum_{ij} O_{ij} \\log \\left( \\frac{O_{ij}}{E_{ij}} \\right)\\] \\(X^2\\) and G increase with the disagreement between the saturated model proportions \\(p_{ij}\\) and the independence model proportions \\(\\pi_{ij}\\). The test is sometimes call the chi-square test for association.↩︎ Independence is usually true by assumption and/or construction. It can be violated by, for example, a sample that includes spouses where one spouse’s status (e.g., preference for a vacation destination) may be related to the other spouse’s status.↩︎ "],["fisher-exact.html", "6.6 Fisher’s Exact Test", " 6.6 Fisher’s Exact Test Whereas the chi-squared and G-test rely on the approximation that the test statistic distribution approaches \\(\\chi^2\\) as \\(n \\rightarrow \\infty\\), Fisher’s exact test is an “exact test” in that the p-value is calculated exactly from the hypergeometric distribution. Therefore Fisher’s test should only apply only to 2 x 2 tables. For some reason, it doesn’t.14 The test assumes the row totals \\(n_{i+}\\) and the column totals \\(n_{+j}\\) are fixed by study design, and the expected values of at least 20% of cells in the table have expected cell count &gt;5, and no expected cell count is 0. The famous example of the Fisher exact test is the “Lady tea testing” example. A lady claims she can guess whether the milk was poured into the cup before or after the tea. The experiment consists of 8 cups, 4 with milk poured first, 4 with milk poured second. The lady guesses correctly in 6 of the 8 cups. (tea &lt;- matrix(c(3, 1, 1, 3), nrow = 2, dimnames = list(Guess = c(&quot;Milk&quot;, &quot;Tea&quot;), Truth = c(&quot;Milk&quot;, &quot;Tea&quot;)))) ## Truth ## Guess Milk Tea ## Milk 3 1 ## Tea 1 3 This is a hypergeometric distribution question because you want to know the probability of 3 or 4 successes in a sample of 4. If \\(X = k\\) is the count of successful events in a sample of size \\(n\\) without replacement from a population of size \\(N\\) containing \\(K\\) successes, then \\(X\\) is a random variable with a hypergeometric distribution \\[f_X(k|N, K, n) = \\frac{{{K}\\choose{k}}{{N-K}\\choose{n-k}}}{{N}\\choose{n}}.\\] The formula follows from the frequency table of the possible outcomes. \\({K}\\choose{k}\\) is the number of ways to get k successes in K draws. \\({N-K}\\choose{n-k}\\) is the number of ways to fail to get k successes in K draws. And the denominator \\({N}\\choose{n}\\) is the total of all ways to succeed and fail. tibble::tribble( ~` `, ~Sampled, ~`Not Sampled`, ~Total, &quot;success&quot;, &quot;k&quot;, &quot;K-k&quot;, &quot;K&quot;, &quot;non-success&quot;, &quot;n-k&quot;, &quot;(N-K)-(n-k)&quot;, &quot;N-K&quot;, &quot;Total&quot;, &quot;n&quot;, &quot;N-n&quot;, &quot;N&quot; ) %&gt;% flextable::flextable() %&gt;% flextable::autofit() .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; border-color: transparent; caption-side: top; } .tabwid-caption-bottom table{ caption-side: bottom; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .katex-display { margin: 0 0 !important; } .cl-f08dad16{}.cl-f07f7246{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-f0864f94{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-f0869184{width:1.143in;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f0869198{width:0.888in;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f08691a2{width:1.168in;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f08691ac{width:0.625in;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f08691ad{width:1.143in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f08691ae{width:0.888in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f08691b6{width:1.168in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f08691b7{width:0.625in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f08691c0{width:1.143in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f08691ca{width:0.888in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f08691cb{width:1.168in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f08691d4{width:0.625in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f08691d5{width:1.143in;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f08691de{width:0.888in;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f08691e8{width:1.168in;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f08691f2{width:0.625in;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} SampledNot SampledTotalsuccesskK-kKnon-successn-k(N-K)-(n-k)N-KTotalnN-nN Function choose() returns the binomial coefficient \\({{n}\\choose{k}} = \\frac{n!}{k!(n-k)!}\\). The probability of choosing correctly at least 3 times out of 4 is the number of combinations of k = 3 plus the number with k = 4 divided by the number of combinations of any outcome. k &lt;- 3; n &lt;- 4; K &lt;- 4; N &lt;- 8 choose(K, k) * choose(N-K, n-k) / choose(N, n) + choose(K, k+1) * choose(N-K, n-(k+1)) / choose(N, n) ## [1] 0.2428571 phyper() does this.15 (pi &lt;- phyper(q = k-1, m = K, n = N-K, k = n, lower.tail = FALSE)) ## [1] 0.2428571 The p-value from Fisher’s exact test is calculated this way. fisher.test(tea, alternative = &quot;greater&quot;) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: tea ## p-value = 0.2429 ## alternative hypothesis: true odds ratio is greater than 1 ## 95 percent confidence interval: ## 0.3135693 Inf ## sample estimates: ## odds ratio ## 6.408309 The odds ratio at the bottom is the odds of success divided by the odds of non-success. The sample odds ratio is (3/1) / (1/3) = 9, but the documentation for fisher.test() explains that this it calculates the conditional maximum likelihood estimate.16 It doesn’t apply to just 2 x 2, so I need to figure out why.↩︎ k-1 because it returns the probability of &gt;k and we want &gt;=k).↩︎ I don’t really know what that means. Brownlee might help.↩︎ "],["case-study-chi-square-fisher.html", "6.7 Case Study: Chi-Square, Fisher", " 6.7 Case Study: Chi-Square, Fisher A researcher investigates whether males and females enrolled in an Exercise Science degree course differ in the type of exercise then engage, competitive or non-competitive. They survey 25 males and 25 females. cs$or2x2 %&gt;% gtsummary::tbl_cross( percent = &quot;row&quot;, label = list(comp ~ &quot;Competitive&quot;) ) %&gt;% gtsummary::add_p() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #gbazacjkxv .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #gbazacjkxv .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #gbazacjkxv .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #gbazacjkxv .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #gbazacjkxv .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #gbazacjkxv .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #gbazacjkxv .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #gbazacjkxv .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #gbazacjkxv .gt_column_spanner_outer:first-child { padding-left: 0; } #gbazacjkxv .gt_column_spanner_outer:last-child { padding-right: 0; } #gbazacjkxv .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #gbazacjkxv .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #gbazacjkxv .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #gbazacjkxv .gt_from_md > :first-child { margin-top: 0; } #gbazacjkxv .gt_from_md > :last-child { margin-bottom: 0; } #gbazacjkxv .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #gbazacjkxv .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #gbazacjkxv .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #gbazacjkxv .gt_row_group_first td { border-top-width: 2px; } #gbazacjkxv .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #gbazacjkxv .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #gbazacjkxv .gt_first_summary_row.thick { border-top-width: 2px; } #gbazacjkxv .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #gbazacjkxv .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #gbazacjkxv .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #gbazacjkxv .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #gbazacjkxv .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #gbazacjkxv .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #gbazacjkxv .gt_footnote { margin: 0px; font-size: 90%; padding-left: 4px; padding-right: 4px; padding-left: 5px; padding-right: 5px; } #gbazacjkxv .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #gbazacjkxv .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #gbazacjkxv .gt_left { text-align: left; } #gbazacjkxv .gt_center { text-align: center; } #gbazacjkxv .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #gbazacjkxv .gt_font_normal { font-weight: normal; } #gbazacjkxv .gt_font_bold { font-weight: bold; } #gbazacjkxv .gt_font_italic { font-style: italic; } #gbazacjkxv .gt_super { font-size: 65%; } #gbazacjkxv .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 75%; vertical-align: 0.4em; } #gbazacjkxv .gt_asterisk { font-size: 100%; vertical-align: 0; } #gbazacjkxv .gt_indent_1 { text-indent: 5px; } #gbazacjkxv .gt_indent_2 { text-indent: 10px; } #gbazacjkxv .gt_indent_3 { text-indent: 15px; } #gbazacjkxv .gt_indent_4 { text-indent: 20px; } #gbazacjkxv .gt_indent_5 { text-indent: 25px; } Competitive Total p-value1 Yes No gender 0.023     Male 18 (72%) 7 (28%) 25 (100%)     Female 10 (40%) 15 (60%) 25 (100%) Total 28 (56%) 22 (44%) 50 (100%) 1 Pearson's Chi-squared test Conditions Use the Chi-Square test of independence for nominal, independent variables. The test requires all cell counts to be greater than 5. If your data does not meet this condition, consider collapsing some levels. If you collapse all the way to a 2 x 2 cross-tabulation and still do not have at least 5 counts per cell, use Fisher’s exact test. Use Fisher’s exact test for dichotomous nominal, independent variables. The test is valid for cross-sectional samples, but not for prospective or retrospective samples. Test Chi-Square To see what’s going on, here is chi-square test done by hand. The expected values are the joint probabilities from the independence model (e.g., \\(E(\\mathrm{male}, {\\mathrm{yes}}) = \\pi_{\\mathrm{male}} \\times \\pi_{\\mathrm{yes}} \\times n\\)). exer_table &lt;- cs$or2x2 %&gt;% table() expected &lt;- marginSums(exer_table, 1) %*% t(marginSums(exer_table, 2)) / sum(exer_table) (X2 &lt;- sum((exer_table - expected)^2 / expected)) ## [1] 5.194805 (df &lt;- (2 - 1) * (2 - 1)) ## [1] 1 pchisq(X2, df, lower.tail = FALSE) ## [1] 0.02265449 And the G test. (G &lt;- 2 * sum(exer_table * log(exer_table / expected))) ## [1] 5.294731 pchisq(G, df, lower.tail = FALSE) ## [1] 0.02139004 The chisq.test() function applies the Yates continuity correction by default to correct for situations with small cell counts. The Yates continuity correction subtracts 0.5 from the \\(O_{ij} - E_{ij}\\) differences. Set correct = FALSE to suppress Yates. The Yates continuity correction only applies to 2 x 2 tables. (cs$or2x2_chisq.test &lt;- chisq.test(exer_table, correct = FALSE)) ## ## Pearson&#39;s Chi-squared test ## ## data: exer_table ## X-squared = 5.1948, df = 1, p-value = 0.02265 Calculate the G test with DescTools::GTest(). (cs$or2x2_g.test &lt;- DescTools::GTest(exer_table, correct = &quot;none&quot;)) ## ## Log likelihood ratio (G-test) test of independence without correction ## ## data: exer_table ## G = 5.2947, X-squared df = 1, p-value = 0.02139 As a side note, if the cell size &gt;5 condition is violated, you can use Monte Carlo simulation. chisq.test(exer_table, correct = FALSE, simulate.p.value = TRUE) ## ## Pearson&#39;s Chi-squared test with simulated p-value (based on 2000 ## replicates) ## ## data: exer_table ## X-squared = 5.1948, df = NA, p-value = 0.05097 Fisher The documentation for fisher.test() explains that the p-value is based on the the first element of the contingency table “with non-centrality parameter given by the odds ratio.” I don’t understand the odds ratio business and cannot figure out how it is calculated. It says in the documentation for the estimate value that “Note that the conditional Maximum Likelihood Estimate (MLE) rather than the unconditional MLE (the sample odds ratio) is used.”, so that may hold the answer. At least the p-value I can calculate by hand. phyper(q = exer_table[1, 1] - 1, # k minus 1 m = sum(exer_table[1, ]), # K n = sum(exer_table[2, ]), # N - K k = sum(exer_table[, 1]), # n lower.tail = FALSE) * 2 ## [1] 0.04500454 (cs$or2x2_fisher.test &lt;- fisher.test(exer_table)) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: exer_table ## p-value = 0.045 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 1.02531 15.01800 ## sample estimates: ## odds ratio ## 3.74678 Post-Test: Phi, Cramer’s V The problem with the chi-square test for association is that it does not measure the strength of any association. Two measures of associate are often calculated after the chi-squared or Fisher tests. Cramer’s V is derived from the chi-square statistic. It restricts the statistic to a range of 0 to 1. 17 Values under .1 are considered poor evidence of association; &gt;.2 are “moderately strong” evidence. \\[V = \\sqrt{\\frac{\\chi^2 / n}{\\mathrm{min}(I, J) - 1}}\\] # by hand sqrt(cs$or2x2_chisq.test$statistic / sum(exer_table) / (min(2, 2) - 1)) ## X-squared ## 0.3223292 # from package (cs$or2x2_v &lt;- rcompanion::cramerV(exer_table)) ## Cramer V ## 0.3223 The Phi Coefficient is defined \\[\\Phi = \\sqrt{\\frac{AD-BC}{(A+B)(C+D)(A+C)(B+D)}}\\] where A, B, C, and D are the four values of the 2 x 2 contingency table. matrix(c(&quot;A&quot;, &quot;C&quot;, &quot;B&quot;, &quot;D&quot;), nrow = 2) ## [,1] [,2] ## [1,] &quot;A&quot; &quot;B&quot; ## [2,] &quot;C&quot; &quot;D&quot; Similar to a Pearson Correlation Coefficient, a Phi Coefficient takes on values between -1 and 1. # by hand det(matrix(exer_table, nrow = 2)) / sqrt(prod(c(marginSums(exer_table, 1), marginSums(exer_table, 2)))) ## [1] 0.3223292 # from package (cs$or2x2_phi &lt;- psych::phi(exer_table)) ## [1] 0.32 Reporting A chi-square test for association was conducted between gender and preference for performing competitive sport. All expected cell frequencies were greater than five. There was a statistically significant association between gender and preference for performing competitive sport, (\\(X^2\\)(1) = 5.195, p = 0.023. There was a moderately strong association between gender and preference for performing competitive sport, V = 0.322. A Fisher’s Exact test was conducted between gender and preference for performing competitive sport. There was a statistically significant association between gender and preference for performing competitive sport, p = 0.045. This tutorial says both variables should have more than two levels, but doesn’t explain why.↩︎ "],["kruskals-lambda.html", "6.8 Kruskal’s Lambda", " 6.8 Kruskal’s Lambda "],["relative-risk.html", "6.9 Relative Risk", " 6.9 Relative Risk "],["odds-ratio.html", "6.10 Odds Ratio", " 6.10 Odds Ratio Calculate relative risks for dichotomous nominal, independent variables when one variable is independent, the other dependent. Relative risks are valid for prospective and retrospective cohort designs, randomized controlled trials (RCTs), and some types of cross-sectional studies, but not with case-control studies. Calculate odds ratios as an alternative to relative risk. It makes the same assumptions. Unlike relative risk, it is valid for all study designs. cs$or2 ## NULL # Smoking vs Lung Cancer # https://statistics.laerd.com/premium/spss/rr2x2/relative-risk-2x2-in-spss.php cs$rr2x2 &lt;- read.spss( &quot;./input/relative-risk-2x2-individual-scores.sav&quot;, to.data.frame = TRUE ) "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

[["index.html", "Statistical Inference Data Analyst Handbook Preface", " Statistical Inference Data Analyst Handbook Michael Foley 2022-01-01 Preface These are notes from books, classes, tutorials, vignettes, etc. They contain mistakes, are poorly organized, and are sloppy on fundamentals. They should improve over time, but thats all I can say for it. Use at your own risk. The focus of this handbook is statistical inference, including population estimates, group comparisons, and regression modeling. Not included here: probability, machine learning, text mining, survey analysis, or survival analysis. These subjects frequently arise at work, but are distinct enough and large enough to warrant separate handbooks. "],["statistical-inference.html", "Statistical Inference", " Statistical Inference Statistical inference is the use of a samples distribution to describe the population distribution. Hypothesis tests, confidence intervals, and effect size estimates are all examples of statistical inference. We wary of published study results. Identical studies might produce significant and non-significant results, yet only the significant result is likely to reach publication (publication bias). The researcher may have tortured the data until they found a statistically significant result. The study might suffer from low statistical power. Applying principles of inference can mitigate these problems. There are at least three approaches to establishing statistical inference: frequentist, likelihood, and Bayesian. Think of them philosophically. The frequentist approach is the path of action. It rejects a null hypothesis if the p-value is low because repeated sample analyses are likely to agree. The likelihood apprach is the path of knowledge. It compares the observed summary measure to the likelihoods of the other possible realities. The Bayesian approach is the path of belief. It uses a summary measure to update the prior belief. "],["frequentist-statistics.html", "Chapter 1 Frequentist Statistics", " Chapter 1 Frequentist Statistics P-values express how surprising the summary measure is given the null hypothesis (H0). Suppose you hypothesize that IQs have increased from the established mean of \\(\\mu_0\\) = 100. H0 is \\(\\mu\\) = 100 and the alternative hypothesis, H1, is \\(\\mu\\) &gt; 100. Also suppose you are right and the population mean IQ is actually 106. Finally, assume the population is some very big number - 1,000,000 for convenience. mu_0 &lt;- 100 mu &lt;- 106 sigma &lt;- 15 N &lt;- 1000000 pop_100 &lt;- data.frame(person = seq(1, N), iq = rnorm(N, mu_0, sigma)) pop_106 &lt;- data.frame(person = seq(1, N), iq = rnorm(N, mu, sigma)) Here is what the distribution of IQs might look like. You take a random sample of n = 30 IQs from the population. n &lt;- 30 x &lt;- sample(pop_106$iq, n) (x_bar &lt;- mean(x)) ## [1] 105.5975 You measure \\(\\bar{x}\\) = 105.6, SD = 15.2. How surprising is this result given H0 that \\(\\mu\\) is 100? I.e., what is the probability of observing \\(\\bar{x}\\) this extreme? According to the Central Limit Theorem (CLM) repeated samples of size n from a large population will yield \\(\\bar{x}\\) values that approach a normal distribution centered at \\(\\mu\\) with a standard deviation equal to the population SD (\\(\\sigma\\)) divided by \\(\\sqrt{n}\\). The standard deviation of the sampling distribution of the mean is commonly referred to as the standard error (SE). For a sample size of n = 30, and a \\(\\sigma\\) of 15, youd expect repeated samples to converge on a mean of 100 with SE = 2.7. You can verify this empirically. sim &lt;- replicate(1000, mean(sample(pop_100$iq, 30))) Here is the distribution of \\(\\bar{x}\\) measurements from 1,000 random samples from the hypothesized population. So what is the probability of measuring \\(\\bar{x}\\) = 106? It is the probability of measuring a value \\(\\frac{\\bar{x} - \\mu_0}{\\sigma / \\sqrt{n}}\\) = 2.0 standard deviations from 100. You can look up the probability from the normal distribution. q &lt;- (x_bar - mu_0) / (sigma / sqrt(n)) pnorm(q, lower.tail = FALSE) ## [1] 0.02048121 You can get a sense of what this probability means by overlaying the sampling distribution of the mean from the presumed (H0) distribution with the sampling distribution of the mean from the actual population. The sampling distribution of the mean is centered at \\(\\bar{x}\\) = 106, and that is well into the \\(\\alpha\\) = .05 level (the yellow-shaded region). The p-value is 0.020  the probability of measuring a mean IQ of 106 from a sample of size n = 30 when the true population mean is 100. Using an \\(\\alpha\\) level of significance, you will reject H0, which is a true positive given the fact that in this example the true population mean is 106. But you can imagine a stricter threshold level of significance, shrinking the yellow region to the right of the dashed blue line. Then we mistakenly fail to reject H0, a false negative (Type II error). "],["type-i-and-ii-errors.html", "1.1 Type I and II Errors", " 1.1 Type I and II Errors Either H0 or H1 is correct, and you must choose to either reject or not reject H0. That means there are four possible states at the end of your study. If your summary measure is extreme enough for you to declare a positive result and reject H0, you are either correct (true positive) or incorrect (false positive). False positives are called Type I errors. Alternatively, if your summary measure is not extreme enough for you to reject H0, you are either correct (true negative) or incorrect (false negative). False negatives are called Type II errors. The probabilities of landing in these four states depend on your chosen significance level, \\(\\alpha\\), and on the statistical power of the study, 1 - \\(\\beta\\). H0 True H1 True Significance test is positive, so you reject H0. False Positive Type I Error Probability = \\(\\alpha\\) True Positive Good Call! Probability = 1 - \\(\\beta\\) Significance test is negative, so you do not reject H0. True Negative Good Call! Probability = (\\(1 - \\alpha\\)) False Negative Type II Error Probability = \\(\\beta\\) \\(\\alpha\\) is the expected rate of Type I errors due to summary measures that are extreme by chance alone. \\(\\beta\\) is the expected rate of Type II errors due to summary measures that are not extreme by chance alone. In the IQ example, if the reality is \\(\\mu\\) = 100, any sample mean over 104.5 would be a false positive under the \\(\\alpha\\) = .05 level of significance. If the reality is \\(\\mu\\) = 106, any sample mean under 104.5 would be a false negative under the \\(1 - \\beta\\) = 0.64 statistical power of the study. "],["statistical-power.html", "1.2 Statistical Power", " 1.2 Statistical Power Type II error rates (\\(\\beta\\)) vary inversely with the power of the study (1 - \\(\\beta\\)). Statistical power is an increasing function of a) sample size, b) effect size, and c) significance level. The positive association with significance level means there is a trade-off between Type I and Type II error rates. A small \\(\\alpha\\) sets a high bar for rejecting H0, but you run the risk of failing to appreciate a real difference. On the other hand, a small \\(\\alpha\\) sets a low bar for rejecting H0, and you run the risk of judging a random difference as a real difference. The 1 - \\(\\beta\\) statistical power threshold is usually set at .80, similar to the \\(\\alpha\\) = .05 level of significance threshold convention. Given a real effect, a study with a statistical power of .80 will only find a positive test result 80% of the time. There may be such a thing as too much power, however. With a large enough sample size, even trivial effect sizes may yield a positive test result. You need to consider both sides of this coin. A power analysis is frequently used to determine the sample size required to detect a threshold effect size given an \\(\\alpha\\) level of significance criteria. A power analysis expresses the relationship among four components. If you know any three, it can return the fourth. The components are a) power (1 - \\(\\beta\\)), b) sample size (n), c) significance (\\(\\alpha\\)), and d) expected effect size (Cohens d). Suppose you set power at .80 and significance at .05. You can use the power test to see the relationship between sample size and effect size. Lets do that with the IQ example. I multiplied Cohens d \\((\\bar{x} - \\mu_0)/\\sigma\\) by \\(\\sigma\\) to get a non-normalized effect size. The plot shows a sample size of 30 is required to detect an effect size of 7 at a .05 significance level with 80% probability. If an effect size of 5 is important, then if you want to detect it at a .05 significance level with 80% probability, you will need a sample size of at least 58 (always round up). pwr::pwr.t.test( d = 5 / sigma, sig.level = .05, power = 0.80, type = &quot;one.sample&quot;, alternative = &quot;greater&quot; ) ## ## One-sample t test power calculation ## ## n = 57.02048 ## d = 0.3333333 ## sig.level = 0.05 ## power = 0.8 ## alternative = greater "],["what-p-values-would-you-expect.html", "1.3 What p-values would you expect?", " 1.3 What p-values would you expect? This section is based on ideas I learned from homework assignment 1 in Daniel Lakenss Coursera class Improving your statistical inferences. What distribution of p-values would you expect if there is a true effect and you repeated the study many times? What if there is no true effect? The answer is completely determined by the statistical power of the study. To see this, run 100,000 simulations of an experiment measuring the average IQ from a sample of size n = 26. The samples will be 26 random values from the normal distribution centered at 106 with a standard deviation of 15. H0 is \\(\\mu\\) = 100. # 100,000 random samples of IQ simulations from a normal distribution where # sigma = 15. True population value is 100, but we&#39;ll try other values. n_sims &lt;- 1E5 mu &lt;- 100 sigma &lt;- 15 run_sim &lt;- function(mu_0 = 106, n = 26) { data.frame(i = 1:n_sims) %&gt;% mutate( x = map(i, ~ rnorm(n = n, mean = mu_0, sd = sigma)), z = map(x, ~ t.test(., mu = mu)), p = map_dbl(z, ~ .x$p.value), x_bar = map_dbl(x, mean) ) %&gt;% select(x_bar, p) } The null hypothesis is that the average IQ is 100. Our rigged simulation finds an average IQ of 106 - an effect size of 6. sim_106_26 &lt;- run_sim(mu_0 = 106, n = 26) glimpse(sim_106_26) ## Rows: 100,000 ## Columns: 2 ## $ x_bar &lt;dbl&gt; 104.0366, 107.4146, 106.3826, 101.9452, 104.9108, 106.0985, 109.~ ## $ p &lt;dbl&gt; 0.1764747839, 0.0345864346, 0.0419669200, 0.4522238256, 0.065089~ mean(sim_106_26$x_bar) ## [1] 106.0062 The statistical power achieved by the simulations is 50%. That is, the typical simulation detected the effect size of 6 at the .05 significance level about 50% of the time. pwr.t.test( n = 26, d = (106 - 100) / 15, sig.level = .05, type = &quot;one.sample&quot;, alternative = &quot;two.sided&quot; ) ## ## One-sample t test power calculation ## ## n = 26 ## d = 0.4 ## sig.level = 0.05 ## power = 0.5004646 ## alternative = two.sided That means that given a population with an average IQ of 106, a two-sided hypothesis test of H0: \\(\\mu\\) = 100 from a sample of size 26 will measure an \\(\\bar{x}\\) with a p-value under .05 only 50% of the time. You can see that in this histogram of p-values. sim_106_26 %&gt;% plot_sim() Had there been no effect to observe, youd expect all p-values to be equally likely, so the 20 bins would all have been 5% of the number of simulations  i.e., uniformly distributed under the null. This is called 0 power, although 5% of the p-values will still be significant at the .05 level. The 5% of p-values &lt; .05 is the Type II error rate - that probability of a positive test result when there is no actual effect to observe. run_sim(mu_0 = 100, n = 26) %&gt;% plot_sim(mu_0 = 100) If you want a higher powered study that would detect the effect at least 80% of the time (the normal standard), youll need a higher sample size. How high? Conduct the power analysis again, but specify the power while leaving out the sample size. pwr.t.test( power = 0.80, d = (106 - 100) / 15, sig.level = .05, type = &quot;one.sample&quot;, alternative = &quot;two.sided&quot; ) ## ## One-sample t test power calculation ## ## n = 51.00945 ## d = 0.4 ## sig.level = 0.05 ## power = 0.8 ## alternative = two.sided You need 51 people (technically, you might want to round up to 52). Heres what that looks like. 80% of p-values are below .05 now. run_sim(mu_0 = 106, n = 51) %&gt;% plot_sim(mu_0 = 106) So far, weve discovered that when there is an effect, the probability that the measure p-value is under the \\(\\alpha\\) significance level equals the power of the study, 1 - \\(\\beta\\) - the true positive rate, and \\(\\beta\\) will be above the \\(\\alpha\\) level - the false negative rate. Weve also discovered that when there is no effect, all p-values are equally likely, so \\(\\alpha\\) of them will be below the \\(alpha\\) level of significance - the false positive rate, and 1 - \\(\\alpha\\) will be above \\(\\alpha\\) - the true negative rate. Its not the case that all p-values below 0.05 are support for the alternative hypothesis. If the statistical power is high enough, a p-value just under .05 can be even less likely under the null hypothesis. run_sim(mu_0 = 108, n = 51) %&gt;% mutate(bin = case_when(p &lt; .01 ~ &quot;0.00 - 0.01&quot;, p &lt; .02 ~ &quot;0.01 - 0.02&quot;, p &lt; .03 ~ &quot;0.02 - 0.03&quot;, p &lt; .04 ~ &quot;0.03 - 0.04&quot;, p &lt; .05 ~ &quot;0.04 - 0.05&quot;, TRUE ~ &quot;other&quot;) ) %&gt;% janitor::tabyl(bin) ## bin n percent ## 0.00 - 0.01 86618 0.86618 ## 0.01 - 0.02 5011 0.05011 ## 0.02 - 0.03 2353 0.02353 ## 0.03 - 0.04 1353 0.01353 ## 0.04 - 0.05 887 0.00887 ## other 3778 0.03778 (Recall that under H0, all p-values are equally likely, so each of the percentile bins would contain 1% of p-values.) In fact, at best, a p-value between .04 and .05 can only be about four times as likely under the alternative hypothesis as the null hypothesis. If your p-value is just under .05, it is at best weak support for the alternative hypothesis. "],["further-reading.html", "1.4 Further Reading", " 1.4 Further Reading Pritha Bhandari has two nice posts on Type I and Type II errors and Statistical Power. Daniel Lakenss Coursera class Improving your statistical inferences has a great p-value simulation exercise in Week 1 (assignment) "],["likelihood-statistics.html", "Chapter 2 Likelihood Statistics", " Chapter 2 Likelihood Statistics Likelihood functions are an approach to statistical inference (along with Frequentist and Bayesian). Likelihoods are functions of a data distribution parameter. For example, the binomial likelihood function is \\[L(\\theta) = \\frac{n!}{x!(n-x)!}\\cdot \\theta^x \\cdot (1-\\theta)^{n-x}\\] You can use the binomial likelihood function to assess the likelihoods of various hypothesized population probabilities, \\(\\theta\\). Suppose you sample n = 10 coin flips and observe x = 8 successful events (heads) for an estimated heads probability of .8. The likelihood of a fair coin, \\(\\theta\\) = .05 given the evidence is only 0.044. dbinom(8, 10, .5) ## [1] 0.04394531 You can see from the plot below that the likelihood function is maximized at \\(\\theta\\) = 0.8 (likelihood = 0.302). The actual value of the likelihood is unimportant - its a density. You can combine likelihood estimates by multiplying them. Suppose one experiment finds 4 of 10 heads and a second experiment finds 8 of 10 heads. Youd hope two experiments could be combined to achieve the same result as a single experiment with 12 of 20 heads, and that is indeed the case. x &lt;- dbinom(4, 10, seq(0, 1, .1)) y &lt;- dbinom(8, 10, seq(0, 1, .1)) z &lt;- dbinom(12, 20, seq(0, 1, .1)) round((x / max(x)) * (y / max(y)), 3) ## [1] 0.000 0.000 0.000 0.004 0.035 0.119 0.178 0.113 0.022 0.000 0.000 round(z, 3) ## [1] 0.000 0.000 0.000 0.004 0.035 0.120 0.180 0.114 0.022 0.000 0.000 Compare competing estimates of \\(\\theta\\) with the likelihood ratio. The likelihood of \\(\\theta\\) = .8 vs \\(\\theta\\) = .5 (fair coin) is \\(\\frac{L(\\theta = 0.8)}{L(\\theta = 0.5)}\\) = 6.87. A likelihood ratio of &gt;= 8 is moderately strong evidence for an alternative hypothesis. A likelihood ratio of &gt;= 32 is strong evidence for the alternative hypothesis. Keep in mind that likelihood ratios are relative evidence of H1 vs H0 - both hypotheses may be quite unlikely! A set of studies usually include both positive and negative test results. You can see this from the likelihood plots below. These are the likelihood curves produced from x = [0..3] successes in a sample of 3. Think of this as the likelihood of [0..3] positive findings in 3 studies based on an \\(\\alpha\\) = .05 level of significance and a .80 1 - \\(\\beta\\) statistical power of the study. The yellow line at .05 is the likelihood of a Type I error of concluding there is an effect when H1 is false. The yellow line at .80 is the likelihood of a Type II error of concluding there is no effect when H1 is true. The likelihood of 0 of 3 experiments reporting a positive effect under \\(\\alpha\\) = .05, 1 - \\(\\beta\\) = .80 is much higher under H0 (\\(\\theta\\) = .05) than under H1 (\\(\\theta\\) = .80): 0.857 vs 0.008 for a likelihood ratio of 107. The likelihood of 1 of 3 experiments reporting a positive effect is still higher under H0 than under H1: 0.135 vs 0.096 for a likelihood ratio of 1.41. For 2 of 3 experiments reporting a positive effect the likelihood ratio is 0.019, and for 3 of 3 experiments reporting a positive effect the likelihood ratio is 0.00024. The blue lines demarcates the points where mixed results are as likely as unanimous results. A set of studies are likely to produce unanimous results only if the number of studies is fairly high \\((\\gt 1 - n / (n+1))\\) or low \\((&lt; n / (n + 1))\\). "],["bayesian-statistics.html", "Chapter 3 Bayesian Statistics", " Chapter 3 Bayesian Statistics Bayesian inference estimates the probability, \\(\\theta\\), that an hypothesis is true. It differs from Frequentist inference in its insistence that all uncertainties be described by probabilities. Bayesian inference updates the prior probability distribution in light of new information. Bayesian inference builds on Bayes Law, so lets start there. "],["bayes-law.html", "3.1 Bayes Law", " 3.1 Bayes Law Bayes Law is a clever re-ordering of the relationship between joint probability and conditional probability, \\(P(\\theta D) = P(\\theta|D)P(D) = P(D|\\theta)P(\\theta)\\), into \\[P(\\theta|D) = \\frac{P(D|\\theta)P(\\theta)}{P(D)}.\\] \\(P(\\theta)\\) is the strength of your belief in \\(\\theta\\) prior to considering the data \\(D\\). \\(P(D|\\theta)\\) is the likelihood of observing \\(D\\) from a generative model with parameter \\(\\theta\\). Note that likelihood is the probability density, and is not quite the same as probability. For a continuous variable, likelihoods can sum to greater than 1. E.g., dbinom(seq(1, 100, 1), 100, .5) sums to 1, but dnorm(seq(0,50,.001), 10, 10) sums to 841. \\(P(D)\\) is the likelihood of observing \\(D\\) from any prior. It is the marginal distribution, or prior predictive distribution of \\(D\\). The likelihood divided by the marginal distribution is the proportional adjustment made to the prior in light of the data. \\(P(\\theta|D)\\) is the strength of your belief in \\(\\theta\\) posterior to considering \\(D\\). Bayes Law is useful for evaluating medical tests. A tests sensitivity is its probability of yielding a positive result \\(D\\) when condition \\(\\theta\\) exists. \\(P(D|\\theta)\\) is a test sensitivity, \\(\\mathrm{sens}\\). \\(P(\\theta)\\) is the probability of \\(\\theta\\) prior to the test (e.g., the general rate in society), \\(\\mathrm{prior}\\). The numerator of Bayes Law, the joint probability \\(P(D \\theta) = P(D|\\theta)P(\\theta)\\), is \\(\\mathrm{sens\\times prior}\\). A tests specificity is the probability of observing negative test result \\(\\hat{D}\\) when the condition does not exist, \\(\\hat{\\theta}\\). The specificity is the compliment of a false positive test result, \\(P(\\hat{D} | \\hat{\\theta}) = 1 - P(D | \\hat{\\theta})\\). The denominator of Bayes Law is the overall probability of a positive test result, \\(P(D) = P(D|\\theta)P(\\theta) + P(D|\\hat\\theta)P(\\hat\\theta)\\) or in terms of sensitivity and specificity, \\(P(D) = \\mathrm{(sens \\times prior) + (1 - spec)(1 - prior)}\\). Example. Suppose E. Coli is typically present in 4.5% of samples, and an E. Coli screen has a sensitivity of 0.95 and a specificity of 0.99. Given a positive test result, what is the probability that E. Coli is actually present? \\[P(\\theta|D) = \\frac{.95\\cdot .045}{.95\\cdot .045 + (1 - .99)(1 - .045)} = \\frac{.04275}{.04275 + .00955} = \\frac{.04275}{.05230} = 81.7\\%.\\] The elements of Bayes Law come directly from the contingency table. The first row is the positive test result. The probability of E. Coli is the joint probability of E. coli and a positive test divided by the probability of a positive test E. Coli Safe Total + Test .95 * .045 = 0.04275 .01 * .955 = 0.00955 0.05230 - Test .05 * .045 = 0.00225 .99 * .955 = 0.94545 0.94770 Total 0.04500 0.95500 1.00000 "],["bayesian-inference.html", "3.2 Bayesian Inference", " 3.2 Bayesian Inference Bayesian inference extends the logic of Bayes Law by replacing the prior probability estimate that \\(\\theta\\) is true with a prior probability distribution that \\(\\theta\\) is true. Rather than saying, I am x% certain \\(\\theta\\) is true, you are saying I believe the probability that \\(\\theta\\) is true is somewhere in a range that has maximum likelihood at x%. Let \\(\\Pi(\\theta)\\) be the prior probability function of \\(\\theta\\). \\(\\Pi(\\theta)\\) has a pmf or pdf \\(P(\\theta)\\), and a set of conditional distributions called the generative model for the observed data \\(D\\) given \\(\\theta\\), \\(\\{f_\\theta(D): \\theta \\in \\Omega\\}\\). \\(f_\\theta(D)\\) is the likelihood of observing \\(D\\) given \\(\\theta\\). Their product, \\(f_\\theta(D)P(\\theta)\\), is a joint distribution of \\((D, \\theta)\\). For continuous prior distributions, the marginal distribution for \\(D\\), called the prior predictive distribution, is \\[m(D) = \\int_\\Omega f_\\theta(D)P(\\theta) d\\theta\\] For discrete prior distributions, replace the integral with a sum, \\(m(D) = \\sum\\nolimits_\\Omega f_\\theta(D) P(\\theta)\\). The posterior probability distribution of \\(\\theta\\), conditioned on the observance of \\(D\\), is \\(\\Pi(\\cdot|D)\\). It is the joint density, \\(f_\\theta(D) P(\\theta),\\) divided by the the marginal density, \\(m(D)\\). \\[P(\\theta | D) = \\frac{f_\\theta(D) P(\\theta)}{m(D)}\\] The numerator makes the posterior proportional to the prior. The denominator is a normalizing constant that scales the likelihood into a proper density function (whose values sum to 1). It is helpful to look first at discrete priors, a list of competing priors to see how the observed evidence shifts the probabilities of the priors into their posterior probabilities. From there it is a straight-forward step to the more abstract case of continuous prior and posterior distributions. "],["discrete-cases.html", "3.3 Discrete Cases", " 3.3 Discrete Cases Suppose you have a string of numbers \\([1,1,1,1,0,0,1,1,1,0]\\) (7 ones and 3 zeros) produced by a Bernoulli random number generator. What parameter \\(p\\) was used in the Bernoulli function?1 D &lt;- c(1, 1, 1, 1, 0, 0, 1, 1, 1, 0) Your best guess is \\(p = 0.7\\), but how confident are you? Posit eleven competing priors, \\(\\theta = [.0, .1, \\ldots, 1]\\) with equal prior probabilities, \\(P(\\theta) = [1/11, \\ldots]\\). theta &lt;- seq(0, 1, by = 0.1) prior &lt;- rep(1/11, 11) Using a Bernoulli generative model, the likelihood of observing 7 ones and 3 zeros are \\(P(D|\\theta) = \\theta^7 + (1-\\theta)^3.\\) likelihood &lt;- theta^7 * (1 - theta)^3 data.frame(theta, likelihood) %&gt;% ggplot() + geom_segment(aes(x = theta, xend = theta, y = 0, yend = likelihood), linetype = 2, color = &quot;steelblue&quot;) + geom_point(aes(x = theta, y = likelihood), color = &quot;steelblue&quot;, size = 3) + scale_x_continuous(breaks = theta) + theme_minimal() + theme(panel.grid.minor = element_blank()) + labs(title = expression(paste(&quot;Maximum likelihood of observing D is at &quot;, theta, &quot; = 0.7.&quot;)), x = expression(theta), y = expression(f[theta](D))) The posterior probability is the likelihood divided by the marginal probability of observing \\(D\\) multiplied by the prior, \\(P(\\theta|D) = \\frac{P(D|\\theta)}{P(D)}\\cdot P(\\theta).\\) In this case, the marginal probability is straight-forward to calculate: it is the sum-product of the priors and their associated likelihoods. posterior &lt;- likelihood / sum(likelihood * prior) * prior What would the posterior look like if we started with an educated guess on \\(P(\\theta)\\) that more heavily weights \\(\\theta = 0.7\\)? prior &lt;- c(.05, .05, .05, .05, .05, .10, .15, .20, .15, .10, .05) posterior &lt;- likelihood / sum(likelihood * prior) * prior What if we now employ a larger data set? To see, generate a sample of 100 Bernoulli(.7) observations. D100 &lt;- rbernoulli(100, p = 0.7) %&gt;% as.numeric() likelihood &lt;- theta^sum(D100) * (1 - theta)^(length(D100)-sum(D100)) posterior &lt;- likelihood / sum(likelihood * prior) * prior What would it look like if it also had more competing hypotheses, \\(\\theta \\in (0, .01, .02, \\ldots, 1)\\). theta &lt;- seq(0, 1, by = .01) prior &lt;- rep(1/100, 101) likelihood &lt;- theta^sum(D100) * (1 - theta)^(length(D100)-sum(D100)) posterior &lt;- likelihood / sum(likelihood * prior) * prior Example borrowed from chris sandbox "],["continuous-cases.html", "3.4 Continuous Cases", " 3.4 Continuous Cases Continuing the example of inferring the parameter \\(p\\) used in the Bernoulli process, what if we considered all values between 0 and 1?2 When prior beliefs are best described in continuous distributions, express them using the beta, gamma, or normal distribution so that the posterior distributions are conjugates of the prior distributions with new parameter values. Otherwise, the marginal distribution is difficult to calculate. In this case, use the beta distribution, described by shape parameters, \\(\\alpha\\) and \\(\\beta\\). \\[P(\\theta|D,\\alpha,\\beta) = \\frac{f_\\theta(D) P(\\theta|\\alpha,\\beta)}{\\int_0^1 f_\\theta(D)P(\\theta|\\alpha, \\beta)d\\theta}\\] As with the discrete case, the numerator is the likelihood of observing \\(D\\) if \\(\\theta\\) is true multiplied by the prior probability, but now the prior is a Beta(\\(\\alpha\\), \\(\\beta\\)) distribution. The denominator, sometimes called the evidence, is the marginal probability of \\(D\\). The likelihood of observing \\(D\\) = \\(a\\) successes and \\(b\\) non-successes given a success probability of \\(p\\) = \\(\\theta\\) is \\[f_\\theta(D) = \\theta^a(1-\\theta)^b\\] The prior distribution is the probability density function of the beta distribution \\[P(\\theta|\\alpha,\\beta) = \\frac{1}{\\mathrm{B}(\\alpha, \\beta)}\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}\\] where \\(\\mathrm{B}(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)}\\) is the beta function. The marginal distribution is \\[\\int_0^1 f_\\theta(D)P(\\theta|\\alpha, \\beta)d\\theta = \\frac{\\mathrm{B}(\\alpha + a, \\beta + b)}{\\mathrm{B}(\\alpha, \\beta)}\\] Putting this all together, the posterior distribution is \\[P(\\theta|D, \\alpha, \\beta) = \\frac{1}{\\mathrm{B}(\\alpha + a, \\beta + b)} \\theta^{\\alpha-1+a}(1-\\theta)^{\\beta-1+b}\\] The posterior equals the prior with shape parameters incremented by the observed counts, \\(a\\) and \\(b.\\) plot_bayes &lt;- function(alpha, beta, a, b) { prior_ev &lt;- (alpha / (alpha + beta)) %&gt;% round(2) posterior_ev &lt;- ((alpha + a) / (alpha + beta + a + b)) %&gt;% round(2) dat &lt;- data.frame(theta = seq(0, 1, by = .01)) %&gt;% mutate(prior = (1 / beta(alpha, beta)) * theta^(alpha-1) * (1-theta)^(beta-1), prior_ci = theta &gt; qbeta(.025, alpha, beta) &amp; theta &lt; qbeta(.975, alpha, beta), likelihood = theta^a * (1-theta)^b, posterior = (1 / beta(alpha + a, beta + b)) * theta^(alpha-1+a) * (1-theta)^(beta-1+b), posterior_ci = theta &gt; qbeta(.025, alpha + a, beta + b) &amp; theta &lt; qbeta(.975, alpha + a, beta + b)) p_prior &lt;- dat %&gt;% ggplot(aes(x = theta, y = prior)) + geom_line(color = &quot;steelblue&quot;) + geom_area(aes(alpha = prior_ci), fill = &quot;steelblue&quot;) + geom_vline(xintercept = prior_ev, color = &quot;steelblue&quot;) + scale_alpha_manual(values = c(.1, .5)) + theme_minimal() + theme(legend.position = &quot;none&quot;) + labs(x = NULL) p_likelihood &lt;- dat %&gt;% ggplot(aes(x = theta, y = likelihood)) + geom_line(color = &quot;steelblue&quot;) + theme_minimal() + theme(legend.position = &quot;none&quot;) + labs(x = NULL) p_posterior &lt;- dat %&gt;% ggplot(aes(x = theta, y = posterior)) + geom_line(color = &quot;steelblue&quot;) + geom_area(aes(alpha = posterior_ci), fill = &quot;steelblue&quot;) + geom_vline(xintercept = posterior_ev, color = &quot;steelblue&quot;) + scale_alpha_manual(values = c(.1, .5)) + theme_minimal() + theme(legend.position = &quot;none&quot;) + labs(x = expression(theta)) out &lt;- p_prior / p_likelihood / p_posterior + plot_annotation( title = glue(&quot;Beta({alpha}, {beta}) prior with observed evidence a = {a} &quot;, &quot;and b = {b}&quot;), subtitle = &quot;with shaded 95% credible interval.&quot;, caption = glue(&quot;Prior expected value = {prior_ev}; Posterior expected &quot;, &quot;value = {posterior_ev}&quot;)) out } Suppose you claim complete ignorance and take a uniform Beta(1, 1) prior. Recall that you observed a = 7 ones and b = 3 zeros. The posterior expected value is still pretty close! plot_bayes(alpha = 10, beta = 10, a = 7, b = 3) Suppose you had prior reason to believe p = 0.7. You would model that as \\(\\alpha\\) = 7, \\(\\beta\\) = 3. The prior probability distribution would be \\(P(\\theta|\\alpha = 7,\\beta = 3) = \\frac{1}{\\mathrm{B}(7, 3)}\\theta^{7-1}(1-\\theta)^{3-1}\\). Then after observing a = 7 ones and b = 3 zeros, the posterior probability distribution would be \\(P(\\theta|\\alpha = 7+7,\\beta = 3+3) = \\frac{1}{\\mathrm{B}(7+7, 3+3)}\\theta^{7+7-1}(1-\\theta)^{3+3-1}\\). plot_bayes(alpha = 7, beta = 3, a = 7, b = 3) Chriss Sandbox again. "],["bayes-factors.html", "3.5 Bayes Factors", " 3.5 Bayes Factors The Bayes Factor (BF) is a measure of the relative evidence of one model over another. Take another look at Bayes formula: \\[P(\\theta|D) = \\frac{P(D|\\theta)P(\\theta)}{P(D)}.\\] Suppose you want to compare how two models explain and observed data outcome, \\(D\\). Model \\(M_1:f_1(D|\\theta_1)\\) says the observed data \\(D\\) was produced by a generative model with pdf \\(f_1\\) parameterized by \\(\\theta_2\\). Model \\(M_2:f_2(D|\\theta_2)\\) says it was produced by a generative model with pdf \\(f_2\\) parameterized by \\(\\theta_2\\). In each model you specify a prior probability distribution for the parameter If you take the ratio of the posterior probabilities, the posterior odds, the \\(P(D)\\) terms cancel and you have \\[\\frac{P(\\theta_1|D)}{P(\\theta_2|D)} = \\frac{P(D|\\theta_1)}{P(D|\\theta_2)} \\cdot \\frac{P(\\theta_1)}{P(\\theta_2)}\\] The posterior odds equals the ratio of the likelihoods multiplied by the prior odds. That likelihood ratio is the Bayes Factor (BF). Rearranging, BF is the odds ratio of the posterior and prior odds. \\[BF = \\frac{P(D|\\theta_1)}{P(D|\\theta_2)} = \\mathrm{\\frac{Posterior Odds}{Prior Odds}}\\] Return to the example of observing \\(D\\) = 7 ones and 3 zeros. You can compare an hypothesized \\(\\theta\\) of .5 to a completely agnostic model where \\(\\theta\\) is uniform over [0, 1]. The likelihood of observing \\(D\\) when \\(\\theta\\) = .5 is \\(P(D|\\theta_1) = 5^7(1-.5)^3\\) = 0.117. The likelihood of observing \\(D\\) where \\(\\theta\\) is uniform on [0, 1] is \\(P(D|\\theta_2) = \\int_0^1 \\binom{10}{3}q^7(1-q)^3dq\\) .5^1 * .5^1 ## [1] 0.25 dbinom(1, 1, .5) ## [1] 0.5 dbinom(11, 11, .5) ## [1] 0.0004882812 beta(11, 11) ## [1] 2.577402e-07 with a uniform Beta(1, 1) prior (i.e., complete agnosticism). The Bayes factor at \\(\\theta\\) = .7 quantifies how much the odds of H0: \\(\\theta\\) = .7 over H1: \\(\\hat{\\theta}\\) = .7. prior &lt;- function(theta, alpha, beta) { (1 / beta(alpha, beta)) * theta^(alpha-1) * (1-theta)^(beta-1) } posterior &lt;- function(theta, alpha, beta, a, b) { (1 / beta(alpha + a, beta + b)) * theta^(alpha-1+a) * (1-theta)^(beta-1+b) } prior(.5, 115, 85) ## [1] 1.164377 posterior(.5, 1, 1, 10, 10) ## [1] 3.700138 posterior(.5, 1, 1, 10, 10) / prior(.5, 1, 1) ## [1] 3.700138 1 / beta(115, 85) ## [1] 4.677704e+59 # Posterior Distribution 1/beta(1+10, 1+10) * .5^(1-1+10) * (1-.5)^(1-1+10) ## [1] 3.700138 dbeta(.5, 11, 11) ## [1] 3.700138 # Prior Beta Distributions 1/beta(1, 1) * .5^(1-1) * (1-.5)^(1-1) ## [1] 1 dbeta(.5, 1, 1) ## [1] 1 dbeta(.5, 115, 85) ## [1] 1.164377 The Bayes factor measures how much your prior belief is altered by the evidence. It is the ratio of the likelihoods at some hypothesized value before and after observing the data. In this case, our confidence increased by a factor of theta &lt;- 0.5 alpha &lt;- 1 beta &lt;- 1 a &lt;- 10 b &lt;- 10 (prior_likelihood &lt;- (1 / beta(alpha, beta)) * theta^(alpha-1) * (1-theta)^(beta-1)) ## [1] 1 (posterior_likelihood &lt;- (1 / beta(alpha + a, beta + b)) * theta^(alpha-1+a) * (1-theta)^(beta-1+b)) ## [1] 3.700138 (bayes_factor &lt;- posterior_likelihood / prior_likelihood) ## [1] 3.700138 # 3.7 on alpha = beta = 1 # 1.91 on alpha = beta = 4 "],["a-gentler-introduction.html", "3.6 A Gentler Introduction", " 3.6 A Gentler Introduction This section is my notes from DataCamp course Fundamentals of Bayesian Data Analysis in R. It is an intuitive approach to Bayesian inference. Suppose you purchase 100 ad impressions on a web site and receive 13 clicks. How would you describe the click rate? The Frequentist approach would be to construct a 95% CI around the click proportion. (ad_prop_test &lt;- prop.test(13, 100)) ## ## 1-sample proportions test with continuity correction ## ## data: 13 out of 100, null probability 0.5 ## X-squared = 53.29, df = 1, p-value = 2.878e-13 ## alternative hypothesis: true p is not equal to 0.5 ## 95 percent confidence interval: ## 0.07376794 0.21560134 ## sample estimates: ## p ## 0.13 How might you model this using Bayesian reasoning? One way is to run 1,000 experiments that sample 100 ad impression events from an rbinom() generative model using a uniform prior distribution of 0-30% click probability. The resulting 1,000 row data set of click probabilities and sampled click counts forms a joint probability distribution. This method of Bayesian analysis is called rejection sampling because you sample across the whole parameter space, then condition on the observed evidence. df_sim &lt;- data.frame(click_prob = runif(1000, 0.0, 0.3)) df_sim$click_n &lt;- rbinom(1000, 100, df_sim$click_prob) Condition the joint probability distribution on the 13 observed clicks to update your prior. The quantile() function returns the median and the .025 and .975 percentile values - the credible interval. # median and credible interval (sim_ci &lt;- df_sim %&gt;% filter(click_n == 13) %&gt;% pull(click_prob) %&gt;% quantile(c(.025, .5, .975))) ## 2.5% 50% 97.5% ## 0.0901092 0.1370293 0.2007142 Your posterior click rate likelihood is 13.7% with 95 credible interval [9.0%, 20.1%]. Here is the density plot of the 43 simulations that produced the 13 clicks. The median and 95% credible interval are marked. Thats pretty close to the frequentist result! Instead of running 1,000 experiments with randomly selected click probabilities and randomly selected click counts based on those probabilities, you could define a discrete set of candidate click probabilities, e.g.Â values between 0 and 0.3 incremented by .01, and calculate the click probability density for the 100 ad impressions. This method of Bayesian analysis is called grid approximation. df_bayes &lt;- expand.grid( click_prob = seq(0, .3, by = .001), click_n = 0:100 ) %&gt;% mutate( prior = dunif(click_prob, min = 0, max = 0.3), likelihood = dbinom(click_n, 100, click_prob), probability = likelihood * prior / sum(likelihood * prior) ) Condition the joint probability distribution on the 13 observed clicks to update your prior. df_bayes_13 &lt;- df_bayes %&gt;% filter(click_n == 13) %&gt;% mutate(posterior = probability / sum(probability)) Instead of using the quantile() function on these values to measure the median and credible interval, resample the posterior probability to create a distribution. sampling_idx &lt;- sample( 1:nrow(df_bayes_13), size = 10000, replace = TRUE, prob = df_bayes_13$posterior ) sampling_vals &lt;- df_bayes_13[sampling_idx, ] (df_bayes_ci &lt;- quantile(sampling_vals$click_prob, c(.025, .5, .975))) ## 2.5% 50% 97.5% ## 0.078 0.134 0.209 You can use a Bayesian model to estimate multiple parameters. Suppose you want to predict the water temperature in a lake on Jun 1 based on 5 years of prior water temperatures. temp &lt;- c(19, 23, 20, 17, 23) You model the water temperature as a normal distribution, \\(\\mathrm{N}(\\mu, \\sigma^2)\\) with a prior distribution \\(\\mu = \\mathrm{N}(18, 5^2)\\) and \\(\\sigma = \\mathrm{unif}(0, 10)\\) based on past experience. Using the grid approximation approach, construct a grid of candidate \\(\\mu\\) values from 8 to 30 degrees incremented by .5 degrees, and candidate \\(\\sigma\\) values from .1 to 10 incremented by .1 - a 4,500 row data frame. mdl_grid &lt;- expand_grid(mu = seq(8, 30, by = 0.5), sigma = seq(.1, 10, by = 0.1)) For each combination of \\(\\mu\\) and \\(\\sigma\\), the prior probabilities are the densities from \\(\\mu = \\mathrm{N}(18, 5^2)\\) and \\(\\sigma = \\mathrm{unif}(0, 10)\\). The combined prior is their product. The likelihoods are the products of the probabilities of observing each temp given the candidate \\(\\mu\\) and \\(\\sigma\\) values. mdl_grid_2 &lt;- mdl_grid %&gt;% mutate( mu_prior = map_dbl(mu, ~dnorm(., mean = 18, sd = 5)), sigma_prior = map_dbl(sigma, ~dunif(., 0, 10)), prior = mu_prior * sigma_prior, # combined prior, likelihood = map2_dbl(mu, sigma, ~dnorm(temp, .x, .y) %&gt;% prod()), posterior = likelihood * prior / sum(likelihood * prior) ) Calculate a credible interval by drawing 10,000 samples from the grid with sampling probability equal to the calculated posterior probabilities. Use the quantile() function to estimate the median and .025 and .975 quantile values. sampling_idx &lt;- sample(1:nrow(mdl_grid), size = 10000, replace = TRUE, prob = mdl_grid$posterior) ## Warning: Unknown or uninitialised column: `posterior`. sampling_vals &lt;- mdl_grid[sampling_idx, c(&quot;mu&quot;, &quot;sigma&quot;)] mu_ci &lt;- quantile(sampling_vals$mu, c(.025, .5, .975)) sigma_ci &lt;- quantile(sampling_vals$sigma, c(.025, .5, .975)) ci &lt;- qnorm(c(.025, .5, .975), mean = mu_ci[2], sd = sigma_ci[2]) data.frame(temp = seq(0, 30, by = .1)) %&gt;% mutate(prob = map_dbl(temp, ~dnorm(., mean = ci[2], sd = sigma_ci[2])), ci = if_else(temp &gt;= ci[1] &amp; temp &lt;= ci[3], &quot;Y&quot;, &quot;N&quot;)) %&gt;% ggplot(aes(x = temp, y = prob)) + geom_area(aes(y = if_else(ci == &quot;N&quot;, prob, 0)), fill = &quot;firebrick&quot;, show.legend = FALSE) + geom_line() + geom_vline(xintercept = ci[2], linetype = 2) + theme_minimal() + scale_x_continuous(breaks = seq(0, 30, 5)) + theme(panel.grid.minor = element_blank()) + labs(title = &quot;Posterior temperature probability&quot;, subtitle = glue(&quot;mu = {ci[2] %&gt;% scales::number(accuracy = .1)}, 95%-CI (&quot;, &quot;{ci[1] %&gt;% scales::number(accuracy = .1)}, &quot;, &quot;{ci[3] %&gt;% scales::number(accuracy = .1)})&quot;)) What is the probability the temperature is at least 18? pred_temp &lt;- rnorm(1000, mean = sampling_vals$mu, sampling_vals$sigma) scales::percent(sum(pred_temp &gt;= 18) / length(pred_temp)) ## [1] &quot;54%&quot; "],["coursera-notes-inference.html", "3.7 Coursera Notes Inference", " 3.7 Coursera Notes Inference Bayesian inference updates prior beliefs with accumulated evidence. The posterior odds equals the prior odds multiplied by the likelihood ratio of observing evidence under the competing hypotheses. \\[\\frac{P(H1|D)}{P(H0|D)} = \\frac{P(D|H1)}{P(D|H0)} \\times \\frac{P(H1)}{P(H0)}\\] (*This isnt quite what I expected. Wouldnt you replace the likelihood ratio, \\(\\frac{P(D|H1)}{P(D|H0)}\\) with the proportional adjustment, \\(\\frac{P(D|H1)}{P(D)}\\)?) Express the prior https://www.bayesrulesbook.com/ "],["one-sample.html", "Chapter 4 One-Sample", " Chapter 4 One-Sample Use one-sample tests to either describe a single variables frequency or central tendency, or to compare the frequency or central tendency to a hypothesized distribution or value. If the data generating process produces continuous outcomes (interval or ratio), and the outcomes are symmetrically distributed, the sample mean, \\(\\bar{x}\\), is a random variable centered at the population mean, \\(\\mu\\). You can then use a theoretical distribution (normal or student t) to estimate a 95% confidence interval (CI) around \\(\\mu\\), or compare \\(\\bar{x}\\) to an hypothesized population mean, \\(\\mu_0\\). If you (somehow) know the population variance, or the Central Limit Theorem (CLT) conditions hold, you can assume the random variable is normally distributed and use the z-test, otherwise assume the random variable has student t distribution and use the t-test.3 If the data generating process produces continuous outcomes that are not symmetrically distributed, use a non-parametric test like the Wilcoxon median test. If the data generating process produces discrete outcomes (counts), the sample count, \\(x\\), is a random variable from a Poisson, binomial, normal, or multinomial distribution, or a random variable from a theoretical outcome. For counts over a fixed time or space, treat the count as a random variable from a Poisson distribution with expected value \\(\\lambda\\) and variance \\(\\lambda\\). For counts within a fixed total that are then classified into two levels (usually yes/no), then treat the count as a random variable from a binomial distribution with expected value \\(n\\pi\\) and variance \\(n\\pi(1-\\pi)\\). For binomial distributions where \\(n\\ge30\\) and the frequency counts of both levels is \\(\\ge\\) 5, treat the proportion as a random variable from the normal distribution with expected valued \\(\\pi\\) and variance \\(\\frac{\\pi(1-\\pi)}{n}\\). For counts within a fixed total that are then classified into three or more levels, treat the count as a random variable from the multinomial distribution with expected value \\(n\\pi_j\\) and variance \\(n\\pi_j(1-\\pi_j)\\). Whatever the source of the expected values, you use either the chi-squared goodness-of-fit test or G test to test whether the observed values fit the expected values from the distribution. In the special case of binary outcomes with small (n &lt; 1,000), you can use Fishers exact test instead. The discrete variable tests are discussed in PSU STATS 504. The t-test returns nearly the same result as the z-test when the CLT holds, so in practice no one bothers with the z-test except as an aid to teach the t-test. "],["one-sample-mean-z-test.html", "4.1 One-Sample Mean z Test", " 4.1 One-Sample Mean z Test The z test is also called the normal approximation z test. It only applies when the sampling distribution of the population mean is normally distributed with known variance, and there are no significant outliers. The sampling distribution is normally distributed when the underlying population is normally distributed, or when the sample size is large \\((n &gt;= 30)\\), as follows from the central limit theorem. The t test returns similar results, plus it is valid when the variance is unknown, and that is pretty much always. For that reason, you probably will never use this test. Under the normal approximation method, the measured mean \\(\\bar{x}\\) approximates the population mean \\(\\mu\\), and the sampling distribution has a normal distribution centered at \\(\\mu\\) with standard error \\(se_\\mu = \\frac{\\sigma}{\\sqrt{n}}\\) where \\(\\sigma\\) is the standard deviation of the underlying population. Define a \\((1 - \\alpha)\\%\\) confidence interval as \\(\\bar{x} \\pm z_{(1 - \\alpha) {/} 2} se_\\mu\\), or test \\(H_0: \\mu = \\mu_0\\) with test statistic \\(Z = \\frac{\\bar{x} - \\mu_0}{se_\\mu}\\). Example The mtcars data set is a sample of n = 32 cars. The mean fuel economy is \\(\\bar{x} \\pm s\\) = 20.1 \\(\\pm\\) 6.0 mpg. The prior measured overall fuel economy for vehicles was \\(\\mu_0 \\pm \\sigma\\) = 18.0 \\(\\pm\\) 6.0 mpg. Has fuel economy improved? The sample size is \\(\\ge\\) 30, so the sampling distribution of the population mean is normally distributed. The population variance is known, so use the z test. \\(H_0: \\mu = 16.0\\), and \\(H_a: \\mu &gt; 16.0\\) - a right-tail test. The test statistic is \\(Z = \\frac{\\bar{x} - \\mu_0}{se_\\mu}=\\) 1.97 where \\(se_{\\mu_0} = \\frac{\\mu_0}{\\sqrt{n}} =\\) 1.06. \\(P(z &gt; Z) =\\) 0.0244, so reject \\(H_0\\) at the \\(\\alpha =\\) 0.05 level of significance. The 95% confidence interval for \\(\\mu\\) is \\(\\bar{x} \\pm z_{(1 - \\alpha){/}2} se_\\mu\\) where \\(z_{(1 - \\alpha){/}2} =\\) 1.96. \\(\\mu =\\) 20.09 \\(\\pm\\) 2.08 (95% CI 18.01 to 22.17). "],["one-sample-mean-t-test.html", "4.2 One-Sample Mean t Test", " 4.2 One-Sample Mean t Test The one-sample t test applies when the sampling distribution of the population mean is normally distributed and there are no significant outliers. Unlike the z test, the population variance can be unknown. The sampling distribution is normally distributed when the underlying population is normally distributed, or when the sample size is large \\((n &gt;= 30)\\), as follows from the central limit theorem. Under the t test method, the measured mean, \\(\\bar{x}\\), approximates the population mean, \\(\\mu\\). The sample standard deviation, \\(s\\), estimates the unknown population standard deviation, \\(\\sigma\\). The resulting sampling distribution has a t distribution centered at \\(\\mu\\) with standard error \\(se_\\bar{x} = \\frac{s}{\\sqrt{n}}\\). Define a \\((1 - \\alpha)\\%\\) confidence interval as \\(\\bar{x} \\pm t_{(1 - \\alpha){/}2} se_\\bar{x}\\) and/or test \\(H_0: \\mu = \\mu_0\\) with test statistic \\(T = \\frac{\\bar{x} - \\mu_0}{se_\\bar{x}}\\). Example A researcher recruits a random sample of n = 40 people to participate in a study about depression intervention. The researcher measures the participants depression level prior to the study. The mean depression score (3.72 \\(\\pm\\) 0.74) was lower than the population normal depression score of 4.0. The null hypothesis is that the sample is representative of the overall population. Should you reject \\(H_0\\)? dep %&gt;% gtsummary::tbl_summary(statistic = list(all_continuous() ~ &quot;{mean} ({sd})&quot;)) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #gjlaqkjnpf .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #gjlaqkjnpf .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #gjlaqkjnpf .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #gjlaqkjnpf .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; border-top-color: #FFFFFF; border-top-width: 0; } #gjlaqkjnpf .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #gjlaqkjnpf .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #gjlaqkjnpf .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #gjlaqkjnpf .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #gjlaqkjnpf .gt_column_spanner_outer:first-child { padding-left: 0; } #gjlaqkjnpf .gt_column_spanner_outer:last-child { padding-right: 0; } #gjlaqkjnpf .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #gjlaqkjnpf .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #gjlaqkjnpf .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #gjlaqkjnpf .gt_from_md > :first-child { margin-top: 0; } #gjlaqkjnpf .gt_from_md > :last-child { margin-bottom: 0; } #gjlaqkjnpf .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #gjlaqkjnpf .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #gjlaqkjnpf .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #gjlaqkjnpf .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #gjlaqkjnpf .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #gjlaqkjnpf .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #gjlaqkjnpf .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #gjlaqkjnpf .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #gjlaqkjnpf .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #gjlaqkjnpf .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #gjlaqkjnpf .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #gjlaqkjnpf .gt_sourcenote { font-size: 90%; padding: 4px; } #gjlaqkjnpf .gt_left { text-align: left; } #gjlaqkjnpf .gt_center { text-align: center; } #gjlaqkjnpf .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #gjlaqkjnpf .gt_font_normal { font-weight: normal; } #gjlaqkjnpf .gt_font_bold { font-weight: bold; } #gjlaqkjnpf .gt_font_italic { font-style: italic; } #gjlaqkjnpf .gt_super { font-size: 65%; } #gjlaqkjnpf .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 65%; } Characteristic N = 401 dep_score 3.72 (0.74) 1 Mean (SD) Conditions The one-sample t test applies when the variable is continuous and the observations are independent. Additionally, there are two conditions related to the data distribution. If either condition fails, try the suggested work-arounds or use the non-parametric [Wilcoxon 1-Sample Median Test for Numeric Var] instead. Outliers. There should be no significant outliers. Outliers exert a large influence on the mean and standard deviation. Test with a box plot. If there are outliers, you might be able to drop them or transform the data. Normality. Values should be nearly normally distributed (nearly because the t-test is robust to the normality assumption). This condition is especially important with small sample sizes. Test with Q-Q plots or the Shapiro-Wilk test for normality. If the data is very non-normal, you might be able to transform the data. Outliers Assess outliers with a box plot. Box plot whiskers extend up to 1.5*IQR from the upper and lower hinges and outliers (beyond the whiskers) are are plotted individually. The boxplot shows no outliers. If the outliers might are data entry errors or measurement errors, fix them or discard them. If the outliers are genuine, you have a couple options before reverting to Wilcoxon. Transform the variable. Dont do this unless the variable is also non-normal. Transformation also has the downside of making interpretation more difficult. Leave it in if it doesnt affect the conclusion (compared to taking it out). Normality Assume the population is normally distributed if n \\(\\ge\\) 30. Otherwise, asses a Q-Q plot, skewness and kurtosis values, or a histogram. If you still dont feel confident about normality, run a [Shapiro-Wilk Test]. The data set has n = 40 observations, so you can assume normality. Here is a QQ plot anyway. The QQ plot indicates normality. dep %&gt;% ggplot(aes(sample = dep_score)) + stat_qq() + stat_qq_line(col = &quot;goldenrod&quot;) + theme_minimal() + labs(title = &quot;Normal Q-Q Plot&quot;) Here is the Shapiro-Wilk normality test. It fails to reject the null hypothesis of a normally distributed population. shapiro.test(dep$dep_score) ## ## Shapiro-Wilk normality test ## ## data: dep$dep_score ## W = 0.98446, p-value = 0.8474 If the data is not normally distributed, you still have a couple options before reverting to Wilcoxon. Transform the dependent variable. Carry on regardless - the one-sample t-test is fairly robust to deviations from normality. Results Conduct the t-test. To get a 95% CI around the difference (instead of around the estimate), run the test using the difference, \\(\\mu_0 - \\bar{x}\\), and leave mu at its default of 0. (dep_95ci &lt;- t.test(x = mu_0 - dep$dep_score, alternative = &quot;two.sided&quot;, conf.level = .95)) ## ## One Sample t-test ## ## data: mu_0 - dep$dep_score ## t = 2.3811, df = 39, p-value = 0.02224 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 0.04176615 0.51323385 ## sample estimates: ## mean of x ## 0.2775 The difference is statistically different from 0 at the p = .05 level. The effect size, called Cohens d, is defined as \\(d = |M_D| / s\\), where \\(|M_D| = \\bar{x} - \\mu_0\\), and \\(s\\) is the sample standard deviation. \\(d &lt;.2\\) is considered trivial, \\(.2 \\le d &lt; .5\\) small, and \\(.5 \\le d &lt; .8\\) large. (d &lt;- rstatix::cohens_d(dep, dep_score ~ 1, mu = 4) %&gt;% pull(effsize) %&gt;% abs()) ## Cohen&#39;s d ## 0.3764788 Cohens d is 0.38, a small effect. Make a habit of constructing a plot, just to make sure your head is on straight. Now you are ready to report the results. A one-sample t-test was run to determine whether depression score in recruited subjects was different from normal, as defined as a depression score of 4.0. Depression scores were normally distributed, as assessed by Shapiro-Wilks test (p &gt; .05) and there were no outliers in the data, as assessed by inspection of a boxplot. Data are mean \\(\\pm\\) standard deviation, unless otherwise stated. Mean depression score (3.72 \\(\\pm\\) 0.74) was lower than the population normal depression score of 4.00, a statistically significant difference of 0.28 (95% CI, 0.04 to 0.51), t(39) = 2.38, p = 0.022, d = 0.38. Appendix: Deciding Sample Size Determine the sample size required for a maximum error \\(\\epsilon\\) in the estimate by solving the confidence interval equation, \\(\\bar{x} \\pm t_{(1 - \\alpha){/}2} \\frac{s}{\\sqrt{n}}\\) for \\(n=\\frac{{t_{\\alpha/2,n-1}^2se^2}}{{\\epsilon^2}}\\) . Unfortunately, \\(t_{\\alpha/2,n-1}^2\\) is dependent on \\(n\\), so replace it with \\(z_{\\alpha/2}^2\\). What about \\(s^2\\)? Estimate it from the literature, a pilot study, or using the empirical rule that 95% of the range falls within two standard deviations, \\(s=range / 4\\). For example, if the maximum tolerable error is* \\(\\epsilon\\) = 3, and \\(s\\) is approximately 10, what sample size produces an \\(\\alpha\\) =0.05 confidence level? ceiling(qnorm(.975)^2 * 10^2 / 3^2) ## [1] 43 "],["one-sample-median-wilcoxon-test.html", "4.3 One-Sample Median Wilcoxon Test", " 4.3 One-Sample Median Wilcoxon Test The Wilcoxon one-sample median test (aka Wilcoxon signed rank test) is a non-parametric alternative to the t-test for cases when the the sampling distribution of the population mean is not normally distributed, but is at least symmetric. Under the Wilcoxon test, the measured median, \\(\\eta_x\\), approximates the population median, \\(\\eta\\). The method calculates the difference between each value and the hypothesized median, \\(\\eta_0\\), ranks the difference magnitudes, then sums the ranks for the negative and the positive differences, \\(W+\\) and \\(W-\\). The test compares the smaller of the two sums to a table of critical values. Here is a case study. A store claims their checkout wait times are \\(\\le\\) 4 minutes. You challenge the claim by sampling 6 checkout experiences. The mean wait time was 4.6, but the data may violate normality. data.frame(wait = wait) %&gt;% ggplot(aes(sample = wait)) + stat_qq() + stat_qq_line(col = &quot;goldenrod&quot;) + theme_minimal() + labs(title = &quot;Normal Q-Q Plot&quot;) Shapiro-Wilk rejects the null hypothesis of a normally distributed population. shapiro.test(wait) ## ## Shapiro-Wilk normality test ## ## data: wait ## W = 0.75105, p-value = 0.0204 Use the Wilcoxon test instead. (wt &lt;- wilcox.test(wait, mu = 4, alternative = &quot;greater&quot;)) ## Warning in wilcox.test.default(wait, mu = 4, alternative = &quot;greater&quot;): cannot ## compute exact p-value with ties ## ## Wilcoxon signed rank test with continuity correction ## ## data: wait ## V = 14.5, p-value = 0.2309 ## alternative hypothesis: true location is greater than 4 A Wilcoxon Signed-Ranks Test indicated that wait times were not statistically significantly higher than the 4-minute claim, z = 14.5, p = 0.231. "],["chi-squared-goodness-of-fit-test.html", "4.4 Chi-Squared Goodness-of-Fit Test", " 4.4 Chi-Squared Goodness-of-Fit Test Use the chi-squared goodness-of-fit test to test whether the observed frequency counts, \\(O_j\\), of the \\(J\\) levels of a categorical variable differ from the expected frequency counts, \\(E_j\\). \\(H_0\\) is \\(O_j = E_j\\). You can use this test for dichotomous, nominal, or ordinal variables. There are only two conditions to use this test: the observations are independent, meaning either random assignment or random sampling without replacement from &lt;10% of the population, and the expected frequency in each group is &gt;=5. The Pearson goodness-of-fit test statistic is \\[X^2 = \\sum \\frac{(O_j - E_j)^2}{E_j}\\] where \\(O_j = p_j n\\) and \\(E_j = \\pi_j n\\). The sampling distribution of \\(X^2\\) approaches the \\(\\chi_{J-1}^2\\) as the sample size \\(n \\rightarrow \\infty\\). The assumption that \\(X^2\\) is distributed \\(\\sim \\chi^2\\) is not quite correct, so you will see researchers subtract .5 from the differences to increase the p-value, the so-called Yates Continuity Correction. \\[X^2 = \\sum \\frac{(O_j - E_j - 0.5)^2}{E_j}\\] \\(X^2 \\rightarrow 0\\) as the saturated model (the observed data represent the fit of the saturated model, the most complex model possible with the data) proportions approach the expected proportions, \\(p_j \\rightarrow \\pi_j\\). The chi-squared test calculates the probability of the occurrence of \\(X^2\\) at least as extreme given that it is a chi-squared random variable with degrees of freedom equal to the number of levels of the variable minus one, \\(J-1\\). Example with Theoretical Values A researcher crosses tall cut-leaf tomatoes with dwarf potato-leaf tomatoes, then classifies the n = 1,611 offsprings phenotype. The four phenotypes should occur with relative frequencies 9:3:3:1. The observed frequencies constitute a one-way table. If you only care about one level (or if the variable is binary) of if, conduct a one-proportion Z-test or an exact binomial test. Otherwise, conduct an exact multinomial test (recommended when n &lt;= 1,000), Pearsons chi-squared goodness-of-fit test, or a G-test. Conditions This is a randomized experiment. The minimum expected frequency was 100, so the chi-squared test of independence is valid. Had the data violated the \\(\\ge\\) 5 condition, you could run an exact test (like the binomial, or in this case, the multinomial), or lump some factor levels together. Results You can calculate \\(X^2\\) by hand, and find the probability of a test statistic at least as extreme using the \\(\\chi^2\\) distribution with 4-1 = 3 degrees of freedom. (pheno_x2 &lt;- sum((pheno_obs - pheno_exp)^2 / pheno_exp)) ## [1] 9.54652 (pheno_p &lt;- pchisq(q = pheno_x2, df = length(pheno_type) - 1, lower.tail = FALSE)) ## [1] 0.02284158 That is what chisq.test() does. The function applies the Yates continuity correction by default, so I had to specify correct = FALSE to exclude it. In this case, setting it to TRUE has almost no effect because the sample size is large. (pheno_chisq_test &lt;- chisq.test(pheno_obs, p = pheno_pi, correct = FALSE)) ## ## Chi-squared test for given probabilities ## ## data: pheno_obs ## X-squared = 9.5465, df = 3, p-value = 0.02284 As always, plot the distribution. At this point you can report, Of the 1,611 offspring produced from the cross-fertiliation, 956 were tall cut-leaf, 258 were tall potato-leaf, 293 where dwarf cut-leaf, and 104 were dwarf potato-leaf. A chi-square goodness-of-fit test was conducted to determine whether the offspring had the same proportion of phenotypes as the theoretical distribution. The minimum expected frequency was 101. The chi-square goodness-of-fit test indicated that the number of tall cut-leaf, tall potato-leaf, dwarf cut-leaf, and dwarf potato-leaf offspring was statistically significantly different from the proportions expected in the theoretical distribution (\\(X^2\\)(3) = 9.547, p = 0.023). If you reject \\(H_0\\), inspect the residuals to learn which differences contribute most to the rejection. Notice how \\(X^2\\) is a sum of squared standardized cell differences, or Pearson residuals, \\[r_i = \\frac{o_j - e_j}{\\sqrt{e_j}}\\] Cells with the largest \\(|r|\\) contribute the most to the total \\(X^2\\). pheno_chisq_test$residuals^2 / pheno_chisq_test$statistic ## tall cut-leaf tall potato-leaf dwarf cut-leaf dwarf potato-leaf ## 0.28682269 0.67328098 0.02848093 0.01141540 The two tall cells contributed over 95% of the \\(X^2\\) test statistic, with the tall potato-leaf accounting for 67%. This aligns with what youd expect from the bar plot. Example with Theoretical Distribution You need to reduce the degrees of freedom (df) in the chi-squared goodness-of-fit test by 1 if you test whether the data conform to a particular distribution instead of a set of theoretical values. j &lt;- c(0:5) o &lt;- c(19, 26, 29, 13, 10, 3) childr_n &lt;- as.character(0:5) Suppose you sample n = 100 families and count the number of children. The count of children is a Poisson random variable, \\(J\\), with maximum likelihood estimate \\(\\hat{\\lambda} = \\sum{j_i O_i} / \\sum{O_i}\\). Test whether the observed values can be described as samples from a Poisson random variable. The probabilities for each possible count are \\[f(j; \\lambda) = \\frac{e^{-\\hat{\\lambda}} \\hat{\\lambda}^j}{j!}.\\] Conditions This is random sampling. The minimum expected frequency was 2, so the data violates the \\(\\ge\\) 5 rule. Lump the last two categories into 4-5. The minimum expected frequency was 6, so now the chi-squared test of independence is valid. Results Compare the expected values to the observed values with the chi-squared goodness of fit test, but in this case \\(df = 5 - 1 - 1\\) because the estimated parameter \\(\\lambda\\) reduces df by 1. You cannot set df in chisq.test(), so perform the test manually. (X2 &lt;- sum((o - e)^2 / e)) ## [1] 7.092968 (p.value &lt;- pchisq(q = X2, df = length(j) - 1 - 1, lower.tail = FALSE)) ## [1] 0.06899286 At this point you can report, Of the 100 families sampled, 19 had no children, 26 had one child, 29 had two children, 13 had three children, and 13 had 4 or 5 children. A chi-square goodness-of-fit test was conducted to determine whether the observed family sizes follow a Poisson distribution. The minimum expected frequency was 13. The chi-square goodness-of-fit test indicated that the number of children was not statistically significantly different from the proportions expected in the Poisson distribution (\\(X^2\\)(3) = 7.093, p = 0.069). "],["g-test.html", "4.5 G-Test", " 4.5 G-Test The G-test is a likelihood-ratio statistical significance test increasingly used instead of chi-squared tests. The test statistic is defined \\[G^2 = 2 \\sum O_j \\log \\left[ \\frac{O_j}{E_j} \\right]\\] where the 2 multiplier asymptotically aligns with the chi-squared test formula. G is distributed \\(\\sim \\chi^2\\), with the same number of degrees of freedom as in the corresponding chi-squared test. In fact, the chi-squared test statistic is a second order Taylor expansion of the natural logarithm around 1. Returning to the phenotype case study in the chi-squared goodness-of-fit test section, you can calculate the \\(G^2\\) test statistic and probability by hand. (pheno_g2 &lt;- 2 * sum(pheno_obs * log(pheno_obs / pheno_exp))) ## [1] 9.836806 (pchisq(q = pheno_g2, df = length(pheno_type) - 1, lower.tail = FALSE)) ## [1] 0.02000552 This is pretty close to the \\(X^2\\) = 9.547, p = 0.023 using the chi-squared goodness-of-fit test. The DescTools::GTest() function to conducts a G-test. DescTools::GTest(pheno_obs, p = pheno_pi) ## ## Log likelihood ratio (G-test) goodness of fit test ## ## data: pheno_obs ## G = 9.8368, X-squared df = 3, p-value = 0.02001 According to the function documentation, the G-test is not usually used for 2x2 tables. EMT::multinomial.test(o, f, useChisq = TRUE) ## ## Exact Multinomial Test, distance measure: chisquare ## ## Events chi2Obs p.value ## 4598126 7.093 0.1479 chisq.test(o, e) ## Warning in chisq.test(o, e): Chi-squared approximation may be incorrect ## ## Pearson&#39;s Chi-squared test ## ## data: o and e ## X-squared = 15, df = 12, p-value = 0.2414 "],["one-sample-poisson-test.html", "4.6 One-Sample Poisson Test", " 4.6 One-Sample Poisson Test If \\(X\\) is the number of successes in \\(n\\) (many) trials when the probability of success \\(\\lambda / n\\) is small, then \\(X\\) is a random variable with a Poisson distribution, and the probability of observing \\(X = x\\) successes is \\[f(x;\\lambda) = \\frac{e^{-\\lambda} \\lambda^x}{x!} \\hspace{1cm} x \\in (0, 1, ...), \\hspace{2mm} \\lambda &gt; 0\\] with \\(E(X)=\\lambda\\) and \\(Var(X) = \\lambda\\) where \\(\\lambda\\) is estimated by the sample \\(\\hat{\\lambda}\\), \\[\\hat{\\lambda} = \\sum_{i=1}^N x_i / n.\\] Poisson sampling is used to model counts of events that occur randomly over a fixed period of time. You can use the Poisson distribution to perform an exact test on a Poisson random variable. Example You are analyzing goal totals from a sample consisting of the 95 matches in the first round of the 2002 World Cup. The average match produced a mean/sd of 1.38 \\(\\pm\\) 1.28 goals, lower than the 1.5 historical average. Should you reject the null hypothesis that the sample is representative of typical values? Conditions The events must be independent of each other. In this case, the goal-count in one match has no effect on goal-counts in other matches. The expected value of each event must be the same (homogeneity). In this case, the expected goal-count of each match is the same regardless of which teams are playing. This assumption is often dubious, causing the distribution variance to be larger than the mean, a conditional called over-dispersion. You might also check whether the data is consistent with a Poisson model. This is random sampling, but the data violates the \\(\\ge\\) 5 rule because the minimum expected frequency was 0. To comply with the minimum frequency rule, lump the last six categories into 3-8. The minimum expected frequency was 15, so now the chi-squared test of independence is valid. Compare the expected values to the observed values with the chi-squared goodness of fit test, but in this case \\(df = 4 - 1 - 1\\) because the estimated parameter \\(\\lambda\\) reduces the df by 1. You cannot set df in chisq.test(), so perform the test manually. (X2 &lt;- sum((o - e)^2 / e)) ## [1] 0.8618219 (p.value &lt;- pchisq(q = X2, df = length(j) - 1 - 1, lower.tail = FALSE)) ## [1] 0.6499168 Of the 95 World Cup matches, 23 had no goals, 37 had one goal, 20 had two goals, and 15 had 3-8 goals. A chi-square goodness-of-fit test was conducted to determine whether the observed goal counts follow a Poisson distribution. The minimum expected frequency was 15. The chi-square goodness-of-fit test indicated that the number of goals scored was not statistically significantly different from the frequencies expected from a Poisson distribution (\\(X^2\\)(2) = 0.862, p = 0.650). Results The conditions for the exact Poisson test were met, so go ahead and run the test. (pois_val &lt;- poisson.test( x = sum(dat_pois$goals * dat_pois$freq), T = sum(dat_pois$freq), r = 1.5) ) ## ## Exact Poisson test ## ## data: sum(dat_pois$goals * dat_pois$freq) time base: sum(dat_pois$freq) ## number of events = 131, time base = 95, p-value = 0.3567 ## alternative hypothesis: true event rate is not equal to 1.5 ## 95 percent confidence interval: ## 1.152935 1.636315 ## sample estimates: ## event rate ## 1.378947 Construct a plot showing the 95% CI around the hypothesized value. For a Poisson distribution, I built the distribution around the expected value, \\(n\\lambda\\), not the rate, \\(\\lambda\\). I think you could report these results like this. A one-sample exact Poisson test was run to determine whether the number of goals scored in the first round of the 2002 World Cup was different from past World Cups, 1.5. A chi-square goodness-of-fit test indicated that the number of goals was not statistically significantly different from the counts expected in the Poisson distribution (\\(X^2\\)(2) = 0.862, p = 0.650). Data are mean \\(\\pm\\) standard deviation, unless otherwise stated. Mean goals scored (1.38 \\(\\pm\\) 1.28) was lower than the historical mean of 1.50, but was not statistically significantly different (95% CI, 1.15 to 1.64), p = 0.357. "],["exact-binomial-test.html", "4.7 Exact Binomial Test", " 4.7 Exact Binomial Test The Clopper-Pearson exact binomial test is precise, but theoretically complicated in that it inverts two single-tailed binomial tests (No theory here - Ill just rely on the software). Use the exact binomial test if you have a small sample size or an extreme success/failure probability that invalidates the chi-square and G tests. The exact binomial also applies when you have a one-tail test. The exact binomial test has two conditions: independence, and at least \\(n\\pi \\ge 5\\) successes or \\(n(1\\pi)\\ge 5\\) failures. You can use this test for multinomial variables too, but the test only compares a single levels proportion to a hypothesized value. Example A pharmaceutical company claims its drug reduces fever in &gt;60% of cases. In a random sample of n = 40 cases the drug reduces fever in 20 cases. Do you reject the claim? You are testing \\(P(x \\le 20)\\) in n = 40 trials when p = 60%, a one-tail test. The sample is a random assignment experiment with 20&gt;5 successes and 20&gt;5 failures, so it meets the conditions for the exact binomial test. binom.test(20, 40, p = 0.6, alternative = &quot;greater&quot;) ## ## Exact binomial test ## ## data: 20 and 40 ## number of successes = 20, number of trials = 40, p-value = 0.9256 ## alternative hypothesis: true probability of success is greater than 0.6 ## 95 percent confidence interval: ## 0.3610917 1.0000000 ## sample estimates: ## probability of success ## 0.5 The exact binomial test uses the method of small p-values, in which the probability of observing a proportion \\(p\\) as far or further from \\(\\pi_0\\) is the sum of all \\(P(X=p_i)\\) where \\(p_i &lt;= p\\). map_dbl(dbinom(0:20, 40, 0.6), ~if_else(. &lt;= 0.5, ., 0)) %&gt;% sum() ## [1] 0.1297657 That is what pbinom() does. pbinom(q = 20, size = 40, p = 0.6, lower.tail = TRUE) ## [1] 0.1297657 A 95% confidence interval means 95% of confidence intervals constructed from a random sample of the population will contain the true population proportion. There are several methods to calculate a binomial confidence interval4 binom.test() uses the Clopper-Pearson interval. This method calculates lower (\\(P_L\\)) and upper (\\(P_U\\)) limits that satisfy \\[ \\begin{eqnarray} \\sum_{x=n_1}^n \\binom{n}{x} p_L^x(1 - p_L)^{n-x} &amp;=&amp; \\alpha/2\\\\ \\sum_{x=0}^{n_1} \\binom{n}{x} p_U^x(1 - p_U)^{n-x} &amp;=&amp; \\alpha/2 \\end{eqnarray} \\] where \\(n_i\\) is the measured successes in \\(n\\) trials. For a one-tail test, the confidence interval is calculated with the right side equaling 0 and \\(/alpha\\) instead of \\(\\alpha/2\\). A right-tailed 95% confidence interval means 95% of confidence intervals will contain a lower limit that is less than the true population proportion. If you wanted to construct a confidence interval around the population proportion, use a two-sided test. binom.test(20, 40, p = 0.6, alternative = &quot;two.sided&quot;) ## ## Exact binomial test ## ## data: 20 and 40 ## number of successes = 20, number of trials = 40, p-value = 0.2007 ## alternative hypothesis: true probability of success is not equal to 0.6 ## 95 percent confidence interval: ## 0.3380178 0.6619822 ## sample estimates: ## probability of success ## 0.5 If you just wanted to know whether 20 successes in 40 trials is compatible with a population proportion of 60%, then you could use the chi-squared goodness-of-fit test. chisq.test(x = c(20, 20), p = c(0.6, 0.4), correct = FALSE) ## ## Chi-squared test for given probabilities ## ## data: c(20, 20) ## X-squared = 1.6667, df = 1, p-value = 0.1967 Wikipedia. "],["one-sample-proportion-z-test.html", "4.8 One-Sample Proportion z Test", " 4.8 One-Sample Proportion z Test The z-test uses the sample proportion of group \\(j\\), \\(p_j\\), as an estimate of the population proportion \\(\\pi_j\\) to evaluate an hypothesized population proportion \\(\\pi_{0j}\\) and/or construct a \\((1\\alpha)\\%\\) confidence interval around \\(p_j\\) to estimate \\(\\pi_j\\) within a margin of error \\(\\epsilon\\). The z-test is intuitive to learn, but it only applies when the central limit theorem conditions hold: the sample is independently drawn, meaning random assignment (experiments), or random sampling without replacement from &lt;10% of the population (observational studies), there are at least 5 successes and 5 failures, the sample size is &gt;=30, and the expected probability of success is not extreme, between 0.2 and 0.8. If these conditions hold, the sampling distribution of \\(\\pi\\) is normally distributed around \\(p\\) with standard error \\(se_p = \\frac{s_p}{\\sqrt{n}} = \\frac{\\sqrt{p(1p)}}{\\sqrt{n}}\\). The measured values \\(p\\) and \\(s_p\\) approximate the population values \\(\\pi\\) and \\(\\sigma_\\pi\\). You can define a \\((1  \\alpha)\\%\\) confidence interval as \\(p \\pm z_{\\alpha / 2}se_p\\). Test the hypothesis of \\(\\pi = \\pi_0\\) with test statistic \\(z = \\frac{p  \\pi_0}{se_{\\pi_0}}\\) where \\(se_{\\pi_0} = \\frac{s_{\\pi_0}}{\\sqrt{n}} = \\frac{\\sqrt{{\\pi_0}(1{\\pi_0})}}{\\sqrt{n}}\\). Example A machine is supposed to randomly churn out prizes in 60% of boxes. In a random sample of n = 40 boxes there are prizes in 20 boxes. Is the machine flawed? prop.test(20, 40, 0.6, &quot;two.sided&quot;, correct = FALSE) ## ## 1-sample proportions test without continuity correction ## ## data: 20 out of 40, null probability 0.6 ## X-squared = 1.6667, df = 1, p-value = 0.1967 ## alternative hypothesis: true p is not equal to 0.6 ## 95 percent confidence interval: ## 0.3519953 0.6480047 ## sample estimates: ## p ## 0.5 The first thing youll notice is that prop.test() performs a chi-squared goodness-of-fit test, not a one-proportion Z-test! chisq.test(c(20, 40-20), p = c(.6, .4), correct = FALSE) ## ## Chi-squared test for given probabilities ## ## data: c(20, 40 - 20) ## X-squared = 1.6667, df = 1, p-value = 0.1967 It turns out \\(P(\\chi^2 &gt; X^2)\\) equals \\(2 \\cdot P(Z &gt; z).\\) Here is the manual calculation of the chi-squared test statistic \\(X^2\\) and resulting p-value on 1 dof. pi_0 &lt;- .6 p &lt;- 20 / 40 observed &lt;- c(p, 1-p) * 40 expected &lt;- c(pi_0, 1-pi_0) * 40 X2 &lt;- sum((observed - expected)^2 / expected) pchisq(X2, 1, lower.tail = FALSE) ## [1] 0.1967056 And here is the manual calculation of the Z-test statistic \\(z\\) and resulting p-value. se &lt;- sqrt(pi_0*(1-pi_0)) / sqrt(40) z &lt;- (p - pi_0) / se pnorm(z, lower.tail = TRUE) * 2 ## [1] 0.1967056 The 95% CI presented by prop.test() is also not the \\(p \\pm z_{\\alpha / 2}se_p\\) Wald interval; it is the Wilson interval! DescTools::BinomCI(20, 40, method = &quot;wilson&quot;) ## est lwr.ci upr.ci ## [1,] 0.5 0.3519953 0.6480047 There are a lot of methods (see ?DescTools::BinomCI), and Wilson is the one Agresti-Coull recommends. If you want Wald, use DescTools::BinomCI() with method = \"wald\". DescTools::BinomCI(20, 40, method = &quot;wald&quot;) ## est lwr.ci upr.ci ## [1,] 0.5 0.3450512 0.6549488 This matches the manual calculation below. z_crit = qnorm(1 - .05/2) se &lt;- sqrt(p*(1-p)) / sqrt(40) (CI &lt;- c(p - z_crit*se, p + z_crit*se)) ## [1] 0.3450512 0.6549488 prop.test() (and chissq.test()) reported a p-value of 0.1967056, so you cannot reject the null hypothesis that \\(\\pi = 0.6\\). Its good practice to plot this out to make sure your head is on straight. Incidentally, if you have a margin of error requirement, you can back into the required sample size to achieve it. Just solve the margin of error equation \\(\\epsilon = z_{\\alpha/2}^2 = \\sqrt{\\frac{\\pi_0(1-\\pi_0)}{n}}\\) for \\(n = \\frac{z_{\\alpha/2}^2 \\pi_0(1-\\pi_0)}{\\epsilon^2}.\\) "],["sample-t-test-for-categorical-var.html", "4.9 1 sample t Test for Categorical Var", " 4.9 1 sample t Test for Categorical Var This test applies when you do not know the population variance. "],["wilcoxon-1-sample-median-test-for-categorical-var.html", "4.10 Wilcoxon 1-Sample Median Test for Categorical Var", " 4.10 Wilcoxon 1-Sample Median Test for Categorical Var This test applies when the variable is not normally distributed. "],["two-group-differences.html", "Chapter 5 Two-Group Differences", " Chapter 5 Two-Group Differences Use independent samples tests to either describe a variables frequency or central tendency difference between two independent groups, or to compare the difference to a hypothesized value. If the data generating process produces continuous outcomes (interval or ratio) and the outcomes are symmetrically distributed, the difference in the sample means, \\(\\hat{d} = \\bar{x} - \\bar{y}\\), is a random variable centered at the population difference, \\(d = \\mu_X - \\mu_Y\\). You can use a theoretical distribution (normal or student t) to estimate a 95% confidence interval (CI) around \\(d\\), or compare \\(\\hat{d}\\) to an hypothesized population difference, \\(d_0\\). If you (somehow) know the sampling distribution variances \\(\\sigma^2_X\\) and \\(\\sigma^2_Y\\), or the Central Limit Theorem (CLT) conditions hold, you can assume the random variable is normally distributed and use the z-test, otherwise assume the random variable has a student t distribution and use the t-test.5 If the data generating process produces continuous outcomes that are not symmetrically distributed, use a non-parametric test like the Mann-Whitney U test. If the data generating process produces discrete outcomes (counts), the sample count, \\(x\\), is a random variable from a Poisson, binomial, normal, or multinomial distribution, or a random variable from a theoretical outcome. For two independent samples, the data can be organized into a two-way table - a frequency table for two categorical variables. If you have a single categorical predictor variable, you can test whether the joint frequency counts differ from the expected frequency counts in the saturated model. You analyze a two-way table one of two ways. If you only care about comparing two levels (like when the response variable is binary), conduct a proportion difference z-test or a Fisher exact-test. If you want to compare the joint frequency counts to expected frequency counts under the independence model (the model of independent explanatory variables), conduct a Pearsons chi-squared independence test, or a G-test. The t-test returns nearly the same result as the z-test when the CLT holds, so in practice no one bothers with the z-test except as an aid to teach the t-test. "],["independent-samples-continuous.html", "5.1 Independent Samples (Continuous)", " 5.1 Independent Samples (Continuous) Independent Samples t-Test If a population measure X is normally distributed with mean \\(\\mu_X\\) and variance \\(\\sigma_X^2\\), and a population measure Y is normally distributed with mean \\(\\mu_Y\\) and variance \\(\\sigma_Y^2\\), then their difference is normally distributed with mean \\(d = \\mu_X - \\mu_Y\\) and variance \\(\\sigma_{XY}^2 = \\sigma_X^2 + \\sigma_Y^2\\). By the CLT, as the sample sizes grow, a non-normally distributed X and Y will approach normality, and so will their difference. The independent samples t-test evaluates an hypothesized difference, \\(d_0\\) (H0: \\(d = d_0\\)), from the difference in sample means \\(\\hat{d} = \\bar{x} - \\bar{y}\\), or constructs a (1 - \\(\\alpha\\))% confidence interval around \\(\\hat{d}\\) to estimate \\(d\\) within a margin of error, \\(\\epsilon\\). In principal, you can evaluate \\(\\hat{d}\\) with either a z-test or a t-test. Both require independent samples and approximately normal sampling distributions. Sampling distributions are normal if the underlying populations are normally distributed, or if the sample sizes are large (\\(n_X\\) and \\(n_Y\\) \\(\\ge\\) 30). However, the z-test additionally requires known sampling distribution variances, \\(\\sigma^2_X\\) and \\(\\sigma^2_Y\\). These variances are never known, so always use the t-test. The z-test assumes \\(d\\) is normally distributed around \\(\\hat{d} = d\\) with standard error \\(SE = \\sqrt{\\frac{\\sigma_X^2}{n_X} + \\frac{\\sigma_Y^2}{n_Y}}.\\) The test statistic for H0: \\(d = d_0\\) is \\(Z = \\frac{\\hat{d} - d_0}{SE}\\). The (1 - \\(\\alpha\\))% CI is \\(d = \\hat{d} \\pm z_{(1 - \\alpha {/} 2)} SE\\). The t-test assumes \\(d\\) has a t-distribution around \\(\\hat{d} = d\\) with standard error \\(SE = \\sqrt{\\frac{s_X^2}{n_X} + \\frac{s_Y^2}{n_Y}}.\\) The test statistic for H0: \\(d = d_0\\) is \\(T = \\frac{\\hat{d} - d_0}{SE}\\). The (1 - \\(\\alpha\\))% CI iss \\(d = \\hat{d} \\pm t_{(1 - \\alpha / 2), (n_X + n_Y - 2)} SE\\). There is a complication with the t-test SE and degrees of freedom. If the sample sizes are small and the standard deviations from each population are similar (the ratios of \\(s_X\\) and \\(s_Y\\) are &lt;2), pool the variances, \\(s_p^2 = \\frac{(n_X - 1) s_X^2 + (n_Y-1) s_Y^2}{n_X + n_Y-2}\\), so that \\(SE = s_p \\sqrt{\\frac{1}{n_X} + \\frac{1}{n_Y}}\\) and the degrees of freedom (df) = \\(n_X + n_Y - 2\\) (the pooled variances t-test). Otherwise, \\(SE = \\sqrt{\\frac{s_X^2}{n_X} + \\frac{s_Y^2}{n_Y}}\\), but you reduce df using the Welch-Satterthwaite correction, \\(df = \\frac{\\left(\\frac{s_X^2}{n_X} + \\frac{s_Y^2}{n_Y}\\right)^2}{\\frac{s_X^4}{n_X^2\\left(N_X-1\\right)} + \\frac{s_Y^4}{n_Y^2\\left(N_Y-1\\right)}}\\) (the separate variance t-test, or Welchs t-test). Wilcoxon Rank Sum Test The Mann-Whitney U test6, is a nonparametric alternative to the independent-samples t-test. Use the the test when the samples are not normally distributed or when the response variables are ordinal rather continuous. In the first case where the normality assumption fails, the test evaluates H0 that the two samples are from the same population distribution. In the second case where the response variables are ordinal, the test evaluates the difference in medians. The Wilcoxon Rank Sum test ranks the response values, then sums the ranks for the reference group, \\(W = \\sum R_1\\). The test statistic is \\(U = W - \\frac{n_2(n_2 + 1)}{2}\\) where \\(n_2\\) is the number of observations in the test group. \\(U\\) will equal 0 if there is complete separation between the groups, and \\(n_1 n_2\\) if there is complete overlap. Reject H0 if \\(U\\) is sufficiently small. Case Study A company shows an advertisement to \\(n_M\\) = 20 males and \\(n_F\\) = 20 females, then measures their engagement with a survey. Do the groups mean engagement scores differ? Laerd has two data sets for this example. One meets the conditions for a t-test, and the other fails the normality test, forcing you to use the Mann-Whitney U test. The t-test data set has the following summary statistics. (ind_num$t_gt &lt;- ind_num$t_dat %&gt;% gtsummary::tbl_summary( by = c(gender), statistic = list(all_continuous() ~ &quot;{mean} ({sd})&quot;) )) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #yvpkngabka .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #yvpkngabka .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #yvpkngabka .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #yvpkngabka .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; border-top-color: #FFFFFF; border-top-width: 0; } #yvpkngabka .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #yvpkngabka .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #yvpkngabka .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #yvpkngabka .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #yvpkngabka .gt_column_spanner_outer:first-child { padding-left: 0; } #yvpkngabka .gt_column_spanner_outer:last-child { padding-right: 0; } #yvpkngabka .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #yvpkngabka .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #yvpkngabka .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #yvpkngabka .gt_from_md > :first-child { margin-top: 0; } #yvpkngabka .gt_from_md > :last-child { margin-bottom: 0; } #yvpkngabka .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #yvpkngabka .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #yvpkngabka .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #yvpkngabka .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #yvpkngabka .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #yvpkngabka .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #yvpkngabka .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #yvpkngabka .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #yvpkngabka .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #yvpkngabka .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #yvpkngabka .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #yvpkngabka .gt_sourcenote { font-size: 90%; padding: 4px; } #yvpkngabka .gt_left { text-align: left; } #yvpkngabka .gt_center { text-align: center; } #yvpkngabka .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #yvpkngabka .gt_font_normal { font-weight: normal; } #yvpkngabka .gt_font_bold { font-weight: bold; } #yvpkngabka .gt_font_italic { font-style: italic; } #yvpkngabka .gt_super { font-size: 65%; } #yvpkngabka .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 65%; } Characteristic Male, N = 201 Female, N = 201 engagement 5.56 (0.29) 5.30 (0.39) 1 Mean (SD) There were 20 male and 20 female participants. Data are mean \\(\\pm\\) standard deviation, unless otherwise stated. The advertisement was more engaging to male viewers, 5.56 (0.29), than female viewers, 5.30 (0.39). The Mann-Whitney data set has the following summary statistics. (ind_num$mw_gt &lt;- ind_num$mw_dat %&gt;% gtsummary::tbl_summary( by = c(gender), statistic = list(all_continuous() ~ &quot;{mean} ({sd})&quot;) )) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #ceeirqtwyx .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #ceeirqtwyx .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #ceeirqtwyx .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #ceeirqtwyx .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; border-top-color: #FFFFFF; border-top-width: 0; } #ceeirqtwyx .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #ceeirqtwyx .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #ceeirqtwyx .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #ceeirqtwyx .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #ceeirqtwyx .gt_column_spanner_outer:first-child { padding-left: 0; } #ceeirqtwyx .gt_column_spanner_outer:last-child { padding-right: 0; } #ceeirqtwyx .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #ceeirqtwyx .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #ceeirqtwyx .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #ceeirqtwyx .gt_from_md > :first-child { margin-top: 0; } #ceeirqtwyx .gt_from_md > :last-child { margin-bottom: 0; } #ceeirqtwyx .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #ceeirqtwyx .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #ceeirqtwyx .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #ceeirqtwyx .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #ceeirqtwyx .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #ceeirqtwyx .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #ceeirqtwyx .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #ceeirqtwyx .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #ceeirqtwyx .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #ceeirqtwyx .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #ceeirqtwyx .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #ceeirqtwyx .gt_sourcenote { font-size: 90%; padding: 4px; } #ceeirqtwyx .gt_left { text-align: left; } #ceeirqtwyx .gt_center { text-align: center; } #ceeirqtwyx .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #ceeirqtwyx .gt_font_normal { font-weight: normal; } #ceeirqtwyx .gt_font_bold { font-weight: bold; } #ceeirqtwyx .gt_font_italic { font-style: italic; } #ceeirqtwyx .gt_super { font-size: 65%; } #ceeirqtwyx .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 65%; } Characteristic Male, N = 201 Female, N = 201 engagement 5.56 (0.35) 5.43 (0.53) 1 Mean (SD) There were 20 male and 20 female participants. Data are mean \\(\\pm\\) standard deviation, unless otherwise stated. The advertisement was more engaging to male viewers, 5.56 (0.35), than female viewers, 5.43 (0.53). Conditions The independent samples t-test and Mann-Whitney U test apply when 1) the response variable is continuous, 2) the independent variable is binomial, and 3) the observations are independent. The decision between the t-test and Mann-Whitney stems from two additional conditions related to the data distribution - if both conditions hold, use the t-test; otherwise use Mann-Whitney. Outliers. There should be no outliers in either group. Outliers exert a large influence on the mean and standard deviation. Test with a box plot. If there are outliers, you might be able to drop them or transform the data. Normality. Values should be nearly normally distributed. The t-test is robust to normality, but this condition is important with small sample sizes. Test with Q-Q plots or the Shapiro-Wilk test for normality. If the data is very non-normal, you might be able to transform the data. If the data passes the two conditions, use the t-test, but now you need to check a third condition related to the variances to determine which flavor of the t-test to use. Homogeneous Variances. Use pooled-variances if the variances are homogeneous; otherwise use the separate variances method. Test with Levenes test of equality of variances. If the data does not pass the first two conditions, use Mann-Whitney, but now you need to check a third condition here as well. The condition does not affect how to perform the test, but rather how to interpret the results. Distribution shape. If the distributions have the same shape, interpret the Mann-Whitney result as a comparison of the medians; otherwise interpret the result as a comparison of the mean ranks. Checking for Outliers Assess outliers with a box plot. Box plot whiskers extend up to 1.5*IQR from the upper and lower hinges and outliers (beyond the whiskers) are are plotted individually. For the t test data set, There were no outliers in the data, as assessed by inspection of a boxplot. and for the Mann-Whitney data set, There was one outlier in the data, as assessed by inspection of a boxplot. If the outliers are data entry errors or measurement errors, fix or discard them. If the outliers are genuine, you have a couple options before reverting to the Mann-Whitney U test. Leave it in if it doesnt affect the conclusion (compared to taking it out). Transform the variable. Dont do this unless the variable is also non-normal. Transformation also has the downside of making interpretation more difficult. Checking for Normality Assume the population is normally distributed if n \\(\\ge\\) 30. Otherwise, assess a Q-Q plot, skewness and kurtosis values, or a histogram. If you still dont feel confident about normality, run a Shapiro-Wilk test. There are only \\(n_M\\) = 20 male and \\(n_F\\) = 20 female observations, so you need to test normality. The QQ plot indicates normality in the t-test data set, but not in the Mann-Whitney data set. bind_rows( `t-test` = ind_num$t_dat, `Mann-Whitney` = ind_num$mw_dat, .id = &quot;set&quot; ) %&gt;% ggplot(aes(sample = engagement, group = gender, color = fct_rev(gender))) + stat_qq() + stat_qq_line(col = &quot;goldenrod&quot;) + theme_minimal() + theme(legend.position = &quot;top&quot;) + facet_wrap(~fct_rev(set)) + labs(title = &quot;Normal Q-Q Plot&quot;, color = NULL) Run Shapiro-Wilk separately for the males and for the females. Since we are looking at two data sets in tandem, there are four tests below. For the t-test data set, (ind_num$t_shapiro &lt;- split(ind_num$t_dat, ind_num$t_dat$gender) %&gt;% map(~shapiro.test(.$engagement)) ) ## $Male ## ## Shapiro-Wilk normality test ## ## data: .$engagement ## W = 0.98344, p-value = 0.9705 ## ## ## $Female ## ## Shapiro-Wilk normality test ## ## data: .$engagement ## W = 0.96078, p-value = 0.5595 Engagement scores for each level of gender were normally distributed, as assessed by Shapiro-Wilks test (p &gt; .05). For the Mann-Whitney data set, (ind_num$mw_shapiro &lt;- split(ind_num$mw_dat, ind_num$mw_dat$gender) %&gt;% map(~shapiro.test(.$engagement)) ) ## $Male ## ## Shapiro-Wilk normality test ## ## data: .$engagement ## W = 0.98807, p-value = 0.9946 ## ## ## $Female ## ## Shapiro-Wilk normality test ## ## data: .$engagement ## W = 0.8354, p-value = 0.003064 Engagement scores for each level of gender were not normally distributed for the Female sample, as assessed by Shapiro-Wilks test (p = 0.003). If the data is not normally distributed, you still have a couple options before reverting to the Mann-Whitney U test. Transform the dependent variable. Carry on regardless - the independent samples t-test is fairly robust to deviations from normality. Checking for Homogenous Variances If the data passed the outliers and normality tests, you will use the t-test, so now you need to test the variances to see which version (pooled-variances method if variances are homogeneous; separate variances if variances are heterogeneous). A rule of thumb is that homogeneous variances have a ratio of standard deviations between 0.5 and 2.0: sd(ind_num$t_dat %&gt;% filter(gender == &quot;Male&quot;) %&gt;% pull(engagement)) / sd(ind_num$t_dat %&gt;% filter(gender == &quot;Female&quot;) %&gt;% pull(engagement)) ## [1] 0.7419967 You can also use the F test to compare the ratio of the sample variances \\(\\hat{r} = s_X^2 / s_Y^2\\) to an hypothesized ratio of population variances \\(r_0 = \\sigma_X^2 / \\sigma_Y^2 = 1.\\) var.test(ind_num$t_dat %&gt;% filter(gender == &quot;Female&quot;) %&gt;% pull(engagement), ind_num$t_dat %&gt;% filter(gender == &quot;Male&quot;) %&gt;% pull(engagement)) ## ## F test to compare two variances ## ## data: ind_num$t_dat %&gt;% filter(gender == &quot;Female&quot;) %&gt;% pull(engagement) and ind_num$t_dat %&gt;% filter(gender == &quot;Male&quot;) %&gt;% pull(engagement) ## F = 1.8163, num df = 19, denom df = 19, p-value = 0.2025 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.7189277 4.5888826 ## sample estimates: ## ratio of variances ## 1.816336 Bartletts test is another option. bartlett.test(ind_num$t_dat$engagement, ind_num$t_dat$gender) ## ## Bartlett test of homogeneity of variances ## ## data: ind_num$t_dat$engagement and ind_num$t_dat$gender ## Bartlett&#39;s K-squared = 1.6246, df = 1, p-value = 0.2024 Levenes test is a third option. Levenes is less sensitive to departures from normality than Bartlett. (ind_num$levene &lt;- with(ind_num$t_dat, car::leveneTest(engagement, gender, center = &quot;mean&quot;)) ) ## Levene&#39;s Test for Homogeneity of Variance (center = &quot;mean&quot;) ## Df F value Pr(&gt;F) ## group 1 1.922 0.1737 ## 38 There was homogeneity of variances for engagement scores for males and females, as assessed by Levenes test for equality of variances (p = 0.174). Checking for Similar Distributions If the data fail either the outliers or the normality test, use the Mann-Whitney test. The Mann-Whitney data set failed both, so the Mann-Whitney test applies. Now you need to test the distributions to determine how to interpret its results. If the distributions are similarly shaped, interpret the Mann-Whitney U test as inferences about differences in medians between the two groups. If the distributions are dissimilar, interpret the test as inferences about the distributions, lower/higher scores and/or mean ranks. Distributions of the engagement scores for males and females were similar, as assessed by visual inspection. Test Conduct the t-test or the Mann-Whitney U test. t-Test The the t-test data the variances were equal, so the pooled-variances version applies (t.test(var.equal = TRUE)). (ind_num$t_test &lt;- t.test(engagement ~ gender, data = ind_num$t_dat, var.equal = TRUE)) ## ## Two Sample t-test ## ## data: engagement by gender ## t = 2.3645, df = 38, p-value = 0.02327 ## alternative hypothesis: true difference in means between group Male and group Female is not equal to 0 ## 95 percent confidence interval: ## 0.03725546 0.48074454 ## sample estimates: ## mean in group Male mean in group Female ## 5.558875 5.299875 There was a statistically significant difference in mean engagement score between males and females, with males scoring higher than females, 0.26 (95% CI, 0.04 to 0.48), t(38) = 2.365, p = 0.023. The effect size, Cohens d, is defined as \\(d = |M_D| / s\\), where \\(|M_D| = \\bar{x} - \\bar{y}\\), and \\(s\\) is the pooled sample standard deviation, \\(s_p = \\sqrt{\\frac{(n_X - 1) s_X^2 + (n_Y-1) s_Y^2}{n_X + n_Y-2}}\\). \\(d &lt;.2\\) is considered trivial, \\(.2 \\le d &lt; .5\\) small, and \\(.5 \\le d &lt; .8\\) large. (d &lt;- effectsize::cohens_d(engagement ~ gender, data = ind_num$t_dat, pooled_sd = TRUE)) ## Cohen&#39;s d | 95% CI ## ------------------------ ## 0.75 | [0.10, 1.39] ## ## - Estimated using pooled SD. There was a large difference in mean engagement score between males and females, Cohens d = 0.75 95% CI [0.10, 1.39] Before rejecting the null hypothesis, construct a plot as a sanity check. Wilcoxon Rank Sum test The reference level for the gender variable is males, so the Wilcoxon Rank Sum test statistic is the sum of male ranks minus \\(n_f(n_f + 1) / 2\\) where \\(n_f\\) is the number of females. You can calculate the test statistic by hand. (ind_num$mw_test_manual &lt;- ind_num$mw_dat %&gt;% mutate(R = rank(engagement)) %&gt;% group_by(gender) %&gt;% summarize(.groups = &quot;drop&quot;, n = n(), R = sum(R), meanR = sum(R)/n()) %&gt;% pivot_wider(names_from = gender, values_from = c(n, R, meanR)) %&gt;% mutate(U = R_Male - n_Female * (n_Female + 1) / 2)) ## # A tibble: 1 x 7 ## n_Male n_Female R_Male R_Female meanR_Male meanR_Female U ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 20 20 465 355 23.2 17.8 255 Compare the test statistic to the Wilcoxon rank sum distribution with pwilcox(). pwilcox( q = ind_num$mw_test_manual[1, ]$U - 1, m = ind_num$mw_test_manual[1, ]$n_Male, n = ind_num$mw_test_manual[1, ]$n_Male, lower.tail = FALSE ) * 2 ## [1] 0.141705 There is a function for all this. (ind_num$mw_test &lt;- wilcox.test( engagement ~ gender, data = ind_num$mw_dat, exact = TRUE, correct = FALSE, conf.int = TRUE)) ## ## Wilcoxon rank sum exact test ## ## data: engagement by gender ## W = 255, p-value = 0.1417 ## alternative hypothesis: true location shift is not equal to 0 ## 95 percent confidence interval: ## -0.055 0.420 ## sample estimates: ## difference in location ## 0.1925 Median engagement score was not statistically significantly different between males and females, U = 255, p = 0.142, using an exact sampling distribution for U. Now you are ready to report the results. Here is how you would report the t test. Data are mean \\(\\pm\\) standard deviation, unless otherwise stated. There were 20 male and 20 female participants. An independent-samples t-test was run to determine if there were differences in engagement to an advertisement between males and females. There were no outliers in the data, as assessed by inspection of a boxplot. Engagement scores for each level of gender were normally distributed, as assessed by Shapiro-Wilks test (p &gt; .05), and there was homogeneity of variances, as assessed by Levenes test for equality of variances (p = 0.174). The advertisement was more engaging to male viewers (5.56 \\(\\pm\\) = 0.29) than female viewers (5.30 \\(\\pm\\) = 0.39), a statistically significant difference of 0.26 (95% CI, 0.04 to 0.48), t(38) = 2.365, p = 0.023, d = 0.75. Here is how you would report the Mann-Whitney U-Test. A Mann-Whitney U test was run to determine if there were differences in engagement score between males and females. Distributions of the engagement scores for males and females were similar, as assessed by visual inspection. Median engagement score for males (5.58) and females (5.38) was not statistically significantly different, U = 255, p = 0.142, using an exact sampling distribution for U. Had the distributions differed, you would report the Mann-Whitney like this: A Mann-Whitney U test was run to determine if there were differences in engagement score between males and females. Distributions of the engagement scores for males and females were not similar, as assessed by visual inspection. Engagement scores for males (mean rank = 23.25) and females (mean rank = 17.75) were not statistically significantly different, U = 255, p = 0.142, using an exact sampling distribution for U. This test is sometimes called the Mann-Whitney test, Mann-Whitney U test, Wilcoxon-Mann-Whitney test, and the two-sample Wilcoxon test "],["paired-samples-continuous.html", "5.2 Paired Samples (Continuous)", " 5.2 Paired Samples (Continuous) There are two common study designs that employ a paired samples t-test to compare two related groups. One relates the groups as two time points for the same subjects. The second relates the groups as two tests of the same subjects, e.g.Â comparing reaction time under two lighting conditions. Paired Samples t-Test The paired samples t-test uses the mean of sampled paired differences \\(\\bar{d}\\) as an estimate of the mean of the population paired differences \\(\\delta\\) to evaluate an hypothesized mean \\(\\delta_0\\). Test \\(H_0: \\delta = \\delta_0\\) with test statistic \\(T = \\frac{\\bar{d} - \\delta_0}{se}\\), or define a \\((1 - \\alpha)\\%\\) confidence interval as \\(\\delta = \\bar{d} \\pm t_{1 - \\alpha / 2, n - 1} se\\). The paired t-test is really just a one-sample mean t-test operating on variable that is defined as the difference between two variables. The paired samples t test applies when the sampling distribution of the mean of the population paired differences is normally distributed and there are no significant outliers. Wilcoxon Signed-Rank Test The Wilcoxon signed-rank test is a nonparametric alternative to the paired-samples t-test for cases in which the paired differences fails the normality condition, but is at least symmetrically distributed. The test statistic is the sum product of the difference signs (-1, +1) and the rank of the difference absolute values, \\(W = \\sum_{i=1}^n sign (d_i) \\cdot R_i\\). The more differences that are of one sign, or of extreme magnitude, the larger \\(W\\) is likely to be, and the more likely to reject \\(H_0\\) of equality of medians. Sign Test The sign test is an alternative to the Wilcoxon signed-rank test for cases in which the paired differences fails the symmetrical distribution condition. The test statistic is the count of pairs whose difference is positive, \\(W = cnt(d_i &gt; 0)\\). \\(W \\sim b(n, 0.5)\\), so the sign test is really just an exact binomial test (exact sign test), or for large n-size, the normal approximation to the binomial (sign test). Case Study \\(n\\) = 20 athletes consume a carb-only or carb+protein drink prior to running as far as possible in 2 hours and a researcher records their distances under each condition. Do the distances differ from 0? Laerd has three data sets for this example. One meets the conditions for a t-test. The second fails the normality condition, but is symmetric and meets the conditions for the Wilcoxon test. The third fails the symmetry condition and requires the sign test. t-test data set (drink$t_gt &lt;- drink$t_dat %&gt;% gtsummary::tbl_summary(statistic = list(all_continuous() ~ &quot;{mean} ({sd})&quot;)) ) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #qkkybgfmvo .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #qkkybgfmvo .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #qkkybgfmvo .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #qkkybgfmvo .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; border-top-color: #FFFFFF; border-top-width: 0; } #qkkybgfmvo .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #qkkybgfmvo .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #qkkybgfmvo .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #qkkybgfmvo .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #qkkybgfmvo .gt_column_spanner_outer:first-child { padding-left: 0; } #qkkybgfmvo .gt_column_spanner_outer:last-child { padding-right: 0; } #qkkybgfmvo .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #qkkybgfmvo .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #qkkybgfmvo .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #qkkybgfmvo .gt_from_md > :first-child { margin-top: 0; } #qkkybgfmvo .gt_from_md > :last-child { margin-bottom: 0; } #qkkybgfmvo .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #qkkybgfmvo .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #qkkybgfmvo .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #qkkybgfmvo .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #qkkybgfmvo .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #qkkybgfmvo .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #qkkybgfmvo .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #qkkybgfmvo .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #qkkybgfmvo .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #qkkybgfmvo .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #qkkybgfmvo .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #qkkybgfmvo .gt_sourcenote { font-size: 90%; padding: 4px; } #qkkybgfmvo .gt_left { text-align: left; } #qkkybgfmvo .gt_center { text-align: center; } #qkkybgfmvo .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #qkkybgfmvo .gt_font_normal { font-weight: normal; } #qkkybgfmvo .gt_font_bold { font-weight: bold; } #qkkybgfmvo .gt_font_italic { font-style: italic; } #qkkybgfmvo .gt_super { font-size: 65%; } #qkkybgfmvo .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 65%; } Characteristic N = 201 carb 11.17 (0.73) carb_protein 11.30 (0.71) diff 0.14 (0.10) 1 Mean (SD) There were 20 participants. Data are mean \\(\\pm\\) standard deviation, unless otherwise stated. Participants ran further after consuming the carbohydrate-protein drink, 11.30 (0.71) km, than the carbohydrate-only drink, 11.17 (0.73) km. Wilcoxon data set Once you learn you need Wilcoxon or the sign-test, show the median and IQR summary statistics instead. (drink$wilcoxon_gt &lt;- drink$wilcoxon_dat %&gt;% gtsummary::tbl_summary() ) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #wkrvjkbdzu .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #wkrvjkbdzu .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #wkrvjkbdzu .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #wkrvjkbdzu .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; border-top-color: #FFFFFF; border-top-width: 0; } #wkrvjkbdzu .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #wkrvjkbdzu .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #wkrvjkbdzu .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #wkrvjkbdzu .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #wkrvjkbdzu .gt_column_spanner_outer:first-child { padding-left: 0; } #wkrvjkbdzu .gt_column_spanner_outer:last-child { padding-right: 0; } #wkrvjkbdzu .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #wkrvjkbdzu .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #wkrvjkbdzu .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #wkrvjkbdzu .gt_from_md > :first-child { margin-top: 0; } #wkrvjkbdzu .gt_from_md > :last-child { margin-bottom: 0; } #wkrvjkbdzu .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #wkrvjkbdzu .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #wkrvjkbdzu .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #wkrvjkbdzu .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #wkrvjkbdzu .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #wkrvjkbdzu .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #wkrvjkbdzu .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #wkrvjkbdzu .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #wkrvjkbdzu .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #wkrvjkbdzu .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #wkrvjkbdzu .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #wkrvjkbdzu .gt_sourcenote { font-size: 90%; padding: 4px; } #wkrvjkbdzu .gt_left { text-align: left; } #wkrvjkbdzu .gt_center { text-align: center; } #wkrvjkbdzu .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #wkrvjkbdzu .gt_font_normal { font-weight: normal; } #wkrvjkbdzu .gt_font_bold { font-weight: bold; } #wkrvjkbdzu .gt_font_italic { font-style: italic; } #wkrvjkbdzu .gt_super { font-size: 65%; } #wkrvjkbdzu .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 65%; } Characteristic N = 201 carb 11.28 (10.43, 11.72) carb_protein 11.37 (10.92, 11.81) diff 0.19 (-0.10, 0.47) 1 Median (IQR) There were 20 participants. Data are medians and IQR unless otherwise stated. Participants ran further after consuming the carbohydrate-protein drink, 11.37 (10.92, 11.81) km, than the carbohydrate-only drink, 11.28 (10.43, 11.72) km. Sign data set (drink$sign_gt &lt;- drink$sign_dat %&gt;% gtsummary::tbl_summary() ) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #blsklmwltf .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #blsklmwltf .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #blsklmwltf .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #blsklmwltf .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; border-top-color: #FFFFFF; border-top-width: 0; } #blsklmwltf .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #blsklmwltf .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #blsklmwltf .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #blsklmwltf .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #blsklmwltf .gt_column_spanner_outer:first-child { padding-left: 0; } #blsklmwltf .gt_column_spanner_outer:last-child { padding-right: 0; } #blsklmwltf .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #blsklmwltf .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #blsklmwltf .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #blsklmwltf .gt_from_md > :first-child { margin-top: 0; } #blsklmwltf .gt_from_md > :last-child { margin-bottom: 0; } #blsklmwltf .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #blsklmwltf .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #blsklmwltf .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #blsklmwltf .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #blsklmwltf .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #blsklmwltf .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #blsklmwltf .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #blsklmwltf .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #blsklmwltf .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #blsklmwltf .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #blsklmwltf .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #blsklmwltf .gt_sourcenote { font-size: 90%; padding: 4px; } #blsklmwltf .gt_left { text-align: left; } #blsklmwltf .gt_center { text-align: center; } #blsklmwltf .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #blsklmwltf .gt_font_normal { font-weight: normal; } #blsklmwltf .gt_font_bold { font-weight: bold; } #blsklmwltf .gt_font_italic { font-style: italic; } #blsklmwltf .gt_super { font-size: 65%; } #blsklmwltf .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 65%; } Characteristic N = 201 carb 11.11 (10.43, 11.49) carb_protein 11.37 (10.92, 11.81) diff 0.23 (0.12, 0.55) 1 Median (IQR) There were 20 participants. Data are median and IQR unless otherwise stated. Participants ran further after consuming the carbohydrate-protein drink, 11.37 (10.92, 11.81) km, than the carbohydrate-only drink, 11.11 (10.43, 11.49) km. Conditions The paired samples t test applies when the variable is continuous and partitioned into dependent pairs, Additionally, there are two conditions related to the data distribution. If either condition fails, consider the suggested work-around or move to the non-parametric alternatives. Outliers. There should be no outliers in the differences because they exert a large influence on the mean and standard deviation. Test with a box plot. If there are outliers, you might be able to drop them if they do not affect the conclusion, or you can transform the data. Normality. Differences should be nearly normally distributed (nearly because the t-test is robust to the normality assumption). This condition is especially important with small sample sizes. Test with Q-Q plots or the Shapiro-Wilk test for normality. If the data is very non-normal, you might be able to transform the data. Outliers Assess outliers with a box plot. Box plot whiskers extend up to 1.5*IQR from the upper and lower hinges and outliers (beyond the whiskers) are are plotted individually. There were no outliers in the data, as assessed by inspection of a boxplot. Had there been outliers, you might report X outliers were detected. Inspection of their values did not reveal them to be extreme and they were kept in the analysis. If the outliers are data entry errors or measurement errors, fix them or discard them. If the outliers are genuine, you can try leaving them in or transforming the data. Normality Assume the population is normally distributed if n \\(\\ge\\) 30. These data sets have n = 20 observations, so you cannot assume normality. Asses a Q-Q plot, skewness and kurtosis values, histogram, or Shapiro-Wilk test. For the t-test data set, (drink$t_shapiro &lt;- shapiro.test(drink$t_dat$diff)) ## ## Shapiro-Wilk normality test ## ## data: drink$t_dat$diff ## W = 0.97119, p-value = 0.7797 The differences between the distance ran in the carbohydrate-only and carbohydrate-protein trial were normally distributed, as assessed by Shapiro-Wilks test (p = 0.780). For the Wilcoxon data set, (drink$wilcoxon_shapiro &lt;- shapiro.test(drink$wilcoxon_dat$diff)) ## ## Shapiro-Wilk normality test ## ## data: drink$wilcoxon_dat$diff ## W = 0.87077, p-value = 0.01212 The differences between the distance ran in the carbohydrate-only and carbohydrate-protein trial were not normally distributed, as assessed by Shapiro-Wilks test (p = 0.012). For the sign-test data set, (drink$sign_shapiro &lt;- shapiro.test(drink$sign_dat$diff)) ## ## Shapiro-Wilk normality test ## ## data: drink$sign_dat$diff ## W = 0.8968, p-value = 0.03593 The differences between the distance ran in the carbohydrate-only and carbohydrate-protein trial were not normally distributed, as assessed by Shapiro-Wilks test (p = 0.036). If the data is normally distributed, use the t-test. If not, you try transforming the dependent variable, or carrying on regardless since the t-test is fairly robust to deviations from normality. Symmetric Distribution If the data passed the outliers test, but failed the normality test, as the Wilcoxon and sign test data sets above did, you will use the Wilcoxon signed-rank test or sign test. Now you need to test the distribution to determine which test. If the distribution is symmetric, use Wilcoxon; otherwise use the sign test. For the Wilcoxon data set, The distribution of the differences between the carbohydrate-protein drink and the carbohydrate-only was symmetric, as assessed by visual inspection. For the sign data set, The distribution of the differences between the carbohydrate-protein drink and the carbohydrate-only was not asymmetric, as assessed by visual inspection. Test t-test (drink$t_t &lt;- t.test(x = drink$t_dat$carb_protein, y = drink$t_dat$carb, paired = TRUE) ) ## ## Paired t-test ## ## data: drink$t_dat$carb_protein and drink$t_dat$carb ## t = 6.3524, df = 19, p-value = 4.283e-06 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.09085492 0.18014508 ## sample estimates: ## mean of the differences ## 0.1355 The carbohydrate-protein drink elicited an increase of 0.135 (95% CI, 0.091 to 0.180) km in the distance run in two hours compared to a carbohydrate-only drink. The effect size, called Cohens d, is the number of standard deviations the measured mean difference is from the hypothesized difference, \\((\\bar{d}-d_0) / s\\), where \\(s\\) is the sample standard deviation. .2 is small, .5 is medium, and .8 is large. This one is large. (drink$t_d &lt;- effectsize::cohens_d(drink$t_dat$diff)) ## Cohen&#39;s d | 95% CI ## ------------------------ ## 1.42 | [0.80, 2.09] You are about to reject the null hypothesis. Construct a plot as a sanity check on your reasoning. Report the results. A paired-samples t-test was used to determine the effect of a new formula of sports drink on running performance. Instead of the regular, carbohydrate-only drink, the new sports drink contains a new carbohydrate-protein mixture. Twenty participants were recruited to the study who each performed two trials in which they had to run as far as possible in two hours on a treadmill. In one of the trials they drank the carbohydrate-only drink and in the other trial they drank the carbohydrate-protein drink. The order of the trials was counterbalanced and the distance they ran in both trials was recorded. Two outliers were detected that were more than 1.5 box-lengths from the edge of the box in a boxplot. Inspection of their values did not reveal them to be extreme and they were kept in the analysis. The assumption of normality was not violated, as assessed by Shapiro-Wilks test (p = 0.780). Data are mean \\(\\pm\\) standard deviation, unless otherwise stated. Participants ran further after consuming the carbohydrate-protein drink, 11.30 (0.71) km, than the carbohydrate-only drink, 11.17 (0.73) km, a statistically significant increase of 0.135 (95% CI, 0.091 to 0.180) km, t(19) = 6.352, p = 0.0000, d = 1.42. Wilcoxon Signed-Rank Test From the distribution plot, you can see that most of the signs were positive, and the largest absolute difference values were among the positives, so expect a pretty large test statistic. (drink$wilcoxon_test &lt;- wilcox.test(drink$wilcoxon_dat$carb_protein, drink$wilcoxon_dat$carb, paired = TRUE)) ## ## Wilcoxon signed rank exact test ## ## data: drink$wilcoxon_dat$carb_protein and drink$wilcoxon_dat$carb ## V = 162, p-value = 0.03277 ## alternative hypothesis: true location shift is not equal to 0 The carbohydrate-protein drink elicited a statistically significant median increase in distance run in two hours compared to the carbohydrate-only drink, W = 162, p = 0.033. Report the results. A Wilcoxon signed-rank test was conducted to determine the effect of a new formula of sports drink on running performance. Instead of the regular, carbohydrate-only drink, the new sports drink contains a new carbohydrate-protein mixture. Twenty participants were recruited to the study who each performed two trials in which they had to run as far as possible in two hours on a treadmill. In one of the trials they drank the carbohydrate-only drink and in the other trial they drank the carbohydrate-protein drink. The order of the trials was counterbalanced and the distance they ran in both trials was recorded. The difference scores were approximately symmetrically distributed, as assessed by a histogram with superimposed normal curve. Data are medians unless otherwise stated. Of the 20 participants recruited to the study, the carbohydrate-protein drink elicited an increase in the distance run in 17 participants compared to the carbohydrate-only drink, whereas two participants saw no improvement and one participant did not run as far with the carbohydrate-protein drink. There was a statistically significant median increase in distance run (0.2300 km) when subjects imbibed the carbohydrate-protein drink (11.368 km) compared to the carbohydrate-only drink (11.108 km), W = 162, p = 0.0328. Sign Signed-Rank Test Conduct the exact sign test since the n-size is not so large that we need the normal approximation to the binomial. Notice n is the count of non-zero differences. (drink$sign_test &lt;- binom.test(sum(drink$sign_dat$diff &gt; 0), n = sum(drink$sign_dat$diff != 0))) ## ## Exact binomial test ## ## data: sum(drink$sign_dat$diff &gt; 0) and sum(drink$sign_dat$diff != 0) ## number of successes = 18, number of trials = 18, p-value = 7.629e-06 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.814698 1.000000 ## sample estimates: ## probability of success ## 1 The carbohydrate-protein drink elicited a statistically significant median increase in distance run (0.230 km) compared to the carbohydrate-only drink, p = 0.000. Report the results. An exact sign test was conducted to determine the effect of a new formula of sports drink on running performance. Instead of the regular, carbohydrate-only drink, the new sports drink contains a new carbohydrate-protein mixture. Twenty participants were recruited to the study who each performed two trials in which they had to run as far as possible in two hours on a treadmill. In one of the trials they drank the carbohydrate-only drink and in the other trial they drank the carbohydrate-protein drink. The order of the trials was counterbalanced and the distance they ran in both trials was recorded. An exact sign test was used to determine whether there was a statistically significant median difference between the distance ran when participants drank a carbohydrate-protein drink compared to a carbohydrate-only drink. Data are medians unless otherwise stated. Of the 20 participants recruited to the study, the carbohydrate-protein drink elicited an increase in the distance run in 18 participants compared to the carbohydrate-only drink, whereas 0 participants did not run as far and 2 participant saw no improvement with the carbohydrate-protein drink. There was a statistically significant median increase in distance run (0.2300 km) when subjects imbibed the carbohydrate-protein drink (11.368 km) compared to the carbohydrate-only drink (11.108 km), p = 0.0000. "],["independent-samples-discrete.html", "5.3 Independent Samples (Discrete)", " 5.3 Independent Samples (Discrete) There are three common tests: the z-test of two proportions; chi-square test of homogeneity; and Fishers exact test. The z-test and chi-square test produce the same statistical significance result because they are algebraically identical, however the chi-square test is more commonly used. Both tests require large n&gt;30 sample sizes. When this assumption is violated, use Fishers exact. z-Test of Two Proportions The z-test uses the difference in sample proportions \\(\\hat{d} = p_1 - p_2\\) as an estimate of the difference in population proportions \\(\\delta = \\pi_1 - \\pi_2\\) to evaluate an hypothesized difference in population proportions \\(d_0 = \\pi_0 - \\pi_1\\) and/or construct a \\((1\\alpha)\\%\\) confidence interval around \\(\\hat{d}\\) to estimate \\(\\delta\\) within a margin of error \\(\\epsilon\\). The z-test applies when the central limit theorem conditions hold so that the normal distribution approximates the binomial distribution. the sample is independently drawn, meaning random assignment (experiments) or random sampling without replacement from \\(n &lt; 10\\%\\) of the population (observational studies), there are at least \\(n_i p_i &gt;= 5\\) successes and \\(n_i (1 - p_i) &gt;= 5\\) failures for each group \\(i\\), the sample sizes are both \\(n_i &gt;= 30\\), and the probability of success for each group is not extreme, \\(0.2 &lt; \\pi_i &lt; 0.8\\). If these conditions hold, the sampling distribution of \\(\\delta\\) is normally distributed around \\(\\hat{d}\\) with standard error \\(se_\\hat{d} = \\sqrt{\\frac{p_1(1 - p_1)}{n_1} + \\frac{p_2(1  p_2)}{n_2}}\\). The measured values \\(\\hat{d}\\) and \\(se_\\hat{d}\\) approximate the population values \\(\\delta\\) and \\(se_\\delta\\). Define a \\((1  \\alpha)\\%\\) confidence interval as \\(\\hat{d} \\pm z_{\\alpha / 2}se_\\hat{d}\\) or test the hypothesis of \\(d = d_0\\) with test statistic \\(z = \\frac{\\hat{d}  d_0}{se_{d_0}}\\) where \\(se_{d_0} = \\sqrt{p^*(1 - p^*) \\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}\\) and \\(p^*\\) is the overall success probability. Chi-Square Test of Homogeneity The chi-square test of homogeneity tests whether frequency counts of the R levels of a categorical variable are distributed identically across the C populations. It tests whether observed joint frequency counts \\(O_{ij}\\) differ from expected frequency counts \\(E_{ij}\\) under the independence model (the model of independent explanatory variables, \\(\\pi_{ij} = \\pi_{i+} \\pi_{+j}\\). \\(H_0\\) is \\(O_{ij} = E_{ij}\\). The chi-square homogeneity test can be extended to cases where \\(I\\) and/or \\(J\\) is greater than 2. There are two possible test statistics for this test, Pearson \\(X^2 = \\sum \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\), and deviance \\(G^2 = 2 \\sum_{ij} O_{ij} \\log \\left( \\frac{O_{ij}}{E_{ij}} \\right)\\). Fishers Exact Test Fishers exact test is an exact test in that the p-value is calculated exactly from the hypergeometric distribution rather than relying on the approximation that the test statistic distribution approaches \\(\\chi^2\\) as \\(n \\rightarrow \\infty\\). The test is applicable in situations where the row totals \\(n_{i+}\\) and the column totals \\(n_+j\\) are fixed by study design (rarely applies), and the expected values of &gt;20% of cells (at least 1 cell in a 2x2 table) have expected cell counts &gt;5, and no expected cell count is &lt;1. The p-value from the test is computed as if the margins of the table are fixed. This leads under a null hypothesis of independence to a hypergeometric distribution of the numbers in the cells of the table (Wikipedia). Fishers exact test is useful for small n-size samples where the chi-squared distribution assumption of the chi-squared and G-test tests fails. Fishers exact test is overly conservative (p values too high) for large n-sizes. The Hypergeometric density function is \\[f_X(k|N, K, n) = \\frac{{{K}\\choose{k}}{{N-K}\\choose{n-k}}}{{N}\\choose{n}}.\\] The density is the exact hypergeometric probability of observing this particular arrangement of the data, assuming the given marginal totals, on the null hypothesis that the conditional probabilities are equal. Case Study The case study below uses a data set from Laerd and a second modified version. The first data set passes the chi-square test of homogeneity requirements. The second (in parentheses), fails the n-sizes test. A researcher recruits 100 (50) patients who have a high classification of cholesterol and who currently have a poor lifestyle. The researcher randomly assigns 50 (25) of them to a drug intervention and 50 (25) to a lifestyle intervention. After six months, a doctor reclassifies the patients as either still having a high classification of cholesterol or now having a normal classification of cholesterol. The chi-sq data set has the following summary statistics. .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-c00c3ca0{}.cl-c002ae88{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-c002ae89{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c002ae8a{width:68.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c002ae8b{width:77.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c002ae8c{width:74.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c002ae8d{width:68.2pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c002ae8e{width:74.3pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c002ae8f{width:77.4pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c002ae90{width:68.2pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c002ae91{width:74.3pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c002ae92{width:77.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c0051196{width:68.2pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c0051197{width:74.3pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c0051198{width:77.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 5.1: Chisq Data Set: Observed vs Expected DatainterventionHighNormalTotalObservedDrug18 (36%)32 (64%) 50 (100%)Lifestyle33 (66%)17 (34%) 50 (100%)Total51 (51%)49 (49%)100 (100%)ExpectedDrug25.5 (51%)24.5 (49%) 50 (100%)Lifestyle25.5 (51%)24.5 (49%) 50 (100%)Total51.0 (51%)49.0 (49%)100 (100%) The Fisher data set has the following summary statistics. .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-c046c3de{}.cl-c03dd936{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-c03dd937{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c03dd938{width:68.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c03dd939{width:77.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c03dd93a{width:74.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c03dd93b{width:71.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c03dd93c{width:68.2pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c03dd93d{width:74.3pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c03dd93e{width:77.4pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c03dd93f{width:71.3pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c03dd940{width:68.2pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c0403bae{width:74.3pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c0403baf{width:77.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c0403bb0{width:71.3pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c0403bb1{width:68.2pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c0403bb2{width:74.3pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c0403bb3{width:77.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c0403bb4{width:71.3pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 5.2: Fisher Data Set: Observed vs Expected DatainterventionHighNormalTotalObservedDrug 9 (36%)16 (64%)25 (100%)Lifestyle16 (64%) 9 (36%)25 (100%)Total25 (50%)25 (50%)50 (100%)ExpectedDrug12.5 (50%)12.5 (50%)25 (100%)Lifestyle12.5 (50%)12.5 (50%)25 (100%)Total25.0 (50%)25.0 (50%)50 (100%) Conditions n-Size The chi-square test of homogeneity applies with the CLT conditions hold. the sample is independently drawn, there are at least 5 successes (Normal) and failures (High) for each group \\(i\\), the sample sizes for both groups are &gt;=30, and the probability of success for each group is not extreme, \\(0.2 &lt; \\pi_i &lt; 0.8\\). The conditions hold for the chi-sq data set, but not for the Fisher data set. Test Chi-Square (ind_discrete$chisq_test &lt;- ind_discrete$chisq_dat %&gt;% tabyl(intervention, risk_level) %&gt;% chisq.test(correct = FALSE)) ## ## Pearson&#39;s Chi-squared test ## ## data: . ## X-squared = 9.0036, df = 1, p-value = 0.002694 100 patients with a high cholesterol classification were randomly assigned to either a drug or lifestyle intervention, 50 in each intervention. The test of two proportions used was the chi-square test of homogeneity. At the conclusion of the drug intervention, 32 patients (64%) had improved their cholesterol classification from high to normal compared to 17 patients (34%) in the lifestyle intervention, a difference in proportions of 0.30, p = 0.0027. Fisher (ind_discrete$fisher_test &lt;- ind_discrete$fisher_dat %&gt;% tabyl(intervention, risk_level) %&gt;% fisher.test()) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: . ## p-value = 0.08874 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 0.08497039 1.15362199 ## sample estimates: ## odds ratio ## 0.3241952 50 patients with a high cholesterol classification were randomly assigned to either a drug or lifestyle intervention, 25 in each intervention. At the conclusion of the drug intervention, 16 patients (64%) had improved their cholesterol classification from high to normal compared to 9 patients (36%) in the lifestyle intervention. Due to small sample sizes, Fishers exact test was run. There was a non-statistically significant difference in proportions of 0.28, p = 0.0887. "],["paired-samples-discrete.html", "5.4 Paired Samples (Discrete)", " 5.4 Paired Samples (Discrete) McNemars Test This test applies when you have paired samples. Wilcoxon Paired-Sample applies when the variable distributions are non-normally distributed and samples are paired. "],["multi-group-comparison-tests.html", "Chapter 6 Multi-Group Comparison Tests", " Chapter 6 Multi-Group Comparison Tests Comparison tests look for differences among group means. They can be used to test the effect of a categorical variable on the mean value of some other characteristic. T-tests are used when comparing the means of precisely two groups (e.g.Â the average heights of men and women). ANOVA and MANOVA tests are used when comparing the means of more than two groups (e.g.Â the average heights of children, teenagers, and adults). Quantitative ~ Categorical "],["independent-t-test.html", "6.1 Independent t-Test", " 6.1 Independent t-Test "],["paired-t-test.html", "6.2 Paired t-Test", " 6.2 Paired t-Test "],["sign-est.html", "6.3 Sign est", " 6.3 Sign est "],["wilcoxon-rank-sum-test-2.html", "6.4 Wilcoxon Rank-Sum Test", " 6.4 Wilcoxon Rank-Sum Test "],["wilcoxon-signed-rank-test-2.html", "6.5 Wilcoxon Signed-Rank Test", " 6.5 Wilcoxon Signed-Rank Test "],["anova.html", "6.6 ANOVA", " 6.6 ANOVA Most of these notes are gleaned from PSU STAT-502 Analysis of Variance and Design of Experiments covers ANOVA. Laerd Statistics is useful for writing up your results for reports. Classic analysis of variance (ANOVA) compares the mean responses from experimental studies. However, ANOVA also compares the mean responses from observational studies, but conclusions are just less rigorous. 6.6.1 One-Way ANOVA Use the one-way ANOVA test to compare the mean response of a continuous dependent variable among the levels of a factor variable. Here is a case study. Researchers compare the plant growth among three fertilizers and a control group. Data set greenhouse contains 6 observations per each of the k = 4 treatment levels (N = 24) - a balanced design. All three fertilizers produced more growth than the control group. Fertilizers F1 and F3 appear to be about tied for most growth, but it is unclear if the fertilizers are significantly different from each other. Treated Control(N=6) F1(N=6) F2(N=6) F3(N=6) All treated(N=18) Overall(N=24) Growth (cm) Mean (SD) 21.0 (1.00) 28.6 (2.44) 25.9 (1.90) 29.2 (1.29) 27.9 (2.35) 26.2 (3.69) Median [Min, Max] 21.0 [19.5, 22.5] 28.3 [25.0, 32.0] 26.3 [22.5, 28.0] 29.4 [27.5, 31.0] 28.0 [22.5, 32.0] 27.3 [19.5, 32.0] greenhouse_desc &lt;- greenhouse %&gt;% group_by(group) %&gt;% summarize(.groups = &quot;drop&quot;, n = n(), mean = mean(growth), sd = sd(growth)) Data is presented as mean \\(\\pm\\) standard deviation. Plant growth (growth) increased from the control (n = 6, 21 \\(\\pm\\) 1.0), to fertilizer 1 (n = 6, 28.6 \\(\\pm\\) 2.4), fertilizer 2 (n = 6, 25.8666667 \\(\\pm\\) 1.9), and fertilizer 3 (n = 6, 29.2 \\(\\pm\\) 1.3) fertilizer groups. ANOVA decomposes the deviation of observation \\(Y_{ij}\\) around the overall mean \\(\\bar{Y}_{..}\\) into two parts: the deviation of the observations around their treatment means, \\(SSE\\), and the deviation of the treatment means around the overall mean, \\(SSR\\). Their ratio, \\(F = SSR/SSE\\) follows an F-distribution with \\(k-1\\) numerator dof and \\(N-k\\) denominator dof. The more observation variance captured by the treatments, the large is \\(F\\), and the less likely that the null hypothesis, \\(H_0 = \\mu_1 = \\mu_2 = \\cdots = \\mu_k\\) is true. Table 6.1: ANOVA Table Source SS df MS F SSR \\(\\sum{n_i(\\bar{Y}_{i.} - \\bar{Y}_{..})^2}\\) \\(k - 1\\) \\({SSR}/{(k - 1)}\\) \\({MSR}/{MSE}\\) SSE \\(\\sum(Y_{ij} - \\bar{Y}_{i.})^2\\) \\(N - k\\) \\({SSE}/{(N - k)}\\) SST \\(\\sum(Y_{ij} - \\bar{Y}_{..})^2\\) \\(N - 1\\) Run an ANOVA test in R like this: greenhouse_aov &lt;- aov(growth ~ group, data = greenhouse) greenhouse_anova &lt;- anova(greenhouse_aov) greenhouse_anova %&gt;% tidy() %&gt;% flextable() %&gt;% set_table_properties(width = 0.8, layout = &quot;autofit&quot;) %&gt;% colformat_num(j = c(3, 4, 5), digits = 1) %&gt;% colformat_num(j = 6, digits = 4) %&gt;% set_caption(&quot;Results of ANOVA for Growth vs Fertilizer Group&quot;) .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-c10ba37a{table-layout:auto;width:80%;}.cl-c101d4d0{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-c101d4d1{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c101d4d2{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c101d4d3{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c101d4d4{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c101d4d5{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c101d4d6{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c101d4d7{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c101d4d8{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 6.2: Results of ANOVA for Growth vs Fertilizer Group termdfsumsqmeansqstatisticp.valuegroup3251.4400083.81333327.464770.0000002711994Residuals2061.033333.051667 The one-way ANOVA indicates amount of growth was statistically significantly different for different levels of fertilizer group, F(3, 20) = 27.5, p &lt; .0001. BTW, it is worth noting the relationship with linear regression. The regression model intercept is the overall mean and the coefficient estimators indirectly indicate the group means. The analysis of variance table in a regression model shows how much of the overall variance is explained by those coefficient estimators. Its the same thing. You may also want to report the \\(\\omega^2\\) effect size, \\[\\omega^2 = \\frac{SSR - df_R \\cdot MSE}{MSE + SST}\\] greenhouse_omega &lt;- sjstats::anova_stats(greenhouse_anova) %&gt;% filter(term == &quot;group&quot;) %&gt;% pull(omegasq) \\(\\omega^2\\) ranges from -1 to +1. In this example, \\(\\omega^2\\) is 0.768. 6.6.1.1 ANOVA Conditions The ANOVA test applies when the dependent variable is continuous, the independent variable is categorical, and the observations are independent within groups. Independent means the observations should be from a random sample, or from an experiment using random assignment. Each groups size should be less than 10% of its population size. The groups must also be independent of each other (non-paired, and non-repeated measures). Additionally, there are three conditions related to the data distribution. If any condition does not hold, and the suggested work-arounds do not work switch to the non-parametric [Kruskal-Wallis Test]. No outliers. There should be no significant outliers in the groups. Outliers exert a large influence on the mean and standard deviation. Test with a box plot. If there are outliers, you might be able to drop them or transform the data. Normality. Each groups values should be nearly normally distributed (nearly because ANOVA is considered robust to the normality assumption). This condition is especially important with small sample sizes. Test with the Q-Q plots or the Shapiro-Wilk test for normality. If the data is very non-normal, you might be able to transform your response variable. Equal Variances. The group variances should be roughly equal. This condition is especially important when sample sizes differ. Test with a box plot, rule of thumb, or one of the formal homogeneity of variance (external) tests such as Bartlett, and Levene. If the variances are very different, use a Games-Howell post hoc test instead of the Tukey post hoc test. Outliers Assess outliers with a box plot. Box plot whiskers extend up to 1.5*IQR from the upper and lower hinges and outliers (beyond the whiskers) are are plotted individually. Our example includes an outlier in fertilizer group F2. Outliers might occur from data entry errors or measurement errors, so investigate and fix or throw them out. However, if the outlier is a genuine extreme value, you still have a couple options before reverting to Kruskal-Wallis. Transform the dependent variable. Dont do this unless the data is also non-normal. It also has the downside of making interpretation more difficult. Leave it in if it doesnt affect the conclusion (compared to taking it out). Lets try removing the outlier (id# 13). greenhouse_aov2 &lt;- aov(growth ~ group, data = greenhouse %&gt;% filter(!id == 13)) greenhouse_anova2 &lt;- anova(greenhouse_aov2) .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-c1931cc4{table-layout:auto;width:80%;}.cl-c18a8e24{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-c18a8e25{font-family:'Arial';font-size:6.6pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;position: relative;bottom:3.3pt;}.cl-c18a9c7a{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c18a9c7b{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c18ab4e4{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c18ab4e5{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c18ab4e6{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c18ab4e7{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c18ab4e8{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c18ab4e9{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c18ab4ea{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 6.3: Results of ANOVA for Growth vs Fertilizer Group termdfsumsqmeansqstatisticp.valuegroup3251.012383.67078333.516290.00000008622504Residuals1947.43202.496421Note: One outlier in group F2 removed. The conclusion is the same, so leaving it in is fine! Normality You can assume the populations are normally distributed if \\(n_j &gt;= 30\\). Otherwise, try the Q-Q plot, or skewness and kurtosis values, or histograms. If you still dont feel confident about normality, run a [Shapiro-Wilk Test] or Kolmogorov-Smirnov Test. If \\(n_j &gt;= 50\\), stick with graphical methods because at larger sample sizes Shapiro-Wilk flags even minor deviations from normality. The QQ plots below appear to be approximately normal. greenhouse %&gt;% ggplot(aes(sample = growth)) + stat_qq() + stat_qq_line(col = &quot;goldenrod&quot;) + facet_wrap(~group) + theme_minimal() + labs(title = &quot;Normal Q-Q Plot&quot;) The Shapiro-Wilk test corroborates this conclusion - it fails to reject the null hypothesis of normally distributed populations. x &lt;- by(greenhouse, greenhouse$group, function(x) shapiro.test(x$growth) %&gt;% tidy()) x[1:4] %&gt;% bind_rows() %&gt;% mutate(group = names(x)) %&gt;% dplyr::select(group, everything(), - method) %&gt;% flextable() %&gt;% set_table_properties(width = 0.6, layout = &quot;autofit&quot;) %&gt;% set_caption(&quot;Shapiro-Wilk Normality Test&quot;) .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-c1ef97e2{table-layout:auto;width:60%;}.cl-c1e7170c{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-c1e7170d{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c1e7170e{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c1e7170f{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c1e71710{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c1e71711{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c1e71712{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c1e71713{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c1e71714{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 6.4: Shapiro-Wilk Normality Test groupstatisticp.valueControl0.97648960.9328373F10.98032330.9531535F20.92569450.5472862F30.97059600.8964141 If the data is not normally distributed, you still have a couple options before reverting to Kruskal-Wallis. Transform the dependent variable. Transformations will generally only work when the distribution of scores in all groups are the same shape. They also have the drawback of making the data less interpretable. carry on regardless. One-way ANOVA is fairly robust to deviations from normality, particularly if the sample sizes are nearly equal. Equal Variances The equality of sample variances condition is less critical when sample sizes are similar among the groups. One rule of thumb is that no groups standard deviation should be more than double that of any other. In this case F1 is more than double Control. .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-c20f75c6{table-layout:auto;width:50%;}.cl-c205ec90{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-c205ec91{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c205ec92{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c2061594{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c2061595{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c2061596{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c2061597{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c2061598{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c2061599{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} groupsdControl1.000000F12.437212F21.899123F31.288410 There are two other common tests, Bartlett and Levene. NIST has a good write-up for Levene and for Bartlett. Levene is less sensitive than Bartlett to departures from normality, so if you know your data is normally distributed, then use Bartlett. Levenes test fails to reject the null hypothesis of equality of variance. greenhouse_levene &lt;- car::leveneTest(growth ~ group, data = greenhouse) greenhouse_levene %&gt;% tidy() %&gt;% flextable() %&gt;% set_table_properties(width = 0.6, layout = &quot;autofit&quot;) %&gt;% set_caption(&quot;Levene&#39;s Test for Homogeneity of Variance&quot;) .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-c2261768{table-layout:auto;width:60%;}.cl-c21b5120{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-c21b5121{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c21b5122{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c21b5123{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 6.5: Levenes Test for Homogeneity of Variance statisticp.valuedfdf.residual1.0491160.3926038320 So does Bartlett. bartlett.test(growth ~ group, data = greenhouse) %&gt;% tidy() %&gt;% dplyr::select(-method) %&gt;% flextable() %&gt;% set_table_properties(width = 0.6, layout = &quot;autofit&quot;) %&gt;% set_caption(&quot;Bartlett&#39;s Test for Homogeneity of Variance&quot;) .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-c2467c4c{table-layout:auto;width:60%;}.cl-c23fbdd0{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-c23fbdd1{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c23fe5a8{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c23fe5a9{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 6.6: Bartletts Test for Homogeneity of Variance statisticp.valueparameter4.1143420.2493793 Heterogeneity is a common problem in ANOVA. Transforming the response variable can often remove the heterogeneity. The Box-Cox procedure can help find a good transformation. The MASS::boxcox() function calculates a profile of log-likelihoods for a power transformation of the response variable \\(Y^\\lambda\\). \\(\\lambda\\) \\(Y^\\lambda\\) Transformation 2 \\(Y^2\\) Square 1 \\(Y^1\\) (no transformation) .5 \\(Y^{.5}\\) Square Root 0 \\(\\ln(Y)\\) Log -.5 \\(Y^{-.5}\\) Inverse Square Root -1 \\(Y^{-1}\\) Inverse The Box-Cox procedure does not recommend any particular transformation of the data in this case. MASS::boxcox(greenhouse_aov, plotit = TRUE) 6.6.1.2 Custom Contrasts Taking this route is appropriate if you have specific hypotheses about the differences between the groups of your independent variable. For example, we might want to test whether the mean of the treatments differ from the control group, \\(H_0: \\sum_i^K{c_i u_i} = 0\\) where \\(c_i = (1, -1/3, -1/3, -1/3)\\). You can test a constrast using the multcomp package. greenhouse_glht &lt;- multcomp::glht(greenhouse_aov, linfct = multcomp::mcp(group = c(-1, 1/3, 1/3, 1/3))) greenhouse_glht_smry &lt;- summary(greenhouse_glht) greenhouse_confint &lt;- confint(greenhouse_glht) greenhouse_glht_smry ## ## Simultaneous Tests for General Linear Hypotheses ## ## Multiple Comparisons of Means: User-defined Contrasts ## ## ## Fit: aov(formula = growth ~ group, data = greenhouse) ## ## Linear Hypotheses: ## Estimate Std. Error t value Pr(&gt;|t|) ## 1 == 0 6.8889 0.8235 8.365 5.81e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## (Adjusted p values reported -- single-step method) Growth was statistically significantly higher in the fertilizer groups (mean of 27.9) compared to the sedentary group (21 \\(\\pm\\) 1), a mean difference of 6.8888889 (95% CI, 5.1711032 to 8.6066746), p = 5.8068537^{-8}. 6.6.1.3 Tukey Post Hoc Test The F test does not indicate which populations cause the rejection of \\(H_0\\). For this, use one of the post-hoc tests: Tukey, Fishers Least Significant Difference (LSD), Bonferroni, Scheffe, or Dunnett. Post hoc tests are appropriate if you are investigating all possible pairwise comparisons with no specific hypotheses about specific groups differing from others. Here is the Tukey test. As expected, all three fertilizer factor levels differ from the control. F3 differed from F2, but F1 was not significantly different from either F2 or F3. greenhouse_tukey &lt;- TukeyHSD(greenhouse_aov) greenhouse_tukey %&gt;% tidy() %&gt;% flextable() %&gt;% set_table_properties(width = 0.8, layout = &quot;autofit&quot;) %&gt;% colformat_num(j = c(4:6), digits = 1) %&gt;% colformat_num(j = 7, digits = 3) %&gt;% set_caption(&quot;Tukey multiple comparisons of means&quot;) %&gt;% footnote(i = 1, j = c(1), value = as_paragraph( paste0(&quot;95% family-wise confidence level\\n&quot;, &quot;Fit: aov(formula = growth ~ group, data = greenhouse)&quot;)), ref_symbols = c(&quot;&quot;), part = &quot;header&quot;) .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-c2907bbc{table-layout:auto;width:80%;}.cl-c287d8ea{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-c287d8eb{font-family:'Arial';font-size:6.6pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;position: relative;bottom:3.3pt;}.cl-c2880004{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c2880005{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c2886d96{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c2886d97{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c2886d98{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c2886d99{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c2886d9a{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c2886d9b{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c2886d9c{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 6.7: Tukey multiple comparisons of means termcontrastnull.valueestimateconf.lowconf.highadj.p.valuegroupF1-Control07.6000004.777064810.422935210.0000016379877groupF2-Control04.8666672.04373157.689601880.0005509424188groupF3-Control08.2000005.377064811.022935210.0000005148374groupF2-F10-2.733333-5.55626850.089601880.0598655148867groupF3-F100.600000-2.22293523.422935210.9324380205275groupF3-F203.3333330.51039816.156268540.017103298651595% family-wise confidence levelFit: aov(formula = growth ~ group, data = greenhouse) Data are mean \\(\\pm\\) standard deviation. There was an increase in growth from 21 \\(\\pm\\) 1 in the control group to 28.6 \\(\\pm\\) 2.4 in the group with fertilizer F1, an increase of 7.6 (95% CI, 4.8 to 10.4), which was statistically significant (p &lt; .0001) etc. 6.6.1.4 Reporting a One-Way ANOVA Report like this. A one-way ANOVA was conducted to determine if plant growth was different for groups with different fertilizer types. Plants were classified into four groups: control (n = 6), fertilizer 1 (n = 6), fertilizer 2 (n = 6), and fertilizer 3 (n = 6). There was a single outlier, as assessed by boxplot, and was retained because it did not change the conclusions; data was normally distributed for each group, as assessed by Shapiro-Wilk test (p &gt; .05); and there was homogeneity of variances, as assessed by Levenes test of homogeneity of variances (p = 0.393). Data is presented as mean \\(\\pm\\) standard deviation. Plant growth was statistically significantly different between different fertilizer groups, F(3, 20) = 27.465, p &lt; .0005, \\(\\omega^2\\) = 0.768. Plant growth increased from the control (21 \\(\\pm\\) 1.0), to fertilizer F1 (28.6 \\(\\pm\\) 2.4), fertilizer F2 (25.9 \\(\\pm\\) 1.9), and fertilizer F3 (29.2 \\(\\pm\\) 1.3) fertilizer groups. Tukey post hoc analysis revealed statistically significant increases from control to F1 (7.6, 95% CI (4.8 to 10.4), p = 1.6e-06), control to F2 (4.9, 95% CI (2.0 to 7.7), p = 0.00055), and control to F3 (8.2, 95% CI (5.4 to 11.0), p = 5.1e-07), as well as the increase from F2 to F3 (3.3, 95% CI (0.51 to 6.2), p = 0.017), but there were no statistically significant group differences between F1 and F2 or F1 and F3. 6.6.2 Welchs ANOVA w/Games-Howell Welchs ANOVA test is an alternative to the one-way ANOVA test in cases where the equality of variances assumption is violated. Here is a case study. Researchers compare the force (in newtons) generated in three steps. Data set newton contains 30 observations per each of the k = 3 step levels (N = 90) - a balanced design. A(N=30) B(N=30) C(N=30) Overall(N=90) Force (newtons) Mean (SD) 429 (88.7) 527 (97.6) 649 (145) 535 (144) Median [Min, Max] 415 [306, 692] 499 [417, 759] 615 [437, 939] 498 [306, 939] newton_desc &lt;- newton %&gt;% group_by(step) %&gt;% summarize(.groups = &quot;drop&quot;, n = n(), mean = mean(newtons), sd = sd(newtons)) Data is presented as mean \\(\\pm\\) standard deviation. Force (newtons) increased from step 1 (n = 30, 429 \\(\\pm\\) 88.7), to step 2 (n = 30, 527 \\(\\pm\\) 97.6), to step 3 (n = 30, 649 \\(\\pm\\) 145). Start by running the standard ANOVA test: newton_aov &lt;- aov(newtons ~ step, data = newton) newton_anova &lt;- anova(newton_aov) newton_anova %&gt;% tidy() %&gt;% flextable() %&gt;% set_table_properties(width = 0.8, layout = &quot;autofit&quot;) %&gt;% colformat_num(j = c(3, 4, 5), digits = 1) %&gt;% colformat_num(j = 6, digits = 4) %&gt;% set_caption(&quot;Results of ANOVA for Force vs Step&quot;) .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-c2ec4190{table-layout:auto;width:80%;}.cl-c2e49152{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-c2e53396{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c2e53397{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c2e577c0{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c2e577c1{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c2e577c2{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c2e577c3{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c2e577c4{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c2e577c5{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 6.8: Results of ANOVA for Force vs Step termdfsumsqmeansqstatisticp.valuestep2727,295363,647.528.419960.0000000003171986Residuals871,113,20812,795.5 The one-way ANOVA indicates amount of force was statistically significantly different for different levels of step, F(2, 87) = 28.4, p &lt; .0001. 6.6.2.1 ANOVA Conditions Check the three ANOVA conditions: no outliers, normality, and equal variances. Outliers Assess outliers with a box plot. Our example includes an outlier in step A. You can either transform the dependent variable, see if taking it out changes your conclusion, or use a non-parametric test. Lets try removing the outlier (id# 13). newton2 &lt;- newton %&gt;% mutate(id = row_number()) newton_aov2 &lt;- aov(newtons ~ step, data = newton2 %&gt;% filter(!id == 7)) newton_anova2 &lt;- anova(newton_aov2) .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-c3411aa8{table-layout:auto;width:80%;}.cl-c33810f2{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-c33810f3{font-family:'Arial';font-size:6.6pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;position: relative;bottom:3.3pt;}.cl-c33810f4{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c33810f5{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c33810f6{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c33810f7{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c33810f8{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c33810f9{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c33810fa{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c33810fb{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c33810fc{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 6.9: Results of ANOVA for Force vs Step termdfsumsqmeansqstatisticp.valuestep2773,863.5386,931.7431.945820.00000000004217012Residuals861,041,642.712,112.12Note: One outlier in step A removed. The conclusion is the same, so leaving it in is fine! Normality You can assume the populations are normally distributed if \\(n_j &gt;= 30\\), but Ill examine the Q-Q plot and run a [Shapiro-Wilk Test] anyway. The QQ plots below appear to be approximately normal newton %&gt;% ggplot(aes(sample = newtons)) + stat_qq() + stat_qq_line(col = &quot;goldenrod&quot;) + facet_wrap(~step) + theme_minimal() + labs(title = &quot;Normal Q-Q Plot&quot;) but the Shapiro-Wilk test fails for step A and B  evidence of its sensitivity for large n. I will ignore this violation. x &lt;- by(newton, newton$step, function(x) shapiro.test(x$newtons) %&gt;% tidy()) x[1:3] %&gt;% bind_rows() %&gt;% mutate(group = names(x)) %&gt;% dplyr::select(group, everything(), - method) %&gt;% flextable() %&gt;% set_table_properties(width = 0.6, layout = &quot;autofit&quot;) %&gt;% set_caption(&quot;Shapiro-Wilk Normality Test&quot;) .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-c395005a{table-layout:auto;width:60%;}.cl-c38a0b96{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-c38a5a42{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c38a5a43{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c38aa7f4{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c38aa7f5{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c38aa7f6{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c38aa7f7{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c38aa7f8{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c38aa7f9{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 6.10: Shapiro-Wilk Normality Test groupstatisticp.valueA0.92548620.03730756B0.90524050.01130931C0.94660730.13705759 Equal Variances The equality of sample variances condition is less critical when sample sizes are similar among the groups. Following the rule of thumb that no groups standard deviation be more than double that of any other, we look okay. .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-c3ab216e{table-layout:auto;width:50%;}.cl-c3a40d52{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-c3a43764{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c3a43765{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c3a43766{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c3a43767{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c3a43768{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c3a43769{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c3a4376a{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c3a4376b{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} stepsdA88.66137B97.58970C144.92031 However, Levenes test rejects the null hypothesis of equality of variance. newton_levene &lt;- car::leveneTest(newtons ~ step, data = newton) newton_levene %&gt;% tidy() %&gt;% flextable() %&gt;% set_table_properties(width = 0.6, layout = &quot;autofit&quot;) %&gt;% set_caption(&quot;Levene&#39;s Test for Homogeneity of Variance&quot;) .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-c3c66c12{table-layout:auto;width:60%;}.cl-c3bf789e{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-c3bfa18e{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c3bfa18f{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c3bfa190{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 6.11: Levenes Test for Homogeneity of Variance statisticp.valuedfdf.residual3.9707430.02237434287 So does Bartlett. bartlett.test(newtons ~ step, data = newton) %&gt;% tidy() %&gt;% dplyr::select(-method) %&gt;% flextable() %&gt;% set_table_properties(width = 0.6, layout = &quot;autofit&quot;) %&gt;% set_caption(&quot;Bartlett&#39;s Test for Homogeneity of Variance&quot;) .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-c3dd87bc{table-layout:auto;width:60%;}.cl-c3d3e4b4{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-c3d3e4b5{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c3d3e4b6{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c3d3e4b7{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 6.12: Bartletts Test for Homogeneity of Variance statisticp.valueparameter8.1969350.016598092 We could transform the response variable to remove the heterogeneity. The Box-Cox procedure suggests an inverse square root transformation. \\(\\lambda\\) \\(Y^\\lambda\\) Transformation 2 \\(Y^2\\) Square 1 \\(Y^1\\) (no transformation) .5 \\(Y^{.5}\\) Square Root 0 \\(\\ln(Y)\\) Log -.5 \\(Y^{-.5}\\) Inverse Square Root -1 \\(Y^{-1}\\) Inverse The Box-Cox procedure does not recommend any particular transformation of the data in this case. # MASS::boxcox(newton_aov, plotit = TRUE) newton3 &lt;- newton %&gt;% mutate(newtons_isr = newtons^(-0.5)) newton_levene3 &lt;- car::leveneTest(newtons_isr ~ step, data = newton3) newton_levene3 %&gt;% tidy() %&gt;% flextable() %&gt;% set_table_properties(width = 0.6, layout = &quot;autofit&quot;) %&gt;% set_caption(&quot;Levene&#39;s Test for Homogeneity of Variance&quot;) .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-c3feeef2{table-layout:auto;width:60%;}.cl-c3f6fbf2{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-c3f72e2e{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c3f775e6{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c3f775e7{background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 6.13: Levenes Test for Homogeneity of Variance statisticp.valuedfdf.residual0.52340380.5943542287 Huzzah - it worked! Before we continue on, we should backtrack and re-test the outliers and normality conditions. However, because the point of this section is to try Welchs ANOVA, Im going use it instead of transforming the response variable. Use oneway.test(..., var.equal = FALSE) to run a Welchs ANOVA. newton_anova &lt;- oneway.test(newtons ~ step, data = newton, var.equal = FALSE) newton_anova ## ## One-way analysis of means (not assuming equal variances) ## ## data: newtons and step ## F = 26.19, num df = 2.000, denom df = 56.184, p-value = 9.196e-09 Welchs ANOVA indicates amount of force was statistically significantly different for different steps, F(2, 56.2) = 26.2, p &lt; .0001. I dont think you can calculate \\(\\omega^2\\) for a Welchs ANOVA object. 6.6.2.2 Games-Howell Post Hoc Test Use the PMCMRplus::gamesHowellTest() to run the Games-Howell post hoc test. As expected, the three steps differ from each other. newton_games_howell &lt;- rstatix::games_howell_test(newton, newtons ~ step) newton_games_howell %&gt;% flextable() %&gt;% autofit() %&gt;% set_caption(&quot;Games-Howell Post Hoc Test&quot;) .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-c4380df4{}.cl-c42c3c22{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-c42ea17e{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c42ea17f{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c42ec88e{width:61.5pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c42ec88f{width:66.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c42ec890{width:72.5pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c42ec891{width:54.8pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c42ec892{width:90.9pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c42ec893{width:73.1pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c42ec894{width:61.5pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c42ec895{width:66.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c42ec896{width:72.5pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c42ec897{width:54.8pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c42ec898{width:90.9pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c4312bb0{width:73.1pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c4312bb1{width:61.5pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c4312bb2{width:66.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c4312bb3{width:72.5pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c4312bb4{width:54.8pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c4312bb5{width:90.9pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c4312bb6{width:73.1pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c4312bb7{width:61.5pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c4312bb8{width:66.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c4312bb9{width:72.5pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c4312bba{width:54.8pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c4312f20{width:90.9pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c4312f21{width:73.1pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 6.14: Games-Howell Post Hoc Test .y.group1group2estimateconf.lowconf.highp.adjp.adj.signifnewtonsAB97.6486739.73284155.56450.0004410000***newtonsAC219.74333144.72971294.75700.0000000162****newtonsBC122.0946745.08372199.10560.0010000000*** 6.6.2.3 Reporting a Welchs ANOVA A Welchs ANOVA was conducted to determine if force was different for different steps. Measurements were classified into three groups: A (n = 30), B (n = 30), and C (n = 30). There was a single outlier, as assessed by boxplot, and was retained because it did not change the conclusions; data was normally distributed for each group, as assessed by Q-Q plot. ; Homogeneity of variances was violated, as assessed by Levenes Test of Homogeneity of Variance (p = 0.022). Data is presented as mean \\(\\pm\\) standard deviation. Force was statistically significantly different between different steps, F(2, 56.1842568) = 26.19, p &lt; .0005. Force increased from A (429.1793333 \\(\\pm\\) 88.66137), to B (526.828 \\(\\pm\\) 97.6), to C (648.9226667 \\(\\pm\\) 144.9). Games-Howell post hoc analysis revealed statistically significant increases from A to B, (97.6, 95% CI (39.7 to 39.7), p = 4e-04), A to C (219.7, 95% CI (144.7 to 144.7), p = 2e-08), and B to C (122.1, 95% CI (45.1 to 45.1), p = 0.001). 6.6.3 MANOVA Multi-factor ANOVA (MANOVA) is a method to compare mean responses by treatment factor level of two or more treatments applied in combination. The null hypotheses are \\(H_0: \\mu_{1.} = \\mu_{2.} = \\dots = \\mu_{a.}\\) for the \\(a\\) levels of factor 1, \\(H_0: \\mu_{.1} = \\mu_{.2} = \\dots = \\mu_{.b}\\) for the \\(b\\) levels of factor 2, etc. for all the factors in the experiment, and $H_0: $ no interaction for all the factor interactions. There are two equivalent ways to state the MANOVA model: \\[Y_{ijk} = \\mu_{ij} + \\epsilon_{ijk}\\] In this notation \\(Y_{ijk}\\) refers to the \\(k^{th}\\) observation in the \\(j^{th}\\) level of factor two and the \\(i^{th}\\) level of factor 1. Potentially there could be additional factors. This model formulation decomposes the response into a cell mean and an error term. The second makes the factor effect more explicit and is thus more common: \\[Y_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\epsilon_{ijk}\\] 6.6.4 Multiple Variance Comparison F Test 6.6.5 Example A study investigates the relationship between oxygen update and two explanatory variables: smoking, and type of stress test. A sample of \\(n = 27\\) persons, 9 non-smoking, 9 moderately-smoking, and 9 heavy-smoking are divided into three stress tests, bicycle, treadmill, and steps and their oxygen uptake was measured. Is oxygen uptake related to smoking status and type of stress test? Is there an interaction effect between smoking status and type of stress test? library(dplyr) library(ggplot2) library(nortest) # for Anderson-Darling test library(stats) # for anova smoker &lt;- c(1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3) stress &lt;- c(1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3) oxytime &lt;- c(12.8, 13.5, 11.2, 16.2, 18.1, 17.8, 22.6, 19.3, 18.9, 10.9, 11.1, 9.8, 15.5, 13.8, 16.2, 20.1, 21.0, 15.9, 8.7, 9.2, 7.5, 14.7, 13.2, 8.1, 16.2, 16.1, 17.8) oxy &lt;- data.frame(oxytime, smoker, stress) oxy$smoker &lt;- ordered(oxy$smoker, levels = c(1, 2, 3), labels = c(&quot;non-smoker&quot;, &quot;moderate&quot;, &quot;heavy&quot;)) oxy$stress &lt;- factor(oxy$stress, labels = c(&quot;bicycle&quot;, &quot;treadmill&quot;, &quot;steps&quot;)) lm_oxy &lt;- lm(oxytime~smoker+stress+smoker*stress, data = oxy) anova(lm_oxy) ## Analysis of Variance Table ## ## Response: oxytime ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## smoker 2 84.899 42.449 12.8967 0.0003348 *** ## stress 2 298.072 149.036 45.2793 9.473e-08 *** ## smoker:stress 4 2.815 0.704 0.2138 0.9273412 ## Residuals 18 59.247 3.291 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 SFU BIO710 6.6.6 Repeated Measures 6.6.7 Two-Way 6.6.8 Randomized Blocks 6.6.9 ANCOVA "],["kruskalwallis-test.html", "6.7 KruskalWallis Test", " 6.7 KruskalWallis Test "],["pairwise.prop.html", "6.8 pairwise.prop.test", " 6.8 pairwise.prop.test library(tidyverse) M &lt;- 3573 F &lt;- 4177 dat &lt;- tribble( ~gender, ~src, ~Y, ~N, &quot;Male&quot;, &quot;Indeed&quot;, 1699, M-1699, &quot;Male&quot;, &quot;LinkedIn&quot;, 1755, M-1755, &quot;Male&quot;, &quot;Google&quot;, 1578, M-1578, &quot;Female&quot;, &quot;Indeed&quot;, 2554, F-2554, &quot;Female&quot;, &quot;LinkedIn&quot;, 1914, F-1914, &quot;Female&quot;, &quot;Google&quot;, 1694, F-1694 ) prop.test(x = dat$Y, n = dat$Y + dat$N) ## ## 6-sample test for equality of proportions without continuity ## correction ## ## data: dat$Y out of dat$Y + dat$N ## X-squared = 412.66, df = 5, p-value &lt; 2.2e-16 ## alternative hypothesis: two.sided ## sample estimates: ## prop 1 prop 2 prop 3 prop 4 prop 5 prop 6 ## 0.4755108 0.4911839 0.4416457 0.6114436 0.4582236 0.4055542 pairwise.prop.test(x = dat$Y, n = dat$Y + dat$N) ## ## Pairwise comparisons using Pairwise comparison of proportions ## ## data: dat$Y out of dat$Y + dat$N ## ## 1 2 3 4 5 ## 2 0.40250 - - - - ## 3 0.02026 0.00021 - - - ## 4 &lt; 2e-16 &lt; 2e-16 &lt; 2e-16 - - ## 5 0.40250 0.02026 0.40250 &lt; 2e-16 - ## 6 6.3e-09 4.8e-13 0.00873 &lt; 2e-16 1.1e-05 ## ## P value adjustment method: holm "],["association.html", "Chapter 7 Association", " Chapter 7 Association Tests of association assess the strength of association between two variables. There are many variations on this theme. Pearsons correlation assesses the strength of a linear relationship between two continuous variables. It applies when the relationship is linear with no outliers and the variables are bi-variate normal. There are two less restrictive alternatives, Spearmans rho and Kendalls tau, that assess the strength and direction of association. If one of the variables is bivariate categorical, use point-biserial correlation, a special case of Pearsons correlation. Pearsons partial correlation controls for one or more variables - linear regression? If both variables are ordinal, use Goodman and Kruskals gamma. Somers d is an alternative if you want to distinguish between a dependent and independent variable (instead of linear regression?). The Mantel-Haenszel test of trend is used to determine whether there is a linear trend (i.e., a linear relationship/association) between two ordinal variables that are represented in a contingency table. The Cochran-Armitage test of trend is used to determine whether there is a linear trend (i.e., a linear relationship/association) between an ordinal independent variable and a dichotomous dependent variable. The chi-square test for association tests for whether two categorical variables are associated. The chi-square test for independence focuses on contingency tables that are greater than 2 x 2, which are often referred to as r x c contingency tables, and tests whether two variables measured at the nominal level are independent (i.e., whether there is an association between the two variables). Relative risks can be calculated from more than one statistical test, but in this guide we will focus on the calculation of relative risk in a 2 x 2 table. Odds Ratio can be calculated from more than one statistical test (e.g., a binomial logistic regression, ordinal logistic regression, multinomial logistic regression, etc), but in this guide we will focus on the calculation of an odds ratio from a 2 x 2 contingency table (i.e., a measure of association between two dichotomous variables). Goodman and Kruskals  (the Greek symbol, , is pronounced lambda) is also referred to as Goodman and Kruskals lambda. It is a nonparametric measure of the strength of association between two nominal variables where a distinction is made between a dependent and independent variable The Fishers exact test can be used to test more than one type of null hypothesis. In this guide we will use Fishers exact test to determine whether two dichotomous variables are independent (i.e., test the null hypothesis of independence). Loglinear analysis is used to understand (and model) associations between two or more categorical variables (i.e., nominal or ordinal variables). However, loglinear analysis is usually employed when dealing with three or more categorical variables, as opposed to two variables, where a chi-square test for association is usually conducted instead. Use association tests to assess a possible two-way linear association between two continuous (interval or ratio) random variables. Association tests return an estimate between +1 (perfect linear relationship) to -1 (perfect linear inverse relationship). Use Pearsons product moment if both random variables are normally distributed. If either variable is skewed, ordinal, or has extreme values, use Spearmans rank correlation. There are three common correlation tests for categorical variables7: Tetrachoric correlation for binary categorical variables; polychoric correlation for ordinal categorical variables; and Cramers V for nominal categorical variables. See https://www.statology.org/correlation-between-categorical-variables/ "],["pearsons-correlation.html", "7.1 Pearsons Correlation", " 7.1 Pearsons Correlation The Pearson product-moment correlation measures the strength and direction of a linear relationship between two continuous variables, x and y. The Pearson correlation coefficient, r, ranges from -1 (perfect negative linear relationship) to +1 (perfect positive linear relationship). A value of 0 indicates no relationship between two variables. \\[r_{x,y} = \\frac{cov(x,y)}{\\sigma(x) \\sigma(y)}\\] The statistic can be used as an estimate of the population correlation, \\(\\rho\\), in a test of statistical significance from 0 (H0: \\(\\rho\\) = 0). \\[\\rho_{X,Y} = \\frac{cov(X,Y)}{\\sigma(X) \\sigma(Y)}\\] A rule of thumb interpretation is that \\(|r|\\) under .1 is no correlation, .1 - .3 is small, .3 - .5 medium/moderate, and .5 - 1.0 large/strong. The test statistic below follows a t-distribution with n  2 degrees of freedom. \\[t = r_{x,y} \\sqrt{\\frac{n - 2}{1 - r^2_{x,y}}}\\] "],["spearmans-rho.html", "7.2 Spearmans Rho", " 7.2 Spearmans Rho Spearmans ranked correlation (Spearmans rho) is a measure of the strength and direction of a monotonic relationship between two variables that are at least ordinal. Spearmans correlation is a non-parametric alternative to Pearson when one or more of its conditions are violated. Unlike Pearson, the relationship need not be linear (it only needs to be monotonic), and has no outliers or bivariate normality conditions. Spearmans correlation is Pearsons correlation applied to the ranks of variables (for ordinal variables, their value already is a rank). However, there is also a second definition that gives the same result, at least when there are no ties in the ranks: \\[\\rho = 1 - \\frac{6 \\sum_i d^2_i}{n(n^2 - 1)}\\] where \\(d_i\\) is the difference in ranks of observation \\(i\\). "],["kendals-tau.html", "7.3 Kendals Tau", " 7.3 Kendals Tau Kendals tau is a second alternative to Pearson and is identical to Spearmans rho with regard to assumptions. Kendals tau only differs from Spearmans rho in how it measures the relationship. Whereas Spearman measures the correlation of the ranks, Kendals tau is a function of concordant (C), discordant (D) and tied (Tx and Ty) pairs of observations. Concordant means both X and Y in one observation of a pair are larger than in the other. Discordant means X is larger in one observation than the other while Y is smaller. Tied mean either both observations have the same X or both have the same Y. \\[\\mathrm{Kendall&#39;s} \\space \\tau_b = \\frac{C - D}{\\sqrt{(C + D + T_x) \\times (C + D + T_Y)}}\\] "],["case-study-pearson-spearman-kendall.html", "7.4 Case Study: Pearson, Spearman, Kendall", " 7.4 Case Study: Pearson, Spearman, Kendall This case study works with two data sets. dat1 is composed of two continuous variables; dat2 is composed of two ordinal variables. A researcher investigates the relationship between cholesterol concentration and time spent watching TV, time_tv, in n = 100 otherwise healthy 45 to 65 year old men (dat1). This data set will meet the conditions for Pearson, so we try all three tests on it. A researcher investigates the relationship between the level of agreement with the statement Taxes are too high (tax_too_high, four level ordinal) and participant income level (three level ordinal) (dat2). The ordinal variables rule out Pearson, leaving Spearman and Kendall. Conditions Pearsons correlation applies when X and Y are continuous (interval or ratio) paired variables with 1) a linear relationship that 2) has no significant outliers, and 3) are bivariate normal. Spearmans rho and Kendalls tau only require that X and Y be at least ordinal with 1) a monotonically increasing or decreasing relationship. Linearity and Monotonicity. A visual inspection of a scatterplot should find a linear relationship (Pearson) or monotonic relationship (Spearman and Kendall). Pearsons correlation additionall requires No Outliers. Identify outliers with the scatterplot. Normality. Bivariate normality is difficult to assess. Instead, check that each variable is individually normally distributed. Use the Shapiro-Wilk test. Linearity / Monotonicity Assess linearity and monotonicity with a scatter plot. dat1 is plotted on the left in Figure 7.1. A second version that fails the linearity test is shown to the right. If the linear relationship assumption fails, consider transforming the variable instead of reverting to Spearman or Kendall. Figure 7.1: The left scatter plot is dat1. It meets Pearsons linearity condition. A second version at right illustrates what a failure might look like. The ordinal variable data set dat2 is plotted in Figure 7.2. Figure 7.2: dat2 meets the mononicity assumption for Spearmans rho and Kendalls tau. No Outliers Pearsons correlation requires no outliers. Both plots in Figure 7.1 are free of outliers. If there were outliers, check whether they are data entry errors or measurement errors and fix or discard them. If the outliers are genuine, leave them in if they do not affect the conclusion. You can also try tranforming the variable. Failing all that, revert to the Spearmans rho or Kendalls tau. Bivariate Normality Bivariate normality is difficult to assess. If two variables are bivariate normal, they will each be individually normal as well. Thats the best you can hope to check for. Use the Shapiro-Wilk test. shapiro.test(cs1$dat1$time_tv) ## ## Shapiro-Wilk normality test ## ## data: cs1$dat1$time_tv ## W = 0.97989, p-value = 0.1304 shapiro.test(cs1$dat1$cholesterol) ## ## Shapiro-Wilk normality test ## ## data: cs1$dat1$cholesterol ## W = 0.97594, p-value = 0.06387 If a variable is not normally distributed, you can transform it, carry on regardless since the Pearson correlation is fairly robust to deviations from normality, or revert to Spearman and Kendall. Test Calculate Pearsons correlation, Spearmans rho, or Kendalls tau. dat meets the assumptions for Pearsons correlation, but try Spearmans rho and Kendalls tau too, just to see how close they come to Pearson. dat2 only meets the assumptions for Spearman and Kendall. Pearsons Correlation dat1 met the conditions for Pearsons correlation. (cs1$cc_pearson &lt;- cor.test(cs1$dat1$cholesterol, cs1$dat1$time_tv, method = &quot;pearson&quot;) ) ## ## Pearson&#39;s product-moment correlation ## ## data: cs1$dat1$cholesterol and cs1$dat1$time_tv ## t = 3.9542, df = 98, p-value = 0.0001451 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.1882387 0.5288295 ## sample estimates: ## cor ## 0.3709418 r = 0.37 falls in the range of a moderate linear relationship. \\(r^2\\) = 14% is the coefficient of determination. Interpret it as the percent of variability in one variable that is explained by the other. If you are not testing a hypothesis (HO: \\(\\rho \\ne 0\\)), you can report just report r. Otherwise, include the p-value. Report your results like this: A Pearsons product-moment correlation was run to assess the relationship between cholesterol concentration and daily time spent watching TV in males aged 45 to 65 years. One hundred participants were recruited. Preliminary analyses showed the relationship to be linear with both variables normally distributed, as assessed by Shapiro-Wilks test (p &gt; .05), and there were no outliers. There was a statistically significant, moderate positive correlation between daily time spent watching TV and cholesterol concentration, r(98) = 0.37, p &lt; .0005, with time spent watching TV explaining 14% of the variation in cholesterol concentration. You wouldnt use Spearmans rho or Kendalls tau here since the more precise Pearsons correlation is available. But just out of curiosity, here are the correlations using those two measures. cor(cs1$dat1$cholesterol, cs1$dat1$time_tv, method = &quot;kendall&quot;) ## [1] 0.2383971 cor(cs1$dat1$cholesterol, cs1$dat1$time_tv, method = &quot;spearman&quot;) ## [1] 0.3322122 Both Kendall and Spearman produced more conservative estimates of the strength of the relationship - Kendall especially so. You probably wouldnt use linear regression here either because it describes the linear relationship between a response variable and changes to an independent explanatory variable. Even though we are reluctant to interpret a regression model in terms of causality, that is what is implied the formulation y ~ x and independence assumption in of X. Nevertheless, correlation and regression are related. The slope term in a simple linear regression of the normalized values equals the Pearson correlation. lm( y ~ x, data = cs1$dat1 %&gt;% mutate(y = scale(cholesterol), x = scale(time_tv)) ) ## ## Call: ## lm(formula = y ~ x, data = cs1$dat1 %&gt;% mutate(y = scale(cholesterol), ## x = scale(time_tv))) ## ## Coefficients: ## (Intercept) x ## -3.092e-16 3.709e-01 Spearmans Rho Data set dat2 did not meet the conditions for Pearsons correlation, so use Spearmans rho and/or Kendalls tau. Start with Spearmans rho. Recall that Spearmans rho is just the Pearson correlation applied to the ranks. Recall also that the Pearsons correlation is just the covariance divided by the product of the standard deviations. You can quickly calculate it by hand. cov(rank(cs1$dat2$income), rank(cs1$dat2$tax_too_high)) / (sd(rank(cs1$dat2$income)) * sd(rank(cs1$dat2$tax_too_high))) ## [1] 0.6024641 Use the function though. I dont get why cor.test requires x and y be numeric. (cs1$spearman &lt;- cor.test( as.numeric(cs1$dat2$tax_too_high), as.numeric(cs1$dat2$income), method = &quot;spearman&quot;) ) ## Warning in cor.test.default(as.numeric(cs1$dat2$tax_too_high), ## as.numeric(cs1$dat2$income), : Cannot compute exact p-value with ties ## ## Spearman&#39;s rank correlation rho ## ## data: as.numeric(cs1$dat2$tax_too_high) and as.numeric(cs1$dat2$income) ## S = 914.33, p-value = 0.001837 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.6024641 Interpret the statistic using the same rule of thumb as for Pearsons correlation. A rho over .5 is a strong correlation. A Spearmans rank-order correlation was run to assess the relationship between income level and views towards income taxes in 24 participants. Preliminary analysis showed the relationship to be monotonic, as assessed by visual inspection of a scatterplot. There was a statistically significant, strong positive correlation between income level and views towards income taxes, \\(r_s\\) = 0.602, p = 0.002. Kendalls Tau Recall that Kendalls tau is a function of concordant (C), discordant (D) and tied (Tx and Ty) pairs of observations. The manual calculation is a little more involved. Here is the function instead. (cs1$kendall &lt;- cor.test( as.numeric(cs1$dat2$tax_too_high), as.numeric(cs1$dat2$income), method = &quot;kendall&quot;) ) ## Warning in cor.test.default(as.numeric(cs1$dat2$tax_too_high), ## as.numeric(cs1$dat2$income), : Cannot compute exact p-value with ties ## ## Kendall&#39;s rank correlation tau ## ## data: as.numeric(cs1$dat2$tax_too_high) and as.numeric(cs1$dat2$income) ## z = 2.9686, p-value = 0.002991 ## alternative hypothesis: true tau is not equal to 0 ## sample estimates: ## tau ## 0.5345225 As with the case study on cholesterol and television, Kendalls tau was more conservative than Spearmans rho. \\(\\tau_b\\) = 0.535 is still in the strong range, though just barely. A Kendalls tau-b correlation was run to assess the relationship between income level and views towards income taxes amongst 24 participants. Preliminary analysis showed the relationship to be monotonic, as assessed by visual inspection of a scatterplot. There was a statistically significant, strong positive correlation between income level and the view that taxes were too high, \\(\\tau_b\\) = 0.535, p = 0.003. "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

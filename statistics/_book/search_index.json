[["bayesian-statistics.html", "Chapter 2 Bayesian Statistics 2.1 Bayes Law 2.2 Bayesian Inference 2.3 Discrete Cases 2.4 Continuous Cases 2.5 Bayes Factors 2.6 A Gentler Introduction 2.7 Coursera Notes Inference", " Chapter 2 Bayesian Statistics Bayesian inference estimates the probability, \\(\\theta\\), that an hypothesis is true. It differs from Frequentist inference in its insistence that all uncertainties be described by probabilities. Bayesian inference updates the prior probability distribution in light of new information. Bayesian inference builds on Bayes Law, so lets start there. 2.1 Bayes Law Bayes Law is a clever re-ordering of the relationship between joint probability and conditional probability, \\(P(\\theta D) = P(\\theta|D)P(D) = P(D|\\theta)P(\\theta)\\), into \\[P(\\theta|D) = \\frac{P(D|\\theta)P(\\theta)}{P(D)}.\\] \\(P(\\theta)\\) is the strength of your belief in \\(\\theta\\) prior to considering the data \\(D\\). \\(P(D|\\theta)\\) is the likelihood of observing \\(D\\) from a generative model with parameter \\(\\theta\\). Note that likelihood is the probability density, and is not quite the same as probability. For a continuous variable, likelihoods can sum to greater than 1. E.g., dbinom(seq(1, 100, 1), 100, .5) sums to 1, but dnorm(seq(0,50,.001), 10, 10) sums to 841. \\(P(D)\\) is the likelihood of observing \\(D\\) from any prior. It is the marginal distribution, or prior predictive distribution of \\(D\\). The likelihood divided by the marginal distribution is the proportional adjustment made to the prior in light of the data. \\(P(\\theta|D)\\) is the strength of your belief in \\(\\theta\\) posterior to considering \\(D\\). Bayes Law is useful for evaluating medical tests. A tests sensitivity is its probability of yielding a positive result \\(D\\) when condition \\(\\theta\\) exists. \\(P(D|\\theta)\\) is a test sensitivity, \\(\\mathrm{sens}\\). \\(P(\\theta)\\) is the probability of \\(\\theta\\) prior to the test (e.g., the general rate in society), \\(\\mathrm{prior}\\). The numerator of Bayes Law, the joint probability \\(P(D \\theta) = P(D|\\theta)P(\\theta)\\), is \\(\\mathrm{sens\\times prior}\\). A tests specificity is the probability of observing negative test result \\(\\hat{D}\\) when the condition does not exist, \\(\\hat{\\theta}\\). The specificity is the compliment of a false positive test result, \\(P(\\hat{D} | \\hat{\\theta}) = 1 - P(D | \\hat{\\theta})\\). The denominator of Bayes Law is the overall probability of a positive test result, \\(P(D) = P(D|\\theta)P(\\theta) + P(D|\\hat\\theta)P(\\hat\\theta)\\) or in terms of sensitivity and specificity, \\(P(D) = \\mathrm{(sens \\times prior) + (1 - spec)(1 - prior)}\\). Example. Suppose E. Coli is typically present in 4.5% of samples, and an E. Coli screen has a sensitivity of 0.95 and a specificity of 0.99. Given a positive test result, what is the probability that E. Coli is actually present? \\[P(\\theta|D) = \\frac{.95\\cdot .045}{.95\\cdot .045 + (1 - .99)(1 - .045)} = \\frac{.04275}{.04275 + .00955} = \\frac{.04275}{.05230} = 81.7\\%.\\] The elements of Bayes Law come directly from the contingency table. The first row is the positive test result. The probability of E. Coli is the joint probability of E. coli and a positive test divided by the probability of a positive test E. Coli Safe Total + Test .95 * .045 = 0.04275 .01 * .955 = 0.00955 0.05230 - Test .05 * .045 = 0.00225 .99 * .955 = 0.94545 0.94770 Total 0.04500 0.95500 1.00000 2.2 Bayesian Inference Bayesian inference extends the logic of Bayes Law by replacing the prior probability estimate that \\(\\theta\\) is true with a prior probability distribution that \\(\\theta\\) is true. Rather than saying, I am x% certain \\(\\theta\\) is true, you are saying I believe the probability that \\(\\theta\\) is true is somewhere in a range that has maximum likelihood at x%. Let \\(\\Pi(\\theta)\\) be the prior probability function of \\(\\theta\\). \\(\\Pi(\\theta)\\) has a pmf or pdf \\(P(\\theta)\\), and a set of conditional distributions called the generative model for the observed data \\(D\\) given \\(\\theta\\), \\(\\{f_\\theta(D): \\theta \\in \\Omega\\}\\). \\(f_\\theta(D)\\) is the likelihood of observing \\(D\\) given \\(\\theta\\). Their product, \\(f_\\theta(D)P(\\theta)\\), is a joint distribution of \\((D, \\theta)\\). For continuous prior distributions, the marginal distribution for \\(D\\), called the prior predictive distribution, is \\[m(D) = \\int_\\Omega f_\\theta(D)P(\\theta) d\\theta\\] For discrete prior distributions, replace the integral with a sum, \\(m(D) = \\sum\\nolimits_\\Omega f_\\theta(D) P(\\theta)\\). The posterior probability distribution of \\(\\theta\\), conditioned on the observance of \\(D\\), is \\(\\Pi(\\cdot|D)\\). It is the joint density, \\(f_\\theta(D) P(\\theta),\\) divided by the the marginal density, \\(m(D)\\). \\[P(\\theta | D) = \\frac{f_\\theta(D) P(\\theta)}{m(D)}\\] The numerator makes the posterior proportional to the prior. The denominator is a normalizing constant that scales the likelihood into a proper density function (whose values sum to 1). It is helpful to look first at discrete priors, a list of competing priors to see how the observed evidence shifts the probabilities of the priors into their posterior probabilities. From there it is a straight-forward step to the more abstract case of continuous prior and posterior distributions. 2.3 Discrete Cases Suppose you have a string of numbers \\([1,1,1,1,0,0,1,1,1,0]\\) (7 ones and 3 zeros) produced by a Bernoulli random number generator. What parameter \\(p\\) was used in the Bernoulli function?1 D &lt;- c(1, 1, 1, 1, 0, 0, 1, 1, 1, 0) Your best guess is \\(p = 0.7\\), but how confident are you? Posit eleven competing priors, \\(\\theta = [.0, .1, \\ldots, 1]\\) with equal prior probabilities, \\(P(\\theta) = [1/11, \\ldots]\\). theta &lt;- seq(0, 1, by = 0.1) prior &lt;- rep(1/11, 11) Using a Bernoulli generative model, the likelihood of observing 7 ones and 3 zeros are \\(P(D|\\theta) = \\theta^7 + (1-\\theta)^3.\\) likelihood &lt;- theta^7 * (1 - theta)^3 data.frame(theta, likelihood) %&gt;% ggplot() + geom_segment(aes(x = theta, xend = theta, y = 0, yend = likelihood), linetype = 2, color = &quot;steelblue&quot;) + geom_point(aes(x = theta, y = likelihood), color = &quot;steelblue&quot;, size = 3) + scale_x_continuous(breaks = theta) + theme_minimal() + theme(panel.grid.minor = element_blank()) + labs(title = expression(paste(&quot;Maximum likelihood of observing D is at &quot;, theta, &quot; = 0.7.&quot;)), x = expression(theta), y = expression(f[theta](D))) The posterior probability is the likelihood divided by the marginal probability of observing \\(D\\) multiplied by the prior, \\(P(\\theta|D) = \\frac{P(D|\\theta)}{P(D)}\\cdot P(\\theta).\\) In this case, the marginal probability is straight-forward to calculate: it is the sum-product of the priors and their associated likelihoods. posterior &lt;- likelihood / sum(likelihood * prior) * prior What would the posterior look like if we started with an educated guess on \\(P(\\theta)\\) that more heavily weights \\(\\theta = 0.7\\)? prior &lt;- c(.05, .05, .05, .05, .05, .10, .15, .20, .15, .10, .05) posterior &lt;- likelihood / sum(likelihood * prior) * prior What if we now employ a larger data set? To see, generate a sample of 100 Bernoulli(.7) observations. D100 &lt;- rbernoulli(100, p = 0.7) %&gt;% as.numeric() likelihood &lt;- theta^sum(D100) * (1 - theta)^(length(D100)-sum(D100)) posterior &lt;- likelihood / sum(likelihood * prior) * prior What would it look like if it also had more competing hypotheses, \\(\\theta \\in (0, .01, .02, \\ldots, 1)\\). theta &lt;- seq(0, 1, by = .01) prior &lt;- rep(1/100, 101) likelihood &lt;- theta^sum(D100) * (1 - theta)^(length(D100)-sum(D100)) posterior &lt;- likelihood / sum(likelihood * prior) * prior 2.4 Continuous Cases Continuing the example of inferring the parameter \\(p\\) used in the Bernoulli process, what if we considered all values between 0 and 1?2 When prior beliefs are best described in continuous distributions, express them using the beta, gamma, or normal distribution so that the posterior distributions are conjugates of the prior distributions with new parameter values. Otherwise, the marginal distribution is difficult to calculate. In this case, use the beta distribution, described by shape parameters, \\(\\alpha\\) and \\(\\beta\\). \\[P(\\theta|D,\\alpha,\\beta) = \\frac{f_\\theta(D) P(\\theta|\\alpha,\\beta)}{\\int_0^1 f_\\theta(D)P(\\theta|\\alpha, \\beta)d\\theta}\\] As with the discrete case, the numerator is the likelihood of observing \\(D\\) if \\(\\theta\\) is true multiplied by the prior probability, but now the prior is a Beta(\\(\\alpha\\), \\(\\beta\\)) distribution. The denominator, sometimes called the evidence, is the marginal probability of \\(D\\). The likelihood of observing \\(D\\) = \\(a\\) successes and \\(b\\) non-successes given a success probability of \\(p\\) = \\(\\theta\\) is \\[f_\\theta(D) = \\theta^a(1-\\theta)^b\\] The prior distribution is the probability density function of the beta distribution \\[P(\\theta|\\alpha,\\beta) = \\frac{1}{\\mathrm{B}(\\alpha, \\beta)}\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}\\] where \\(\\mathrm{B}(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)}\\) is the beta function. The marginal distribution is \\[\\int_0^1 f_\\theta(D)P(\\theta|\\alpha, \\beta)d\\theta = \\frac{\\mathrm{B}(\\alpha + a, \\beta + b)}{\\mathrm{B}(\\alpha, \\beta)}\\] Putting this all together, the posterior distribution is \\[P(\\theta|D, \\alpha, \\beta) = \\frac{1}{\\mathrm{B}(\\alpha + a, \\beta + b)} \\theta^{\\alpha-1+a}(1-\\theta)^{\\beta-1+b}\\] The posterior equals the prior with shape parameters incremented by the observed counts, \\(a\\) and \\(b.\\) plot_bayes &lt;- function(alpha, beta, a, b) { prior_ev &lt;- (alpha / (alpha + beta)) %&gt;% round(2) posterior_ev &lt;- ((alpha + a) / (alpha + beta + a + b)) %&gt;% round(2) dat &lt;- data.frame(theta = seq(0, 1, by = .01)) %&gt;% mutate(prior = (1 / beta(alpha, beta)) * theta^(alpha-1) * (1-theta)^(beta-1), prior_ci = theta &gt; qbeta(.025, alpha, beta) &amp; theta &lt; qbeta(.975, alpha, beta), likelihood = theta^a * (1-theta)^b, posterior = (1 / beta(alpha + a, beta + b)) * theta^(alpha-1+a) * (1-theta)^(beta-1+b), posterior_ci = theta &gt; qbeta(.025, alpha + a, beta + b) &amp; theta &lt; qbeta(.975, alpha + a, beta + b)) p_prior &lt;- dat %&gt;% ggplot(aes(x = theta, y = prior)) + geom_line(color = &quot;steelblue&quot;) + geom_area(aes(alpha = prior_ci), fill = &quot;steelblue&quot;) + geom_vline(xintercept = prior_ev, color = &quot;steelblue&quot;) + scale_alpha_manual(values = c(.1, .5)) + theme_minimal() + theme(legend.position = &quot;none&quot;) + labs(x = NULL) p_likelihood &lt;- dat %&gt;% ggplot(aes(x = theta, y = likelihood)) + geom_line(color = &quot;steelblue&quot;) + theme_minimal() + theme(legend.position = &quot;none&quot;) + labs(x = NULL) p_posterior &lt;- dat %&gt;% ggplot(aes(x = theta, y = posterior)) + geom_line(color = &quot;steelblue&quot;) + geom_area(aes(alpha = posterior_ci), fill = &quot;steelblue&quot;) + geom_vline(xintercept = posterior_ev, color = &quot;steelblue&quot;) + scale_alpha_manual(values = c(.1, .5)) + theme_minimal() + theme(legend.position = &quot;none&quot;) + labs(x = expression(theta)) out &lt;- p_prior / p_likelihood / p_posterior + plot_annotation( title = glue(&quot;Beta({alpha}, {beta}) prior with observed evidence a = {a} &quot;, &quot;and b = {b}&quot;), subtitle = &quot;with shaded 95% credible interval.&quot;, caption = glue(&quot;Prior expected value = {prior_ev}; Posterior expected &quot;, &quot;value = {posterior_ev}&quot;)) out } Suppose you claim complete ignorance and take a uniform Beta(1, 1) prior. Recall that you observed a = 7 ones and b = 3 zeros. The posterior expected value is still pretty close! plot_bayes(alpha = 10, beta = 10, a = 7, b = 3) Suppose you had prior reason to believe p = 0.7. You would model that as \\(\\alpha\\) = 7, \\(\\beta\\) = 3. The prior probability distribution would be \\(P(\\theta|\\alpha = 7,\\beta = 3) = \\frac{1}{\\mathrm{B}(7, 3)}\\theta^{7-1}(1-\\theta)^{3-1}\\). Then after observing a = 7 ones and b = 3 zeros, the posterior probability distribution would be \\(P(\\theta|\\alpha = 7+7,\\beta = 3+3) = \\frac{1}{\\mathrm{B}(7+7, 3+3)}\\theta^{7+7-1}(1-\\theta)^{3+3-1}\\). plot_bayes(alpha = 7, beta = 3, a = 7, b = 3) 2.5 Bayes Factors The Bayes Factor (BF) is a measure of the relative evidence of one model over another. Take another look at Bayes formula: \\[P(\\theta|D) = \\frac{P(D|\\theta)P(\\theta)}{P(D)}.\\] Suppose you want to compare two models, \\(\\theta_1\\) and \\(\\theta_2\\). If you take the ratio of the posterior probabilities, the posterior odds, the \\(P(D)\\) terms cancel and you have \\[\\frac{P(\\theta_1|D)}{P(\\theta_2|D)} = \\frac{P(D|\\theta_1)}{P(D|\\theta_2)} \\cdot \\frac{P(\\theta_1)}{P(\\theta_2)}\\] The posterior odds equals the ratio of the likelihoods multiplied by the prior odds. That likelihood ratio is the Bayes Factor (BF). Rearranging, BF is the odds ratio of the posterior and prior odds. \\[BF = \\frac{P(D|\\theta_1)}{P(D|\\theta_2)} = \\mathrm{\\frac{Posterior Odds}{Prior Odds}}\\] Return to the example of observing \\(D\\) = 7 ones and 3 zeros. You can compare an hypothesized \\(\\theta\\) of .5 to a completely agnostic model where \\(\\theta\\) is uniform over [0, 1]. The likelihood of observing \\(D\\) when \\(\\theta\\) = .5 is \\(P(D|\\theta_1) = \\binom{10}{3}.5^7(1-.5)^3\\) = 0.117. The likelihood of observing \\(D\\) where \\(\\theta\\) is uniform on [0, 1] is \\(P(D|\\theta_2) = \\int_0^1 \\binom{10}{3}q^7(1-q)^3dq\\) with a uniform Beta(1, 1) prior (i.e., complete agnosticism). The Bayes factor at \\(\\theta\\) = .7 quantifies how much the odds of H0: \\(\\theta\\) = .7 over H1: \\(\\hat{\\theta}\\) = .7. prior &lt;- function(theta, alpha, beta) { (1 / beta(alpha, beta)) * theta^(alpha-1) * (1-theta)^(beta-1) } posterior &lt;- function(theta, alpha, beta, a, b) { (1 / beta(alpha + a, beta + b)) * theta^(alpha-1+a) * (1-theta)^(beta-1+b) } prior(.5, 115, 85) ## [1] 1.164377 posterior(.5, 1, 1, 10, 10) ## [1] 3.700138 posterior(.5, 1, 1, 10, 10) / prior(.5, 1, 1) ## [1] 3.700138 1 / beta(115, 85) ## [1] 4.677704e+59 The Bayes factor measures how much your prior belief is altered by the evidence. It is the ratio of the likelihoods at some hypothesized value before and after observing the data. In this case, our confidence increased by a factor of theta &lt;- 0.5 alpha &lt;- 1 beta &lt;- 1 a &lt;- 10 b &lt;- 10 (prior_likelihood &lt;- (1 / beta(alpha, beta)) * theta^(alpha-1) * (1-theta)^(beta-1)) ## [1] 1 (posterior_likelihood &lt;- (1 / beta(alpha + a, beta + b)) * theta^(alpha-1+a) * (1-theta)^(beta-1+b)) ## [1] 3.700138 (bayes_factor &lt;- posterior_likelihood / prior_likelihood) ## [1] 3.700138 # 3.7 on alpha = beta = 1 # 1.91 on alpha = beta = 4 2.6 A Gentler Introduction This section is my notes from DataCamp course Fundamentals of Bayesian Data Analysis in R. It is an intuitive approach to Bayesian inference. Suppose you purchase 100 ad impressions on a web site and receive 13 clicks. How would you describe the click rate? The Frequentist approach would be to construct a 95% CI around the click proportion. (ad_prop_test &lt;- prop.test(13, 100)) ## ## 1-sample proportions test with continuity correction ## ## data: 13 out of 100, null probability 0.5 ## X-squared = 53.29, df = 1, p-value = 2.878e-13 ## alternative hypothesis: true p is not equal to 0.5 ## 95 percent confidence interval: ## 0.07376794 0.21560134 ## sample estimates: ## p ## 0.13 How might you model this using Bayesian reasoning? One way is to run 1,000 experiments that sample 100 ad impression events from an rbinom() generative model using a uniform prior distribution of 0-30% click probability. The resulting 1,000 row data set of click probabilities and sampled click counts forms a joint probability distribution. This method of Bayesian analysis is called rejection sampling because you sample across the whole parameter space, then condition on the observed evidence. df_sim &lt;- data.frame(click_prob = runif(1000, 0.0, 0.3)) df_sim$click_n &lt;- rbinom(1000, 100, df_sim$click_prob) Condition the joint probability distribution on the 13 observed clicks to update your prior. The quantile() function returns the median and the .025 and .975 percentile values - the credible interval. # median and credible interval (sim_ci &lt;- df_sim %&gt;% filter(click_n == 13) %&gt;% pull(click_prob) %&gt;% quantile(c(.025, .5, .975))) ## 2.5% 50% 97.5% ## 0.07886203 0.14034103 0.18831995 Your posterior click rate likelihood is 14.0% with 95 credible interval [7.9%, 18.8%]. Here is the density plot of the 36 simulations that produced the 13 clicks. The median and 95% credible interval are marked. Thats pretty close to the frequentist result! Instead of running 1,000 experiments with randomly selected click probabilities and randomly selected click counts based on those probabilities, you could define a discrete set of candidate click probabilities, e.g. values between 0 and 0.3 incremented by .01, and calculate the click probability density for the 100 ad impressions. This method of Bayesian analysis is called grid approximation. df_bayes &lt;- expand.grid( click_prob = seq(0, .3, by = .001), click_n = 0:100 ) %&gt;% mutate( prior = dunif(click_prob, min = 0, max = 0.3), likelihood = dbinom(click_n, 100, click_prob), probability = likelihood * prior / sum(likelihood * prior) ) Condition the joint probability distribution on the 13 observed clicks to update your prior. df_bayes_13 &lt;- df_bayes %&gt;% filter(click_n == 13) %&gt;% mutate(posterior = probability / sum(probability)) Instead of using the quantile() function on these values to measure the median and credible interval, resample the posterior probability to create a distribution. sampling_idx &lt;- sample( 1:nrow(df_bayes_13), size = 10000, replace = TRUE, prob = df_bayes_13$posterior ) sampling_vals &lt;- df_bayes_13[sampling_idx, ] (df_bayes_ci &lt;- quantile(sampling_vals$click_prob, c(.025, .5, .975))) ## 2.5% 50% 97.5% ## 0.078 0.135 0.211 You can use a Bayesian model to estimate multiple parameters. Suppose you want to predict the water temperature in a lake on Jun 1 based on 5 years of prior water temperatures. temp &lt;- c(19, 23, 20, 17, 23) You model the water temperature as a normal distribution, \\(\\mathrm{N}(\\mu, \\sigma^2)\\) with a prior distribution \\(\\mu = \\mathrm{N}(18, 5^2)\\) and \\(\\sigma = \\mathrm{unif}(0, 10)\\) based on past experience. Using the grid approximation approach, construct a grid of candidate \\(\\mu\\) values from 8 to 30 degrees incremented by .5 degrees, and candidate \\(\\sigma\\) values from .1 to 10 incremented by .1 - a 4,500 row data frame. mdl_grid &lt;- expand_grid(mu = seq(8, 30, by = 0.5), sigma = seq(.1, 10, by = 0.1)) For each combination of \\(\\mu\\) and \\(\\sigma\\), the prior probabilities are the densities from \\(\\mu = \\mathrm{N}(18, 5^2)\\) and \\(\\sigma = \\mathrm{unif}(0, 10)\\). The combined prior is their product. The likelihoods are the products of the probabilities of observing each temp given the candidate \\(\\mu\\) and \\(\\sigma\\) values. mdl_grid_2 &lt;- mdl_grid %&gt;% mutate( mu_prior = map_dbl(mu, ~dnorm(., mean = 18, sd = 5)), sigma_prior = map_dbl(sigma, ~dunif(., 0, 10)), prior = mu_prior * sigma_prior, # combined prior, likelihood = map2_dbl(mu, sigma, ~dnorm(temp, .x, .y) %&gt;% prod()), posterior = likelihood * prior / sum(likelihood * prior) ) Calculate a credible interval by drawing 10,000 samples from the grid with sampling probability equal to the calculated posterior probabilities. Use the quantile() function to estimate the median and .025 and .975 quantile values. sampling_idx &lt;- sample(1:nrow(mdl_grid), size = 10000, replace = TRUE, prob = mdl_grid$posterior) ## Warning: Unknown or uninitialised column: `posterior`. sampling_vals &lt;- mdl_grid[sampling_idx, c(&quot;mu&quot;, &quot;sigma&quot;)] mu_ci &lt;- quantile(sampling_vals$mu, c(.025, .5, .975)) sigma_ci &lt;- quantile(sampling_vals$sigma, c(.025, .5, .975)) ci &lt;- qnorm(c(.025, .5, .975), mean = mu_ci[2], sd = sigma_ci[2]) data.frame(temp = seq(0, 30, by = .1)) %&gt;% mutate(prob = map_dbl(temp, ~dnorm(., mean = ci[2], sd = sigma_ci[2])), ci = if_else(temp &gt;= ci[1] &amp; temp &lt;= ci[3], &quot;Y&quot;, &quot;N&quot;)) %&gt;% ggplot(aes(x = temp, y = prob)) + geom_area(aes(y = if_else(ci == &quot;N&quot;, prob, 0)), fill = &quot;firebrick&quot;, show.legend = FALSE) + geom_line() + geom_vline(xintercept = ci[2], linetype = 2) + theme_minimal() + scale_x_continuous(breaks = seq(0, 30, 5)) + theme(panel.grid.minor = element_blank()) + labs(title = &quot;Posterior temperature probability&quot;, subtitle = glue(&quot;mu = {ci[2] %&gt;% scales::number(accuracy = .1)}, 95%-CI (&quot;, &quot;{ci[1] %&gt;% scales::number(accuracy = .1)}, &quot;, &quot;{ci[3] %&gt;% scales::number(accuracy = .1)})&quot;)) What is the probability the temperature is at least 18? pred_temp &lt;- rnorm(1000, mean = sampling_vals$mu, sampling_vals$sigma) scales::percent(sum(pred_temp &gt;= 18) / length(pred_temp)) ## [1] &quot;57%&quot; 2.7 Coursera Notes Inference Bayesian inference updates prior beliefs with accumulated evidence. The posterior odds equals the prior odds multiplied by the likelihood ratio of observing evidence under the competing hypotheses. \\[\\frac{P(H1|D)}{P(H0|D)} = \\frac{P(D|H1)}{P(D|H0)} \\times \\frac{P(H1)}{P(H0)}\\] (*This isnt quite what I expected. Wouldnt you replace the likelihood ratio, \\(\\frac{P(D|H1)}{P(D|H0)}\\) with the proportional adjustment, \\(\\frac{P(D|H1)}{P(D)}\\)?) Express the prior https://www.bayesrulesbook.com/ Example borrowed from chris sandbox Chriss Sandbox again. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

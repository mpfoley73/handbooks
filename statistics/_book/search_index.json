[["two-group-comparison-tests.html", "Chapter 2 Two-Group Comparison Tests 2.1 Independent Samples t-Test 2.2 2 Sample Welchs t Test for Numeric Vars 2.3 Paired Sample t Test for Numeric Vars 2.4 Mann-Whitney U Test for Numeric Vars 2.5 Wilcoxon Paired-Sample Test for Numeric Vars 2.6 Pearsons Correlation for Numeric Vars 2.7 Spearmans Ranked Correlation for Numeric Vars 2.8 2 Sample Independent t Test for Categorical Vars 2.9 2 Sample Welchs t Test for Categorical Vars 2.10 Paired Sample t Test for Categorical Vars 2.11 Mann-Whitney U Test for Categorical Vars 2.12 Wilcoxon Paired-Sample Test for Categorical Vars 2.13 Pearsons Correlation for Categorical Vars 2.14 Spearmans Ranked Correlation for Categorical Vars 2.15 Fishers Exact Test", " Chapter 2 Two-Group Comparison Tests Use independent samples tests to either describe a variables frequency or central tendency difference between two independent groups, or to compare the difference to a hypothesized value. If the data generating process produces continuous outcomes (interval or ratio) and the outcomes are symmetrically distributed, the difference in the sample means, \\(\\hat{d} = \\bar{x} - \\bar{y}\\), is a random variable centered at the population difference, \\(d = \\mu_X - \\mu_Y\\). You can use a theoretical distribution (normal or student t) to estimate a 95% confidence interval (CI) around \\(d\\), or compare \\(\\hat{d}\\) to an hypothesized population difference, \\(d_0\\). If you (somehow) know the sampling distribution variances \\(\\sigma^2_X\\) and \\(\\sigma^2_Y\\), or the Central Limit Theorem (CLT) conditions hold, you can assume the random variable is normally distributed and use the z-test, otherwise assume the random variable has a student t distribution and use the t-test.1 If the data generating process produces continuous outcomes that are not symmetrically distributed, use a non-parametric test like the Mann-Whitney U test. If the data generating process produces discrete outcomes (counts), the sample count, \\(x\\), is a random variable from a Poisson, binomial, normal, or multinomial distribution, or a random variable from a theoretical outcome. For two independent samples, the data can be organized into a two-way table - a frequency table for two categorical variables. If you have a single categorical predictor variable, you can test whether the joint frequency counts differ from the expected frequency counts in the saturated model. You analyze a two-way table one of two ways. If you only care about comparing two levels (like when the response variable is binary), conduct a proportion difference z-test or a Fisher exact test. If you want to compare the joint frequency counts to expected frequency counts under the independence model (the model of independent explanatory variables), conduct a Pearsons chi-squared independence test, or a G-test. 2.1 Independent Samples t-Test If a population measure X is normally distributed with mean \\(\\mu_X\\) and variance \\(\\sigma_X^2\\), and population measure Y is normally distributed with mean \\(\\mu_Y\\) and variance \\(\\sigma_Y^2\\), then their difference is normally distributed with mean \\(d = \\mu_X - \\mu_Y\\) and variance \\(\\sigma_{XY}^2 = \\sigma_X^2 + \\sigma_Y^2\\). By the CLT, as the sample sizes grow, non-normally distributed X and Y will approach normality, and so will their difference. The independent samples t-test uses the difference in sample means \\(\\hat{d} = \\bar{x} - \\bar{y}\\) as an estimate of \\(d\\) to evaluate an hypothesized difference, \\(d_0\\). The null hypothesis is \\(d = d_0\\). Alternatively, you can construct a \\((1 - \\alpha)\\%\\) confidence interval around \\(\\hat{d}\\) to estimate \\(d\\) within a margin of error, \\(\\epsilon\\). In principal, you can test the difference between independent means with either a z test or a t test. Both require independent samples and approximately normal sampling distributions. The sampling distributions are normal if the underlying populations are normally distributed, or if the sample sizes are large (\\(n_X\\) and \\(n_Y\\) \\(\\ge\\) 30). However, the z-test additionally requires known sampling distribution variances \\(\\sigma^2_X\\) and \\(\\sigma^2_Y\\). These variances are never known, so always use the t-test. The z-test assumes \\(d\\) has a normal distribution centered at \\(\\hat{d} = d\\) with standard error \\(se = \\sqrt{\\frac{\\sigma_X^2}{n_X} + \\frac{\\sigma_Y^2}{n_Y}}.\\) Test \\(H_0: d = d_0\\) with test statistic \\(Z = \\frac{\\hat{d} - d_0}{se}\\) or define a \\((1 - \\alpha)\\%\\) confidence interval as \\(d = \\hat{d} \\pm z_{(1 - \\alpha {/} 2)} se\\). The t-test assumes \\(d\\) has a t distribution centered at \\(\\hat{d} = d\\) with standard error \\(se = \\sqrt{\\frac{s_X^2}{n_X} + \\frac{s_Y^2}{n_Y}}.\\) Test \\(H_0: d = d_0\\) with test statistic \\(T = \\frac{\\hat{d} - d_0}{se}\\), or define a \\((1 - \\alpha)\\%\\) confidence interval as \\(d = \\hat{d} \\pm t_{(1 - \\alpha / 2), (n_X + n_Y - 2)} se\\). However, there is an issue with the t test degrees of freedom. If the sample sizes are small, and the standard deviations from each population are similar (the ratios of \\(s_X\\) and \\(s_Y\\) are &lt;2), you can pool the variances \\(s_p^2 = \\frac{(n_X - 1) s_X^2 + (n_Y-1) s_Y^2}{n_X + n_Y-2}\\) so that \\(se = s_p \\sqrt{\\frac{1}{n_X} + \\frac{1}{n_Y}}\\) and \\(df = n_X + n_Y -2\\). This is call the pooled variances t-test. Otherwise, \\(se = \\sqrt{\\frac{s_X^2}{n_X} + \\frac{s_Y^2}{n_Y}}\\), but you must reduce the degrees of freedom using the Welch-Satterthwaite correction, \\(df = \\frac{\\left(\\frac{s_X^2}{n_X} + \\frac{s_Y^2}{n_Y}\\right)^2}{\\frac{s_X^4}{n_X^2\\left(N_X-1\\right)} + \\frac{s_Y^4}{n_Y^2\\left(N_Y-1\\right)}}.\\) This is called the separate variance t-test, or Welchs t-test. There is a third way to perform a t test, the paired two-sample test. It uses the mean of sampled paired differences \\(\\bar{d} = \\sum_{i = 1}^n (x_i - y_i) {/} n\\) as an estimate of the mean of the population paired differences \\(\\delta\\) to evaluate the hypothesized mean of population paired differences \\(\\delta_0\\). Test \\(H_0: \\delta = \\delta_0\\) with test statistic \\(T = \\frac{\\bar{d} - \\delta_0}{se}\\), or define a \\((1 - \\alpha)\\%\\) confidence interval as \\(\\delta = \\bar{d} \\pm t_{1 - \\alpha / 2, n - 1} se\\). The paired t-test is really just a single mean t-test operating on variable that is defined as the difference between two variables. Example A company shows an advertisement to \\(n_M\\) = 20 males and \\(n_F\\) = 20 females, then measures their engagement with a survey. Do the mean engagement scores differ between the groups? engage %&gt;% gtsummary::tbl_summary(by = gender, statistic = list(all_continuous() ~ &quot;{mean} ({sd})&quot;)) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #ywymruitot .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #ywymruitot .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #ywymruitot .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #ywymruitot .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #ywymruitot .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #ywymruitot .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #ywymruitot .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #ywymruitot .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #ywymruitot .gt_column_spanner_outer:first-child { padding-left: 0; } #ywymruitot .gt_column_spanner_outer:last-child { padding-right: 0; } #ywymruitot .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #ywymruitot .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #ywymruitot .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #ywymruitot .gt_from_md > :first-child { margin-top: 0; } #ywymruitot .gt_from_md > :last-child { margin-bottom: 0; } #ywymruitot .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #ywymruitot .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #ywymruitot .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #ywymruitot .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #ywymruitot .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #ywymruitot .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #ywymruitot .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #ywymruitot .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #ywymruitot .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #ywymruitot .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #ywymruitot .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #ywymruitot .gt_sourcenote { font-size: 90%; padding: 4px; } #ywymruitot .gt_left { text-align: left; } #ywymruitot .gt_center { text-align: center; } #ywymruitot .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #ywymruitot .gt_font_normal { font-weight: normal; } #ywymruitot .gt_font_bold { font-weight: bold; } #ywymruitot .gt_font_italic { font-style: italic; } #ywymruitot .gt_super { font-size: 65%; } #ywymruitot .gt_footnote_marks { font-style: italic; font-size: 65%; } Characteristic Male, N = 201 Female, N = 201 engagement 5.56 (0.29) 5.30 (0.39) 1 Statistics presented: Mean (SD) You can report the following initially. There were 20 male and 20 female participants. The advertisement was more engaging to male viewers (M = 5.56, SD = 0.29) than female viewers (M = 5.30, SD = 0.39). 2.1.0.1 Conditions The independent samples t test applies when the variable is continuous, partitioned into two independent samples, and the observations are independent. Additionally, there are three conditions related to the data distribution. If either condition fails, try the suggested work-arounds or use the non-parametric Mann-Whitney U Test for Numeric Vars instead. Outliers. There should be no significant outliers in either group. Outliers exert a large influence on the mean and standard deviation. Test with a box plot. If there are outliers, you might be able to drop them or transform the data. Normality. Values should be nearly normally distributed (nearly because the t-test is robust to the normality assumption). This condition is especially important with small sample sizes. Test with Q-Q plots or the Shapiro-Wilk test for normality. If the data is very non-normal, you might be able to transform the data. Homogeneous Variances. If variances are homogenous, you can use the pooled-variances method; otherwise you use the separate variance method. Test with Levenes test of equality of variances. Outliers Assess outliers with a box plot. Box plot whiskers extend up to 1.5*IQR from the upper and lower hinges and outliers (beyond the whiskers) are are plotted individually. There were no outliers plotted here. Report this as There were no outliers in the data, as assessed by inspection of a boxplot. If the outliers are data entry errors or measurement errors, fix or discard them. If the outliers are genuine, you have a couple options before reverting to the Mann-Whitney U test. Leave it in if it doesnt affect the conclusion (compared to taking it out). Transform the variable. Dont do this unless the variable is also non-normal. Transformation also has the downside of making interpretation more difficult. Normality Assume the population is normally distributed if n \\(\\ge\\) 30. Otherwise, asses a Q-Q plot, skewness and kurtosis values, or a histogram. If you still dont feel confident about normality, run a Mann-Whitney U Test. The data set has \\(n_M\\) = 20 male and \\(n_F\\) = 20 female observations, so you cannot assume normality. Here is a QQ plot. The QQ plot indicates normality. engage %&gt;% ggplot(aes(sample = engagement, group = gender, color = fct_rev(gender))) + stat_qq() + stat_qq_line(col = &quot;goldenrod&quot;) + theme_minimal() + theme(legend.position = &quot;top&quot;) + labs(title = &quot;Normal Q-Q Plot&quot;, color = NULL) Here is the Shapiro-Wilk normality test. Both tests (of males and females) fail to reject the normality assumption. Report this as Engagement scores for each level of gender were normally distributed, as assessed by Shapiro-Wilks test (p &gt; .05). split(engage, engage$gender) %&gt;% map(~shapiro.test(.$engagement)) ## $Male ## ## Shapiro-Wilk normality test ## ## data: .$engagement ## W = 0.98344, p-value = 0.9705 ## ## ## $Female ## ## Shapiro-Wilk normality test ## ## data: .$engagement ## W = 0.96078, p-value = 0.5595 If the data is not normally distributed, you still have a couple options before reverting to the Mann-Whitney U test. Transform the dependent variable. Carry on regardless - the independent samples t-test is fairly robust to deviations from normality. Homogenous Variances If variances are homogenous, you use the pooled-variances method; otherwise you use the separate variance method. As a rule of thumb, homogenous variances have a ratio of standard deviations between 0.5 and 2.0. The data satisfies the rule of thumb: sd(engage %&gt;% filter(gender == &quot;Male&quot;) %&gt;% pull(engagement)) / sd(engage %&gt;% filter(gender == &quot;Female&quot;) %&gt;% pull(engagement)) ## [1] 0.7419967 You can also use the F test to compare the ratio of the sample variances \\(\\hat{r} = s_X^2 / s_Y^2\\) to an hypothesized ratio of population variances \\(r_0 = \\sigma_X^2 / \\sigma_Y^2\\), usually \\(1\\). var.test(engage %&gt;% filter(gender == &quot;Female&quot;) %&gt;% pull(engagement), engage %&gt;% filter(gender == &quot;Male&quot;) %&gt;% pull(engagement)) ## ## F test to compare two variances ## ## data: engage %&gt;% filter(gender == &quot;Female&quot;) %&gt;% pull(engagement) and engage %&gt;% filter(gender == &quot;Male&quot;) %&gt;% pull(engagement) ## F = 1.8163, num df = 19, denom df = 19, p-value = 0.2025 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.7189277 4.5888826 ## sample estimates: ## ratio of variances ## 1.816336 Bartletts test is a second option. bartlett.test(engage$engagement, engage$gender) ## ## Bartlett test of homogeneity of variances ## ## data: engage$engagement and engage$gender ## Bartlett&#39;s K-squared = 1.6246, df = 1, p-value = 0.2024 Compared to Bartletts test, Levenes is less sensitive to departures from normality. engage_report$levene &lt;- with(engage, car::leveneTest(engagement, gender, center = &quot;mean&quot;)) Report these results as There was homogeneity of variances for engagement scores for males and females, as assessed by Levenes test for equality of variances (p = 0.174). Results Conduct the t-test. To get a 95% CI around the difference (instead of around the estimate), run the test using the difference, \\(\\mu_0 - \\bar{x}\\), and leave mu at its default of 0. (engage_95ci &lt;- t.test( x = engage %&gt;% filter(gender == &quot;Male&quot;) %&gt;% pull(engagement), y = engage %&gt;% filter(gender == &quot;Female&quot;) %&gt;% pull(engagement), alternative = &quot;two.sided&quot;, mu = 0, paired = FALSE, var.equal = TRUE, conf.level = 0.95) ) ## ## Two Sample t-test ## ## data: engage %&gt;% filter(gender == &quot;Male&quot;) %&gt;% pull(engagement) and engage %&gt;% filter(gender == &quot;Female&quot;) %&gt;% pull(engagement) ## t = 2.3645, df = 38, p-value = 0.02327 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.03725546 0.48074454 ## sample estimates: ## mean of x mean of y ## 5.558875 5.299875 Report this as: There was a statistically significant difference in mean engagement score between males and females, with males scoring higher than females, 0.26 (95% CI, 0.04 to 0.48), t(38) = 2.365, p = 0.023. The effect size, called Cohens d, is defined as \\(d = |M_D| / s\\), where \\(|M_D| = \\bar{x} - \\bar{y}\\), and \\(s\\) is the pooled sample standard deviation, \\(s_p = \\sqrt{\\frac{(n_X - 1) s_X^2 + (n_Y-1) s_Y^2}{n_X + n_Y-2}}\\). \\(d &lt;.2\\) is considered trivial, \\(.2 \\le d &lt; .5\\) small, and \\(.5 \\le d &lt; .8\\) large. # (d &lt;- rstatix::cohens_d(dep, dep_score ~ 1, mu = 4) %&gt;% pull(effsize) %&gt;% abs()) #&lt;!-- Cohen&#39;s *d* is `r d %&gt;% format(digits = 2, nsmall = 2)`, a small effect. --&gt; Make a habit of constructing a plot, just to make sure your head is on straight. Now you are ready to report the results. Data are mean \\(\\pm\\) standard deviation, unless otherwise stated. There were 20 male and 20 female participants. An independent-samples t-test was run to determine if there were differences in engagement to an advertisement between males and females. There were no outliers in the data, as assessed by inspection of a boxplot. Engagement scores for each level of gender were normally distributed, as assessed by Shapiro-Wilks test (p &gt; .05), and there was homogeneity of variances, as assessed by Levenes test for equality of variances (p = 0.174). There were 20 male and 20 female participants. The advertisement was more engaging to male viewers (5.56 \\(\\pm\\) = 0.29) than female viewers (5.30 \\(\\pm\\) = 0.39), a statistically significant difference of 0.26 (95% CI, 0.04 to 0.48), t(38) = 2.365, p = 0.023. 2.2 2 Sample Welchs t Test for Numeric Vars This test applies when the variable variances are unequal. 2.3 Paired Sample t Test for Numeric Vars This test applies when you have paired samples. 2.4 Mann-Whitney U Test for Numeric Vars This test applies when the variable distributions are non-normally. 2.5 Wilcoxon Paired-Sample Test for Numeric Vars This test applies when the variable distributions are non-normally distributed and samples are paired. 2.6 Pearsons Correlation for Numeric Vars 2.7 Spearmans Ranked Correlation for Numeric Vars 2.8 2 Sample Independent t Test for Categorical Vars This test applies when you know the population variance. 2.9 2 Sample Welchs t Test for Categorical Vars This test applies when the variable variances are unequal. 2.10 Paired Sample t Test for Categorical Vars This test applies when you have paired samples. 2.11 Mann-Whitney U Test for Categorical Vars This test applies when the variable distributions are non-normally. 2.12 Wilcoxon Paired-Sample Test for Categorical Vars This test applies when the variable distributions are non-normally distributed and samples are paired. 2.13 Pearsons Correlation for Categorical Vars 2.14 Spearmans Ranked Correlation for Categorical Vars 2.15 Fishers Exact Test Use Fishers Exact Test to test whether the observed frequencies of a single discrete dichotomous variable are equal. Suppose you have a treatment and control, and a binary outcome with two levels, sick and cured. The observed frequency counts, \\(O_j\\), of the \\(J\\) levels of a categorical variable differ from the expected frequency counts, \\(E_j\\). \\(H_0\\) is \\(O_j = E_j\\). The p-value from the test is computed as if the margins of the table are fixed. This leads under a null hypothesis of independence to a hypergeometric distribution of the numbers in the cells of the table The test is applicable in situations where the row totals \\(n_{i+}\\) and the column totals \\(n_+j\\) are fixed by study design (rarely applies), and the expected values of &gt;20% of cells (at least 1 cell in a 2x2 table) have expected cell counts &gt;5, and no expected cell count is &lt;1. In practice, it appears that researchers use Fishers exact test for 2x2 tables with small (n &lt; 1,000) sample sizes. It is more accurate than the chi-square and G tests. ([Wikipedial(https://en.wikipedia.org/wiki/Fisher%27s_exact_test)). Fishers exact test is useful for small n-size samples where the chi-squared distribution assumption of the chi-squared and G-test tests fails. Fishers exact test is overly conservative (p values too high) for large n-sizes. The t-test returns nearly the same result as the z-test when the CLT holds, so in practice no one bothers with the z-test except as an aid to teach the t-test. "]]

[["association.html", "Chapter 6 Association", " Chapter 6 Association Tests of association assess the strength of association between two variables. There are many variations on this theme. Pearsons correlation assesses the strength of a linear relationship between two continuous variables. It applies when the relationship is linear with no outliers and the variables are bi-variate normal. There are two less restrictive alternatives, Spearmans rho and Kendalls tau, that assess the strength and direction of association. If one of the variables is bivariate categorical, use point-biserial correlation, a special case of Pearsons correlation. Pearsons partial correlation controls for one or more variables - linear regression? If both variables are ordinal, use Goodman and Kruskals gamma. Somers d is an alternative if you want to distinguish between a dependent and independent variable (instead of linear regression?). The Mantel-Haenszel test of trend is used to determine whether there is a linear trend (i.e., a linear relationship/association) between two ordinal variables that are represented in a contingency table. The Cochran-Armitage test of trend is used to determine whether there is a linear trend (i.e., a linear relationship/association) between an ordinal independent variable and a dichotomous dependent variable. The chi-square test for association tests for whether two categorical variables are associated. The chi-square test for independence focuses on contingency tables that are greater than 2 x 2, which are often referred to as r x c contingency tables, and tests whether two variables measured at the nominal level are independent (i.e., whether there is an association between the two variables). Relative risks can be calculated from more than one statistical test, but in this guide we will focus on the calculation of relative risk in a 2 x 2 table. Odds Ratio can be calculated from more than one statistical test (e.g., a binomial logistic regression, ordinal logistic regression, multinomial logistic regression, etc), but in this guide we will focus on the calculation of an odds ratio from a 2 x 2 contingency table (i.e., a measure of association between two dichotomous variables). Goodman and Kruskals  (the Greek symbol, , is pronounced lambda) is also referred to as Goodman and Kruskals lambda. It is a nonparametric measure of the strength of association between two nominal variables where a distinction is made between a dependent and independent variable The Fishers exact test can be used to test more than one type of null hypothesis. In this guide we will use Fishers exact test to determine whether two dichotomous variables are independent (i.e., test the null hypothesis of independence). Loglinear analysis is used to understand (and model) associations between two or more categorical variables (i.e., nominal or ordinal variables). However, loglinear analysis is usually employed when dealing with three or more categorical variables, as opposed to two variables, where a chi-square test for association is usually conducted instead. Use association tests to assess a possible two-way linear association between two continuous (interval or ratio) random variables. Association tests return an estimate between +1 (perfect linear relationship) to -1 (perfect linear inverse relationship). Use Pearsons product moment if both random variables are normally distributed. If either variable is skewed, ordinal, or has extreme values, use Spearmans rank correlation. There are three common correlation tests for categorical variables1: Tetrachoric correlation for binary categorical variables; polychoric correlation for ordinal categorical variables; and Cramers V for nominal categorical variables. See https://www.statology.org/correlation-between-categorical-variables/ "],["pearsons-correlation.html", "6.1 Pearsons Correlation", " 6.1 Pearsons Correlation The Pearson product-moment correlation measures the strength and direction of a linear relationship between two continuous variables, x and y. The Pearson correlation coefficient, r, ranges from -1 (perfect negative linear relationship) to +1 (perfect positive linear relationship). A value of 0 indicates no relationship between two variables. \\[r_{x,y} = \\frac{cov(x,y)}{\\sigma(x) \\sigma(y)}\\] The statistic can be used as an estimate of the population correlation, \\(\\rho\\), in a test of statistical significance from 0 (H0: \\(\\rho\\) = 0). \\[\\rho_{X,Y} = \\frac{cov(X,Y)}{\\sigma(X) \\sigma(Y)}\\] A rule of thumb interpretation is that \\(|r|\\) under .1 is no correlation, .1 - .3 is small, .3 - .5 medium/moderate, and .5 - 1.0 large/strong. The test statistic below follows a t-distribution with n  2 degrees of freedom. \\[t = r_{x,y} \\sqrt{\\frac{n - 2}{1 - r^2_{x,y}}}\\] "],["spearmans-rho.html", "6.2 Spearmans Rho", " 6.2 Spearmans Rho Spearmans ranked correlation (Spearmans rho) is a measure of the strength and direction of a monotonic relationship between two variables that are at least ordinal. Spearmans correlation is a non-parametric alternative to Pearson when one or more of its conditions are violated. Unlike Pearson, the relationship need not be linear (it only needs to be monotonic), and has no outliers or bivariate normality conditions. Spearmans correlation is Pearsons correlation applied to the ranks of variables (for ordinal variables, their value already is a rank). However, there is also a second definition that gives the same result, at least when there are no ties in the ranks: \\[\\rho = 1 - \\frac{6 \\sum_i d^2_i}{n(n^2 - 1)}\\] where \\(d_i\\) is the difference in ranks of observation \\(i\\). "],["kendals-tau.html", "6.3 Kendals Tau", " 6.3 Kendals Tau Kendals tau is a second alternative to Pearson and is identical to Spearmans rho with regard to assumptions. Kendals tau only differs from Spearmans rho in how it measures the relationship. Whereas Spearman measures the correlation of the ranks, Kendals tau is a function of concordant (C), discordant (D) and tied (Tx and Ty) pairs of observations. Concordant means both X and Y in one observation of a pair are larger than in the other. Discordant means X is larger in one observation than the other while Y is smaller. Tied mean either both observations have the same X or both have the same Y. \\[\\mathrm{Kendall&#39;s} \\space \\tau_b = \\frac{C - D}{\\sqrt{(C + D + T_x) \\times (C + D + T_Y)}}\\] "],["case-study-pearson-spearman-kendall.html", "6.4 Case Study: Pearson, Spearman, Kendall", " 6.4 Case Study: Pearson, Spearman, Kendall This case study works with two data sets. dat1 is composed of two continuous variables; dat2 is composed of two ordinal variables. A researcher investigates the relationship between cholesterol concentration and time spent watching TV, time_tv, in n = 100 otherwise healthy 45 to 65 year old men (dat1). This data set will meet the conditions for Pearson, so we try all three tests on it. A researcher investigates the relationship between the level of agreement with the statement Taxes are too high (tax_too_high, four level ordinal) and participant income level (three level ordinal) (dat2). The ordinal variables rule out Pearson, leaving Spearman and Kendall. Conditions Pearsons correlation applies when X and Y are continuous (interval or ratio) paired variables with 1) a linear relationship that 2) has no significant outliers, and 3) are bivariate normal. Spearmans rho and Kendalls tau only require that X and Y be at least ordinal with 1) a monotonically increasing or decreasing relationship. Linearity and Monotonicity. A visual inspection of a scatterplot should find a linear relationship (Pearson) or monotonic relationship (Spearman and Kendall). Pearsons correlation additionall requires No Outliers. Identify outliers with the scatterplot. Normality. Bivariate normality is difficult to assess. Instead, check that each variable is individually normally distributed. Use the Shapiro-Wilk test. Linearity / Monotonicity Assess linearity and monotonicity with a scatter plot. dat1 is plotted on the left in Figure 6.1. A second version that fails the linearity test is shown to the right. If the linear relationship assumption fails, consider transforming the variable instead of reverting to Spearman or Kendall. Figure 6.1: The left scatter plot is dat1. It meets Pearsons linearity condition. A second version at right illustrates what a failure might look like. The ordinal variable data set dat2 is plotted in Figure 6.2. Figure 6.2: dat2 meets the mononicity assumption for Spearmans rho and Kendalls tau. No Outliers Pearsons correlation requires no outliers. Both plots in Figure 6.1 are free of outliers. If there were outliers, check whether they are data entry errors or measurement errors and fix or discard them. If the outliers are genuine, leave them in if they do not affect the conclusion. You can also try tranforming the variable. Failing all that, revert to the Spearmans rho or Kendalls tau. Bivariate Normality Bivariate normality is difficult to assess. If two variables are bivariate normal, they will each be individually normal as well. Thats the best you can hope to check for. Use the Shapiro-Wilk test. shapiro.test(cs1$dat1$time_tv) ## ## Shapiro-Wilk normality test ## ## data: cs1$dat1$time_tv ## W = 0.97989, p-value = 0.1304 shapiro.test(cs1$dat1$cholesterol) ## ## Shapiro-Wilk normality test ## ## data: cs1$dat1$cholesterol ## W = 0.97594, p-value = 0.06387 If a variable is not normally distributed, you can transform it, carry on regardless since the Pearson correlation is fairly robust to deviations from normality, or revert to Spearman and Kendall. Test Calculate Pearsons correlation, Spearmans rho, or Kendalls tau. dat meets the assumptions for Pearsons correlation, but try Spearmans rho and Kendalls tau too, just to see how close they come to Pearson. dat2 only meets the assumptions for Spearman and Kendall. Pearsons Correlation dat1 met the conditions for Pearsons correlation. (cs1$cc_pearson &lt;- cor.test(cs1$dat1$cholesterol, cs1$dat1$time_tv, method = &quot;pearson&quot;) ) ## ## Pearson&#39;s product-moment correlation ## ## data: cs1$dat1$cholesterol and cs1$dat1$time_tv ## t = 3.9542, df = 98, p-value = 0.0001451 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.1882387 0.5288295 ## sample estimates: ## cor ## 0.3709418 r = 0.37 falls in the range of a moderate linear relationship. \\(r^2\\) = 14% is the coefficient of determination. Interpret it as the percent of variability in one variable that is explained by the other. If you are not testing a hypothesis (HO: \\(\\rho \\ne 0\\)), you can report just report r. Otherwise, include the p-value. Report your results like this: A Pearsons product-moment correlation was run to assess the relationship between cholesterol concentration and daily time spent watching TV in males aged 45 to 65 years. One hundred participants were recruited. Preliminary analyses showed the relationship to be linear with both variables normally distributed, as assessed by Shapiro-Wilks test (p &gt; .05), and there were no outliers. There was a statistically significant, moderate positive correlation between daily time spent watching TV and cholesterol concentration, r(98) = 0.37, p &lt; .0005, with time spent watching TV explaining 14% of the variation in cholesterol concentration. You wouldnt use Spearmans rho or Kendalls tau here since the more precise Pearsons correlation is available. But just out of curiosity, here are the correlations using those two measures. cor(cs1$dat1$cholesterol, cs1$dat1$time_tv, method = &quot;kendall&quot;) ## [1] 0.2383971 cor(cs1$dat1$cholesterol, cs1$dat1$time_tv, method = &quot;spearman&quot;) ## [1] 0.3322122 Both Kendall and Spearman produced more conservative estimates of the strength of the relationship - Kendall especially so. You probably wouldnt use linear regression here either because it describes the linear relationship between a response variable and changes to an independent explanatory variable. Even though we are reluctant to interpret a regression model in terms of causality, that is what is implied the formulation y ~ x and independence assumption in of X. Nevertheless, correlation and regression are related. The slope term in a simple linear regression of the normalized values equals the Pearson correlation. lm( y ~ x, data = cs1$dat1 %&gt;% mutate(y = scale(cholesterol), x = scale(time_tv)) ) ## ## Call: ## lm(formula = y ~ x, data = cs1$dat1 %&gt;% mutate(y = scale(cholesterol), ## x = scale(time_tv))) ## ## Coefficients: ## (Intercept) x ## -3.092e-16 3.709e-01 Spearmans Rho Data set dat2 did not meet the conditions for Pearsons correlation, so use Spearmans rho and/or Kendalls tau. Start with Spearmans rho. Recall that Spearmans rho is just the Pearson correlation applied to the ranks. Recall also that the Pearsons correlation is just the covariance divided by the product of the standard deviations. You can quickly calculate it by hand. cov(rank(cs1$dat2$income), rank(cs1$dat2$tax_too_high)) / (sd(rank(cs1$dat2$income)) * sd(rank(cs1$dat2$tax_too_high))) ## [1] 0.6024641 Use the function though. I dont get why cor.test requires x and y be numeric. (cs1$spearman &lt;- cor.test( as.numeric(cs1$dat2$tax_too_high), as.numeric(cs1$dat2$income), method = &quot;spearman&quot;) ) ## Warning in cor.test.default(as.numeric(cs1$dat2$tax_too_high), ## as.numeric(cs1$dat2$income), : Cannot compute exact p-value with ties ## ## Spearman&#39;s rank correlation rho ## ## data: as.numeric(cs1$dat2$tax_too_high) and as.numeric(cs1$dat2$income) ## S = 914.33, p-value = 0.001837 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.6024641 Interpret the statistic using the same rule of thumb as for Pearsons correlation. A rho over .5 is a strong correlation. A Spearmans rank-order correlation was run to assess the relationship between income level and views towards income taxes in 24 participants. Preliminary analysis showed the relationship to be monotonic, as assessed by visual inspection of a scatterplot. There was a statistically significant, strong positive correlation between income level and views towards income taxes, \\(r_s\\) = 0.602, p = 0.002. Kendalls Tau Recall that Kendalls tau is a function of concordant (C), discordant (D) and tied (Tx and Ty) pairs of observations. The manual calculation is a little more involved. Here is the function instead. (cs1$kendall &lt;- cor.test( as.numeric(cs1$dat2$tax_too_high), as.numeric(cs1$dat2$income), method = &quot;kendall&quot;) ) ## Warning in cor.test.default(as.numeric(cs1$dat2$tax_too_high), ## as.numeric(cs1$dat2$income), : Cannot compute exact p-value with ties ## ## Kendall&#39;s rank correlation tau ## ## data: as.numeric(cs1$dat2$tax_too_high) and as.numeric(cs1$dat2$income) ## z = 2.9686, p-value = 0.002991 ## alternative hypothesis: true tau is not equal to 0 ## sample estimates: ## tau ## 0.5345225 As with the case study on cholesterol and television, Kendalls tau was more conservative than Spearmans rho. \\(\\tau_b\\) = 0.535 is still in the strong range, though just barely. A Kendalls tau-b correlation was run to assess the relationship between income level and views towards income taxes amongst 24 participants. Preliminary analysis showed the relationship to be monotonic, as assessed by visual inspection of a scatterplot. There was a statistically significant, strong positive correlation between income level and the view that taxes were too high, \\(\\tau_b\\) = 0.535, p = 0.003. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

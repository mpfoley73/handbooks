[["time-series.html", "Chapter 9 Time Series 9.1 Toolbox 9.2 Exploratory Analysis 9.3 Toolbox 9.4 Toolbox 9.5 Judgmental Forecasts 9.6 Time Series Regression 9.7 Exponential Smoothing 9.8 ARIMA 9.9 Dynamic Regression", " Chapter 9 Time Series These notes are based on the Time Series with R skill track at DataCamp and Forecasting: Principles and Practice (Rob J Hyndman 2021). Not all forecasting is quantitative - if you have no data you use qualitative judgment procedures. But quantitative forecasting isnt necessarily based on time series models - your model may be a cross-sectional analysis of relevant factors, perhaps even including time-related factors such as month of year or day of week. Time series forecasting is a specific type of forecasting: it is a quantitative forecast of future outcomes based, at least in part, on the assumption that future outcomes are functionally dependent upon prior outcomes. In most cases your forecasting objective is to project a time series. In these cases, the time series forecast is either an exercise of decomposing a time series into trend and seasonality components (exponential smoothing models) or describing the autocorrelation within the data (ARIMA models). There may also be cases where you include other predictor variables (dynamic models). These notes are structured as four sections. The toolbox section is about using R to explore and wrangle time series data. The next three sections describe the three main modeling techniques: exponential smoothing, ARIMA, and dynamic models. 9.1 Toolbox This section deals with foundational concepts in time series analysis: benchmarking methods, prediction intervals, evaluations accuracy. 9.1.1 R Structures Use a tsibble object to work with time series data. tsibble, from the package of the same name, is a time-series tibble. Unlike ts, zoo, and xts, tsibble preserves the time index, making heterogeneous data structures possible. But tsibble is new, so you will encounter the other other frameworks and should at least be familiar with them. Ill work with the prison_population.csv file accompanying Hyndmans text to get familiar with these packages. It contains quarterly prison population statistics grouped by several attributes. In essence, several time series within one file. prison &lt;- readr::read_csv(&quot;https://OTexts.com/fpp3/extrafiles/prison_population.csv&quot;) head(prison) ## # A tibble: 6 x 6 ## Date State Gender Legal Indigenous Count ## &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2005-03-01 ACT Female Remanded ATSI 0 ## 2 2005-03-01 ACT Female Remanded Non-ATSI 2 ## 3 2005-03-01 ACT Female Sentenced ATSI 0 ## 4 2005-03-01 ACT Female Sentenced Non-ATSI 5 ## 5 2005-03-01 ACT Male Remanded ATSI 7 ## 6 2005-03-01 ACT Male Remanded Non-ATSI 58 ts, xts, and zoo ts is the base R time series package. The ts object is essentially a matrix of observations indexed by a chronological identifier. Because it is a matrix, any descriptive attributes need to enter as numeric, perhaps by one-hot encoding, or pivoting the data (yuck). ts will not recognize irregularly spaced time series. Define a ts object with ts(x, start, frequency) where frequency is the number of observations in the seasonal pattern: 7 for days in a week cycle; 5 for weekdays in a week cycle; 24 for hours in a day cycle, 24*7 for hours in a week cycle, etc. In this case prison is quarterly starting with 2005 Q1. Had the series started at 2005 Q2, youd specify start = c(2005, 2). Ill pull out a single time series from the file, defined by the key State, Gender, Legal and Indigenous. prison_ts &lt;- prison %&gt;% filter(State == &quot;ACT&quot; &amp; Gender == &quot;Male&quot; &amp; Legal == &quot;Remanded&quot; &amp; Indigenous == &quot;ATSI&quot;) %&gt;% arrange(Date) %&gt;% select(Count) %&gt;% ts(start = 2005, frequency = 4) str(prison_ts) ## Time-Series [1:48, 1] from 2005 to 2017: 7 7 9 9 12 9 8 7 6 11 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : NULL ## ..$ : chr &quot;Count&quot; zoo (Zeileiss ordered observations) better supports ordered observations and provides methods similar to those for ts. zoo objects contain an array of data values and an index attribute to provide information about the data ordering. zoo was introduced in 2014. xts (extensible time series) extends zoo. xts is more flexible than ts while imposing reasonable constraints to make it a true time-based class. xts objects are essentially a matrix of observations indexed by a time object. Create an xts object with xts(x, order.by) where order.by is a vector of dates/times to index the data. You can also add metadata to the xts object by declaring name-value pairs such as born = as.POSIXct(\"1899-05-08\"). x &lt;- prison %&gt;% filter(State == &quot;ACT&quot; &amp; Gender == &quot;Male&quot; &amp; Legal == &quot;Remanded&quot; &amp; Indigenous == &quot;ATSI&quot;) %&gt;% arrange(Date) prison_xts &lt;- xts(x$Count, order.by = x$Date, State = &quot;ACT&quot;, Gender = &quot;Male&quot;, Legal = &quot;Remanded&quot;, Indigenous = &quot;ATSI&quot;) str(prison_xts) ## An &#39;xts&#39; object on 2005-03-01/2016-12-01 containing: ## Data: num [1:48, 1] 7 7 9 9 12 9 8 7 6 11 ... ## Indexed by objects of class: [Date] TZ: UTC ## xts Attributes: ## List of 4 ## $ State : chr &quot;ACT&quot; ## $ Gender : chr &quot;Male&quot; ## $ Legal : chr &quot;Remanded&quot; ## $ Indigenous: chr &quot;ATSI&quot; Create subsets of a time series (to create training and test data sets) with base R function window(x, start, end). window(prison_ts, start = c(2005, 4), end = c(2007, 3)) ## Qtr1 Qtr2 Qtr3 Qtr4 ## 2005 9 ## 2006 12 9 8 7 ## 2007 6 11 7 window(prison_xts, start = as.Date(&quot;2005-12-01&quot;), end = as.Date(&quot;2007-09-01&quot;)) ## [,1] ## 2005-12-01 9 ## 2006-03-01 12 ## 2006-06-01 9 ## 2006-09-01 8 ## 2006-12-01 7 ## 2007-03-01 6 ## 2007-06-01 11 ## 2007-09-01 7 tsibble A tsibble object is a tibble uniquely defined by key columns plus a date index column. This structure accommodates multiple series, and descriptive attribute columns. prison_tsibble &lt;- prison %&gt;% mutate(Date = yearquarter(Date)) %&gt;% rename(Qtr = Date) %&gt;% tsibble(key = c(State, Gender, Legal, Indigenous), index = Qtr) head(prison_tsibble) ## # A tsibble: 6 x 6 [1Q] ## # Key: State, Gender, Legal, Indigenous [1] ## Qtr State Gender Legal Indigenous Count ## &lt;qtr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2005 Q1 ACT Female Remanded ATSI 0 ## 2 2005 Q2 ACT Female Remanded ATSI 1 ## 3 2005 Q3 ACT Female Remanded ATSI 0 ## 4 2005 Q4 ACT Female Remanded ATSI 0 ## 5 2006 Q1 ACT Female Remanded ATSI 1 ## 6 2006 Q2 ACT Female Remanded ATSI 1 tsibbles behave just like tibbles, so the framework will feel natural - all the tidyverse verbs will work. The only thing that will trip you up is that tsibble objects are pre-grouped by the index, so if you used the summarize() verb on it, youll get one row per index value.1 prison_tsibble %&gt;% group_by(State) %&gt;% summarize(sum_Count = sum(Count)) ## # A tsibble: 384 x 3 [1Q] ## # Key: State [8] ## State Qtr sum_Count ## &lt;chr&gt; &lt;qtr&gt; &lt;dbl&gt; ## 1 ACT 2005 Q1 178 ## 2 ACT 2005 Q2 183 ## 3 ACT 2005 Q3 187 ## 4 ACT 2005 Q4 204 ## 5 ACT 2006 Q1 190 ## 6 ACT 2006 Q2 190 ## 7 ACT 2006 Q3 165 ## 8 ACT 2006 Q4 179 ## 9 ACT 2007 Q1 172 ## 10 ACT 2007 Q2 160 ## # ... with 374 more rows 9.1.2 Fitting Models Consider whether you are fitting a model for explanatory purposes (which factors are important and what is there affect on the outcome) or for predictive purposes (expected future outcomes). If explanation is your goal, you will fit a model, check that assumptions related to inference are met, then summarize the model parameters. If prediction is your goal, you might compare multiple models, cross-validate the results against a hold-out data set, then make predictions. Fit a model using fabletools::model(). You can even fit multiple models. Below, I will fit a simple naive model (projection of last value) to the Google closing stock prices from 2015, then use it to predict values from Jan 2016. google_dat &lt;- tsibbledata::gafa_stock %&gt;% filter(Symbol == &quot;GOOG&quot;, year(Date) &gt;= 2015) %&gt;% # re-index on trading day arrange(Date) %&gt;% mutate(trading_day = row_number()) %&gt;% update_tsibble(index = trading_day, regular = TRUE) # Break into training and test google_train &lt;- google_dat %&gt;% filter(year(Date) == 2015) google_test &lt;- google_dat %&gt;% filter(yearmonth(Date) == yearmonth(&quot;2016 Jan&quot;)) # Train model google_mdl &lt;- google_train %&gt;% model(mdl_naive = NAIVE(Close)) # Generate predictions (forecast) google_fc &lt;- google_mdl %&gt;% forecast(new_data = google_test) google_fc %&gt;% autoplot(color = &quot;goldenrod&quot;) + autolayer(google_train, Close, color = &quot;goldenrod&quot;) + theme_light() + theme(legend.position = &quot;none&quot;) + labs(title = &quot;20d naive forecast from model fit to CY-2015&quot;, caption = &quot;Shaded area is 80%- and 90% confidence interval.&quot;, x = &quot;Trading Day&quot;, y = &quot;Closing Price&quot;) 9.1.3 Evaluating Models Evaluate the model fit with residuals diagnostics.2 broom::augment() adds three columns to the relevant model cols: .fitted, .resid, and .innov. .innov is the residual from the transformed data - it equals .resid if no transformation. google_mdl_aug &lt;- google_mdl %&gt;% broom::augment() Innovation residuals should be independent random variables normally distributed with mean zero and constant variance. Happily, feasts has just what you need. google_mdl %&gt;% gg_tsresiduals() The autocorrelation plot supports the independence assumption. The histogram is pretty normal, but the right tail is awfully long. The residuals plot has mean zero and constant variance. You can carry out a portmanteau test test on the autocorrelation assumption. Two common tests are the Box-Pierce and the Ljung-Box. These tests check the likelihood of a combination of autocorrelations at once, without testing any one correlation - kind of like an ANOVA test. The Ljung-Box test statistic is a sum of squared \\(k\\)-lagged autocorrelations, \\(r_k^2\\), \\[Q^* = T(T+2) \\sum_{k=1}^l(T-k)^{-1}r_k^2.\\] The test statistic has a \\(\\chi^2\\) distribution with \\(l - K\\) degrees of freedom (where \\(K\\) is the number of parameters in the model). Use \\(l = 10\\) for non-seasonal data and \\(l = 2m\\) for seasonal data. If your model has no explanatory variables, \\(K = 0.\\) google_mdl_aug %&gt;% features(.var = .innov, features = ljung_box, lag = 10, dof = 0) ## # A tibble: 1 x 4 ## Symbol .model lb_stat lb_pvalue ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 GOOG mdl_naive 7.91 0.637 The p-value is not under .05, so do not reject the assumption of autocorrelation - i.e., the assumption of white noise. Use the checkresiduals() function from the forecast package to check these assumptions. The Ljung-Box test should have a p-value &gt;.05 and the histogram should be normal. The code chunk below tests the naive model on the goog200 dataset of daily Google stock prices. The ACF of the residuals show no residuals outside the 95% acceptance region of no-autocorrelation. The Ljung-Box fails to reject the null hypothesis of no-autocorrelation (p = 0.3551). The histogram is centered on zero and normally distributed (maybe slightly skewed right). The residuals plot shows constant variance. 9.1.4 Benchmark Forecasting Methods Some forecasting methods are extremely simple and surprisingly effective. Here are four. The average method projects the historical average, \\(\\hat{y}_{T+h|T} = \\bar{y}.\\) The naive method projects the last observation, \\(\\hat{y}_{T+h|T} = y_T.\\) The seasonal naive method projects the last seasonal observation, \\(\\hat{y}_{T+h|T} = y_{T+h-m(k+1)}.\\) The drift method projects the straight line from the first and last observation, \\(\\hat{y}_{T+h|T} = y_T + h\\left(\\frac{y_T - y_1}{T-1}\\right).\\) aus_production %&gt;% filter(Quarter &gt;= yearquarter(&quot;1970 Q1&quot;) &amp; Quarter &lt;= yearquarter(&quot;2004 Q4&quot;)) %&gt;% # I know you want to stick to a familiar framework, but tsibble does have a # convenient function for this: # filter_index(&quot;1970 Q1&quot; ~ &quot;2004 Q4&quot;) model(Mean = MEAN(Bricks), Naive = NAIVE(Bricks), SNaive = SNAIVE(Bricks), RW = RW(Bricks)) %&gt;% forecast(h = 14) %&gt;% ggplot(aes(x = Quarter)) + geom_line(aes(y = .mean, color = .model)) + # autoplot(level = NULL) + geom_line(data = aus_production %&gt;% filter_index(&quot;1970 Q1&quot; ~ &quot;2004 Q4&quot;), aes(y = Bricks)) + theme_light() + guides(color = guide_legend(title = &quot;Forecast&quot;)) + labs(title = &quot;These simple forecast methods are useful benchmarks.&quot;, subtitle = &quot;Quarterly brick production.&quot;, x = NULL, y = NULL) 9.2 Exploratory Analysis Start an analysis by viewing the data values and structure. Then take some summary statistics. fabletools::features() is great for this. prison_tsibble %&gt;% features(Count, list(mean = mean, quantile = quantile)) ## # A tibble: 64 x 10 ## State Gender Legal Indigenous mean `quantile_0%` `quantile_25%` ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ACT Female Remanded ATSI 1.42 0 1 ## 2 ACT Female Remanded Non-ATSI 4.96 1 3.75 ## 3 ACT Female Sentenced ATSI 1.25 0 0 ## 4 ACT Female Sentenced Non-ATSI 7.17 2 5 ## 5 ACT Male Remanded ATSI 15.6 6 11 ## 6 ACT Male Remanded Non-ATSI 66.7 44 54.8 ## 7 ACT Male Sentenced ATSI 23.4 1 9 ## 8 ACT Male Sentenced Non-ATSI 130. 40 89.5 ## 9 NSW Female Remanded ATSI 71.3 48 54.8 ## 10 NSW Female Remanded Non-ATSI 163. 124 141. ## # ... with 54 more rows, and 3 more variables: quantile_50% &lt;dbl&gt;, ## # quantile_75% &lt;dbl&gt;, quantile_100% &lt;dbl&gt; There are many autocorrelation features you might want to review. I dont understand why youd want to know all of these, but feat_acf has them. prison_tsibble %&gt;% features(Count, feat_acf) ## # A tibble: 64 x 11 ## State Gender Legal Indigenous acf1 acf10 diff1_acf1 diff1_acf10 diff2_acf1 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ACT Female Remand~ ATSI 0.622 0.798 -0.443 0.286 -0.645 ## 2 ACT Female Remand~ Non-ATSI 0.516 0.421 -0.285 0.272 -0.566 ## 3 ACT Female Senten~ ATSI 0.657 0.964 -0.102 0.474 -0.367 ## 4 ACT Female Senten~ Non-ATSI 0.558 0.926 -0.184 0.168 -0.467 ## 5 ACT Male Remand~ ATSI 0.839 2.36 -0.186 0.0916 -0.522 ## 6 ACT Male Remand~ Non-ATSI 0.711 1.11 -0.0115 0.195 -0.344 ## 7 ACT Male Senten~ ATSI 0.923 3.96 -0.122 0.151 -0.514 ## 8 ACT Male Senten~ Non-ATSI 0.931 4.60 0.167 0.302 -0.282 ## 9 NSW Female Remand~ ATSI 0.849 2.71 -0.0657 0.473 -0.340 ## 10 NSW Female Remand~ Non-ATSI 0.818 1.48 0.139 0.134 -0.358 ## # ... with 54 more rows, and 2 more variables: diff2_acf10 &lt;dbl&gt;, ## # season_acf1 &lt;dbl&gt; 9.2.1 Graphical Analysis The first task in any data analysis is to plot the data to identify patterns, unusual observations, changes over time, and relationships between variables. Look for trend, cycles, and seasonality in your exploratory plots. These features inform the subsequent forecasting process. ggplot2::autoplot() does a great job picking out the right plot, but I feel more comfortable staying old-school for now. data(&quot;ansett&quot;, package = &quot;tsibbledata&quot;) ansett %&gt;% filter(Airports == &quot;MEL-SYD&quot;, Class == &quot;Economy&quot;) %&gt;% ggplot(aes(x = Week, y = Passengers)) + geom_line(color = &quot;goldenrod&quot;) + theme_light() + labs(title = &quot;Start with a simple time series plot.&quot;, subtitle = &quot;Weekly passenger volume.&quot;, x = NULL, y = NULL) What does this one reveal? There was a period in 1989 of zero passengers (strike). There was a period in 1992 where passenger load dropped (planes temporarily reconfigured). Volume increased during the second half of 1992. Several large post-holiday dips in volume. Some larger trends of increasing and decreasing volume (the data is cyclic). Also appears to be some missing observations. (common practice with missing observations is to impute values with the time series mean.) If a data series has trend and seasonality, highlight it with feasts::gg_season(). data(&quot;PBS&quot;, package = &quot;tsibbledata&quot;) a10 &lt;- PBS %&gt;% filter(ATC2 == &quot;A10&quot;) %&gt;% select(Month, Cost) %&gt;% summarize(Cost = sum(Cost)) p1 &lt;- a10 %&gt;% filter(year(Month) %in% c(1991, 1995, 2000, 2005)) %&gt;% ggplot(aes(x = month(Month, label = TRUE, abbr = TRUE), y = Cost, group = factor(year(Month)), color = factor(year(Month)), label = if_else(month(Month) %in% c(1, 12), year(Month), NA_real_))) + geom_line(show.legend = FALSE) + geom_text(show.legend = FALSE) + theme_light() + theme(axis.text.x = element_text(angle = 90, vjust = 0.25)) + labs(subtitle = &quot;using gglot&quot;, x = NULL, y = NULL) p2 &lt;- a10 %&gt;% filter(year(Month) %in% c(1991, 1995, 2000, 2005)) %&gt;% fill_gaps() %&gt;% # otherwise, gg_season() barks. gg_season(Cost, labels = &quot;both&quot;) + theme_light() + theme(axis.text.x = element_text(angle = 90, vjust = 0.25)) + labs(subtitle = &quot;using feasts&quot;, x = NULL, y = NULL) p1 + p2 + plot_annotation(title = &quot;Medicare script costs increase from Feb - following Jan.&quot;, subtitle = &quot;12-month seasonality plot, selected years.&quot;) Emphasize the seasonality further by faceting on the sub-series with feasts::gg_subseries(). yint &lt;- a10 %&gt;% as.tibble() %&gt;% group_by(month(Month, label = TRUE, abbr = TRUE)) %&gt;% mutate(mean_cost = mean(Cost) / 1e6) p1 &lt;- a10 %&gt;% ggplot(aes(x = year(Month), y = Cost / 1e6)) + geom_line(show.legend = FALSE, color = &quot;goldenrod&quot;) + geom_hline(data = yint, aes(yintercept = mean_cost), linetype = 2) + theme_light() + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), panel.grid = element_blank()) + scale_y_continuous(labels = scales::dollar) + facet_wrap(vars(month(Month, label = TRUE, abbr = TRUE)), nrow = 1) + labs(subtitle = &quot;using gglot&quot;, x = NULL, y = NULL) p2 &lt;- a10 %&gt;% mutate(Cost = Cost / 1e6) %&gt;% fill_gaps() %&gt;% # otherwise, gg_subseries() barks. gg_subseries(Cost) + theme_light() + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), panel.grid = element_blank()) + scale_y_continuous(labels = scales::dollar) + labs(subtitle = &quot;using feasts&quot;, x = NULL, y = NULL) p1 / p2 + plot_annotation(title = &quot;Medicare script costs follow monthly seasonality, inreasing annually.&quot;, subtitle = &quot;Monthly subseries plot of cost $MM, 1991 - 2009.&quot;) Explore the correlation between two quantitative variables with the Pearson correlation coefficient. Recall that the covariance between series \\(X\\) and \\(Y\\) is defined \\(Cov(X, Y) = E[(X - \\mu_X) (Y - \\mu_Y)]\\) which simplifies to \\(Cov(X, Y) = E[XY] - \\mu_X \\mu_Y\\). The covariance of \\(X\\) and \\(Y\\) is positive if \\(X\\) and \\(Y\\) increase together, negative if they move in opposite directions, and if \\(X\\) and \\(Y\\) are independent, \\(E[XY] = E[X]E[Y] = \\mu_X \\mu_Y\\), so \\(Cov(X, Y) = 0\\). Covariance is usually inconvenient because its values depend on the units of the series. Dividing \\(Cov(X, Y)\\) by the standard deviations \\(\\sigma_X \\sigma_Y\\) creates a unit-less variable with range [-1, 1], also known as the Pearson correlation.3 \\[\\rho = \\frac{\\sigma_{XY}} {\\sigma_X \\sigma_Y}.\\] PBS %&gt;% group_by(ATC1) %&gt;% summarize(.groups = &quot;drop&quot;, Cost = sum(Cost)) %&gt;% pivot_wider(names_from = ATC1, values_from = Cost) %&gt;% as_tibble() %&gt;% select(-Month) %&gt;% cor() %&gt;% ggcorrplot::ggcorrplot() + theme_light() + labs(title = &quot;Z and P indexes negatively correlated with others.&quot;, subtitle = &quot;Correlation plot of Medicare ATC1 indexes.&quot;, x = NULL, y = NULL) Autocorrelation is correlation with lagging observations. Lag plots of current period vs lags are a particular kind of correlation scatterplot useful for identifying seasonality. feasts::ACF() extends the base R acf() function to tsibbles. aus_production contains quarterly production levels. The 4-period lag is a year-over-year correlation and is strong because of seasonality in production. The 8-period lag is less strong. The 1-, 2-, and 3-period lags are not positively correlated. In fact, the lag-2 is negatively correlated. data(&quot;aus_production&quot;, package = &quot;tsibbledata&quot;) prod2k &lt;- aus_production %&gt;% filter(year(Quarter) &gt;= 2000) prod2k %&gt;% gg_lag(Beer, geom = &quot;point&quot;) + theme_light() + labs(title = &quot;Production is seasonal with Q4 peak, Q1 second.&quot;, subtitle = &quot;Quarterly lag plot of beer production&quot;, x = NULL, y = NULL) p1 &lt;- prod2k %&gt;% ACF(Beer, lag_max = 16) %&gt;% ggplot(aes(x = lag)) + geom_linerange(aes(ymin = 0, ymax = acf), color = &quot;goldenrod&quot;) + geom_point(aes(y = acf), color = &quot;goldenrod&quot;) + geom_hline(yintercept = 0) + geom_hline(yintercept = -2 / sqrt(nrow(prod2k)), linetype = 2) + geom_hline(yintercept = +2 / sqrt(nrow(prod2k)), linetype = 2) + theme_light() + theme(panel.grid = element_blank()) + labs(subtitle = &quot;using ggplot&quot;, y = NULL) p2 &lt;- prod2k %&gt;% ACF(Beer, lag_max = 16) %&gt;% autoplot() + theme_light() + labs(subtitle = &quot;using autoplot&quot;, y = NULL) p1 + p2 + plot_annotation(title = &quot;Strongest correlation is 4-period lag, negative 2-period lag.&quot;, subtitle = &quot;Quarterly autocorrelation of beer production.&quot;, caption = &quot;Dashed lines are +/- square root of series length, the white noise bound.&quot;) Autocorrelation for trending data tends to be large and positive because observations nearby in time are also nearby in size. The ACF tends to have positive values that slowly decrease as the lags increase. Autocorrelation for seasonal data tends to be larger for the seasonal lags (at multiples of the seasonal frequency). The Quarterly Australian Beer Production ACF above shows seasonality. Autocorrelation for both trended and seasonal data has a combination of these effects. Time series that show no autocorrelation are called white noise. White noise series have near-zero autocorrelation. Stock price changes often exhibit white noise. Almost all lags have autocorrelation insignificantly different from zero. Expect 95% of spikes to lie withing \\(\\pm 2 / \\sqrt{T}\\) where \\(T\\) is the length of the data series. data(&quot;gafa_stock&quot;, package = &quot;tsibbledata&quot;) stock &lt;- gafa_stock %&gt;% filter(Symbol == &quot;FB&quot;) %&gt;% mutate(l1 = Close - lag(Close)) stock %&gt;% ACF(l1, lag_max = 16) %&gt;% ggplot(aes(x = lag)) + geom_linerange(aes(ymin = 0, ymax = acf), color = &quot;goldenrod&quot;) + geom_point(aes(y = acf), color = &quot;goldenrod&quot;) + geom_hline(yintercept = 0) + geom_hline(yintercept = -2 / sqrt(nrow(stock)), linetype = 2) + geom_hline(yintercept = +2 / sqrt(nrow(stock)), linetype = 2) + theme_light() + theme(panel.grid = element_blank()) + labs(title = &quot;Stock price changes tend to exhibit white noise.&quot;, subtitle = &quot;Daily autocorrelation of stock price changes.&quot;, caption = &quot;Dashed lines are +/- square root of series length, the white noise bound.&quot;) The Ljung-Box test tests the randomness of a series; a p-value under 0.05 rejects the null hypothesis of white noise. The test reject white noise for the beer production, but not for stock price changes. Box.test(prod2k$Beer, lag = 16, fitdf = 0, type = &quot;Ljung&quot;) ## ## Box-Ljung test ## ## data: prod2k$Beer ## X-squared = 187.44, df = 16, p-value &lt; 2.2e-16 Box.test(stock$l1, lag = 16, fitdf = 0, type = &quot;Ljung&quot;) ## ## Box-Ljung test ## ## data: stock$l1 ## X-squared = 16.745, df = 16, p-value = 0.4023 9.2.2 Transformations Remove known sources of variation (e.g., days per month, population growth, inflation). E.g., monthly totals may vary due to differing month lengths. milk &lt;- fma::milk %&gt;% as_tsibble() %&gt;% # milk is a `ts` object mutate(daily_avg = value / lubridate::days_in_month(index)) p1 &lt;- milk %&gt;% ggplot(aes(x = index, y = value)) + geom_line(color = &quot;goldenrod&quot;) + theme_light() + theme(axis.text.x = element_blank()) + labs(subtitle = &quot;Original&quot;, x = NULL, y = NULL) p2 &lt;- milk %&gt;% ggplot(aes(x = index, y = daily_avg)) + geom_line(color = &quot;goldenrod&quot;) + theme_light() + labs(subtitle = &quot;Daily Average&quot;, x = NULL, y = NULL) p1 / p2 + plot_annotation(title = &quot;Convert monthly sums into daily averages per month.&quot;, subtitle = &quot;Average daily milk production by month.&quot;) Make patterns more consistent across the data set. Simpler patterns usually lead to more accurate forecasts. A Box-Cox transformation can equalize seasonal variation. \\[w_t = \\begin{cases} \\mathrm{log}(y_t), &amp; \\mbox{if } \\lambda\\mbox{ = 0} \\\\ \\left(\\mathrm{sign}(y_t) |y_t|^\\lambda - 1 \\right) / \\lambda, &amp; \\mbox{otherwise} \\end{cases}\\] \\(\\lambda\\) can take any value, but values near the following yield familiar transformations. \\(\\lambda = 1\\): no substantive transformation. \\(\\lambda = 0.5\\): square root plus linear transformation. \\(\\lambda = 0.333\\): cube root plus linear transformation. \\(\\lambda = 0\\): natural log. \\(\\lambda = -1\\): inverse. A good value of \\(\\lambda\\) is one which makes the size of the seasonal variation about the same across the whole series. fabletools::features() and BoxCox.lambda() optimize \\(\\lambda\\), but try to choose a simple value to make interpretation clearer. Note that while forecasts are not sensitive to \\(\\lambda\\), prediction intervals are. data(&quot;aus_production&quot;, package = &quot;tsibbledata&quot;) lambda &lt;- aus_production %&gt;% # guerrero() applies Guerrero&#39;s method to select lambda that minimizes the # coefficient of variation: .12. features(Gas, features = guerrero) %&gt;% pull(lambda_guerrero) # supposedly the same method, but returns .10 instead. lambda_v2 &lt;- forecast::BoxCox.lambda(aus_production$Gas, method = &quot;guerrero&quot;) p1 &lt;- aus_production %&gt;% mutate(gas_xform = box_cox(Gas, lambda)) %&gt;% ggplot(aes(x = Quarter)) + geom_line(aes(y = Gas), color = &quot;goldenrod&quot;) + theme_light() + theme(axis.text.x = element_blank()) + labs(subtitle = &quot;Original&quot;, x = NULL) p2 &lt;- aus_production %&gt;% mutate(gas_xform = box_cox(Gas, lambda)) %&gt;% ggplot(aes(x = Quarter)) + geom_line(aes(y = gas_xform), color = &quot;goldenrod&quot;) + theme_light() + labs(subtitle = glue(&quot;Box-Cox, lambda = {scales::comma(lambda, accuracy = 0.01)}&quot;), y = glue(&quot;Gas^{scales::comma(lambda, accuracy = 0.01)}&quot;)) p1 / p2 + plot_annotation(title = &quot;Box-Cox transformation equalizes the seasonal component.&quot;, subtitle = &quot;Box-Cox transformation (Guerrero&#39;s method) of quarterly gas production.&quot;) 9.2.3 Decomposition Time series data often has trending, seasonality, and cycles. Its usually useful to lump trending and cycles into a trend-cycle components, or simply trend, and treat time series data as consisting of seasonality \\(S_t\\), trend \\(T_t\\), and a remainder \\(R_t\\). If the magnitude of the seasonal fluctuations and the variation in the trend cycle are constant, then these components are additive, \\(y_t = S_t + T_t + R_t\\); if they are proportional to the level, then these components are multiplicative, \\(y_t = S_t \\times T_t \\times R_t\\). 9.2.3.1 Classical Decomposition Classical decomposition was commonly used until the 1950s. It is still the basis of other methods, so it is good to understand. Classical decomposition is based on moving averages. An m-MA moving average of order \\(m = 2k + 1\\) averages the \\(k\\) observations before \\(t\\) through the \\(k\\) observations after \\(t\\). The slider package is great for this. \\(m\\) is usually an odd number so that the number of periods before and after are equal. \\[\\hat{T}_t = \\frac{1}{m}\\sum_{j = -k}^k y_{t+j}\\] library(slider) data(&quot;global_economy&quot;, package = &quot;tsibbledata&quot;) global_economy %&gt;% filter(Country == &quot;Australia&quot;) %&gt;% mutate( `3-MA` = slide_dbl(Exports, mean, .before = 1, .after = 1, .complete = TRUE), `5-MA` = slide_dbl(Exports, mean, .before = 2, .after = 2, .complete = TRUE), `7-MA` = slide_dbl(Exports, mean, .before = 3, .after = 3, .complete = TRUE), `9-MA` = slide_dbl(Exports, mean, .before = 4, .after = 4, .complete = TRUE) ) %&gt;% pivot_longer(cols = ends_with(&quot;-MA&quot;), names_to = &quot;MA_name&quot;, values_to = &quot;MA&quot;) %&gt;% ggplot(aes(x = Year)) + geom_line(aes(y = MA, color = MA_name), na.rm = TRUE) + geom_line(aes(y = Exports), size = 1.25, color = &quot;#000000&quot;, alpha = 0.4) + theme_light() + # guides(color = guide_legend(title = &quot;series&quot;)) + labs(title = &quot;m-MA simple moving averages smooth a time series.&quot;, subtitle = &quot;m-MA of annual export proportion of GDP for m = 3, 5, 7, 9.&quot;, x = NULL, y = &quot;Pct of GDP&quot;, color = NULL) But what if you have known seasonal periods in the data? For example, with quarterly seasonality it makes sense to take a moving average of the 4 periods at once. Do this with a moving average of a four-period moving average. aus_production %&gt;% filter(year(Quarter) &gt;= 1992) %&gt;% mutate( `4-MA` = slide_dbl(Beer, mean, .before = 1, .after = 2, .complete = TRUE), `2x4-MA` = slide_dbl(`4-MA`, mean, .before = 1, .after = 0, .complete = TRUE) ) %&gt;% pivot_longer(cols = ends_with(&quot;-MA&quot;), names_to = &quot;MA_type&quot;, values_to = &quot;MA&quot;) %&gt;% ggplot(aes(x = Quarter)) + geom_line(aes(y = Beer), color = &quot;gray&quot;, size = 1) + geom_line(aes(y = MA, color = MA_type), na.rm = TRUE) + theme_light() + ggthemes::scale_color_few() + guides(color = guide_legend(title = &quot;series&quot;)) + labs(title = &quot;MAs of MAs smooth seasonal periods.&quot;, subtitle = &quot;2-MA of a 4-MA for quarterly data.&quot;, y = NULL) Classical decomposition calculates a trend-cycle component \\(\\hat{T}_t\\) with an m-MA (odd order) or \\(2 \\times m\\)-MA. The de-trended series is the difference, \\(y_t - \\hat{T}_t\\). The seasonal component \\(\\hat{S}_t\\) is the seasonal average (e.g., for monthly seasons, then \\(\\hat{S}_1\\) would be the January average). The remainder is the residual, \\(\\hat{R}_t = y_t - \\hat{T}_t - \\hat{S}_t.\\) If the cycle variation and seasonal magnitude increases with the observation level, then the same principles apply except the subtrations are replaced with divisions, \\(y_t / \\hat{T}_t\\) and remainder \\(\\hat{R}_t = y_t / (\\hat{T}_t \\hat{S}_t)\\). data(&quot;us_employment&quot;, package = &quot;fpp3&quot;) us_employment %&gt;% filter(year(Month) &gt;= 1990 &amp; Title == &quot;Retail Trade&quot;) %&gt;% model(classical = classical_decomposition(Employed, type = &quot;additive&quot;)) %&gt;% components() %&gt;% pivot_longer(cols = Employed:random, names_to = &quot;component&quot;, values_to = &quot;employed&quot;) %&gt;% mutate(component = factor(component, levels = c(&quot;Employed&quot;, &quot;trend&quot;, &quot;seasonal&quot;, &quot;random&quot;))) %&gt;% ggplot(aes(x = Month, y = employed)) + geom_line(na.rm = TRUE, color = &quot;goldenrod&quot;) + theme_light() + facet_grid(vars(component), scales = &quot;free_y&quot;) + labs(title = &quot;Classical additive decomposition of US retail employment&quot;, subtitle = &quot;Employed = trend + seasonal + random&quot;, y = NULL) Classical decomposition has weaknesses: the trend-cycle is unavailable for the first few and and last few periods; it assumes the seasonal component is stable over time; and it also tends to over-smooth the data. 9.2.3.2 X-11 and SEATS X-11 and Seasonal Extraction in ARIMA Time Series (SEATS) are commonly used by governmental agencies. X-11 overcomes some of classical decompositions drawbacks by adding extra steps. It creates trend-cycle estimates for all periods, and accommodates a slowly varying seasonal component. us_employment %&gt;% filter(year(Month) &gt;= 1990 &amp; Title == &quot;Retail Trade&quot;) %&gt;% model(x11 = X_13ARIMA_SEATS(Employed ~ x11())) %&gt;% components() %&gt;% pivot_longer(cols = Employed:irregular, names_to = &quot;component&quot;, values_to = &quot;employed&quot;) %&gt;% mutate(component = factor(component, levels = c(&quot;Employed&quot;, &quot;trend&quot;, &quot;seasonal&quot;, &quot;irregular&quot;))) %&gt;% ggplot(aes(x = Month, y = employed)) + geom_line(na.rm = TRUE, color = &quot;goldenrod&quot;) + theme_light() + facet_grid(vars(component), scales = &quot;free_y&quot;) + labs(title = &quot;X11 decomposition of US retail employment&quot;, subtitle = &quot;Employed = trend + seasonal + irregular&quot;, y = NULL) SEATS is another one (too complicated to discuss evidently!). us_employment %&gt;% filter(year(Month) &gt;= 1990 &amp; Title == &quot;Retail Trade&quot;) %&gt;% model(x11 = X_13ARIMA_SEATS(Employed ~ seats())) %&gt;% components() %&gt;% pivot_longer(cols = Employed:irregular, names_to = &quot;component&quot;, values_to = &quot;employed&quot;) %&gt;% mutate(component = factor(component, levels = c(&quot;Employed&quot;, &quot;trend&quot;, &quot;seasonal&quot;, &quot;irregular&quot;))) %&gt;% ggplot(aes(x = Month, y = employed)) + geom_line(na.rm = TRUE, color = &quot;goldenrod&quot;) + theme_light() + facet_grid(vars(component), scales = &quot;free_y&quot;) + labs(title = &quot;SEATS decomposition of US retail employment&quot;, subtitle = &quot;Employed = f(trend, seasonal, irregular)&quot;, y = NULL) 9.2.3.3 STL Seasonal and Trend decomposition using Loess (STL) has several advantages over classical decomposition, and the SEATS and X-11 methods. It handles any type of seasonality (not just monthly and quarterly); the seasonality component can change; the smoothness of the trend-cycle can be changed by the modeler; and it is robust to outliers. The widow settings inside model() control how rapidly the components change. us_employment %&gt;% filter(year(Month) &gt;= 1990 &amp; Title == &quot;Retail Trade&quot;) %&gt;% model(STL = STL(Employed ~ trend(window = 7) + season(window = &quot;periodic&quot;), robust = TRUE)) %&gt;% components() %&gt;% pivot_longer(cols = Employed:remainder, names_to = &quot;component&quot;, values_to = &quot;employed&quot;) %&gt;% mutate(component = factor(component, levels = c(&quot;Employed&quot;, &quot;trend&quot;, &quot;season_year&quot;, &quot;remainder&quot;))) %&gt;% ggplot(aes(x = Month, y = employed)) + geom_line(na.rm = TRUE, color = &quot;goldenrod&quot;) + theme_light() + facet_grid(vars(component), scales = &quot;free_y&quot;) + labs(title = &quot;STL decomposition of US retail employment&quot;, subtitle = &quot;Employed = f(trend, seasonal, irregular)&quot;, y = NULL) STL decomposition is the basis for other insights into the data series. You can measure the relative strength of trend and seasonality by the relative size of their variance: \\(F_T = 1 - \\mathrm{Var}(R_T) / \\mathrm{Var}(R_T + T_T)\\) and \\(F_S = 1 - \\mathrm{Var}(S_T) / \\mathrm{Var}(S_T + T_T)\\). us_employment_featues &lt;- us_employment %&gt;% features(Employed, feat_stl) %&gt;% inner_join(us_employment %&gt;% as_tibble() %&gt;% select(Series_ID, Title) %&gt;% unique(), by = &quot;Series_ID&quot;) us_1 &lt;- us_employment_featues %&gt;% filter(trend_strength &gt;= 0.995 &amp; seasonal_strength_year &gt; 0.9) %&gt;% slice_sample(n = 1) %&gt;% pull(Series_ID) us_2 &lt;- us_employment_featues %&gt;% filter(trend_strength &gt;= 0.995 &amp; seasonal_strength_year &lt; 0.5) %&gt;% slice_sample(n = 1) %&gt;% pull(Series_ID) us_3 &lt;- us_employment_featues %&gt;% filter(trend_strength &lt;= 0.985 &amp; seasonal_strength_year &lt; 0.5) %&gt;% slice_sample(n = 1) %&gt;% pull(Series_ID) us_4 &lt;- us_employment_featues %&gt;% filter(trend_strength &lt;= 0.985 &amp; seasonal_strength_year &gt; 0.9) %&gt;% slice_sample(n = 1) %&gt;% pull(Series_ID) us_employment_ids &lt;- c(us_1, us_2, us_3, us_4) us_employment_featues %&gt;% mutate(Series_lbl = if_else(Series_ID %in% us_employment_ids, Title, NA_character_)) %&gt;% ggplot(aes(x = trend_strength, y = seasonal_strength_year)) + geom_point(color = &quot;goldenrod&quot;) + geom_text(aes(label = Series_lbl), color = &quot;goldenrod4&quot;) + theme_light() + labs(title = &quot;Some sectors have seasonal employment, othes trend, others both.&quot;) 9.3 Toolbox This section deals with ### Prediction The White Noise (WN) model is the simplest example of a stationary process. It has a fixed mean and variance. The WN model is one of several autoregressive integrated moving average (ARIMA) models. An ARIMA(p, d, q) model has three parts, the autoregressive order p (number of time lags), the order of integration (or differencing) d, and the moving average order q. When two out of the three terms are zeros, the model may be referred to based on the non-zero parameter, dropping AR, I or MA from the acronym describing the model. For example, ARIMA (1, 0,0) is AR(1), ARIMA(0,1,0) is I(1), and ARIMA(0,0,1) is MA(1). The WN model is ARIMA(0,0,0). Simulate a WN time series using the arima.sim() function with argument model = list(order = c(0, 0, 0)). Here is a 50-period WN model with mean 100 and standard deviation sd of 10. wn &lt;- arima.sim(model = list(order = c(0, 0, 0)), n = 50, mean = 100, sd = 10) ts.plot(wn, xlab = &quot;Period&quot;, ylab = &quot;&quot;, main = &quot;WN Model, mean = 100, sd = 10&quot;) Fit a WN model to a dataset with arima(x, order = c(0, 0, 0)). The model returns the mean, var, and se. arima(wn, order = c(0, 0, 0)) ## ## Call: ## arima(x = wn, order = c(0, 0, 0)) ## ## Coefficients: ## intercept ## 100.5823 ## s.e. 1.5006 ## ## sigma^2 estimated as 112.6: log likelihood = -189.04, aic = 382.08 The model mean will be identical to the series mean. The variance will be close to the series variance. # mean = intercept mean(wn) ## [1] 100.5823 # se ~ s.e. sqrt(var(wn) / length(wn)) ## [1] 1.515825 # var ~ sigma^2 var(wn) ## [1] 114.8863 The Random Walk (RW) model is a WN model with a strong time dependance, so there is no fixed mean or variance. It is a non-stationary model. The random walk model is \\(Y_t = c + Y_{t-1} + \\epsilon_t\\) where \\(c\\) is the drift coefficient, an overall slope parameter. Simulate a RW time series using the arima.sim() function with argument model = list(order = c(0, 1, 0)). Here is a 50-period RW model with mean 0. rw &lt;- arima.sim(model = list(order = c(0, 1, 0)), n = 50) ts.plot(rw, xlab = &quot;Period&quot;, ylab = &quot;&quot;, main = &quot;RW Model, mean = 0&quot;) The first difference of a RW model is just a WN model. ts.plot(diff(rw), xlab = &quot;Period&quot;, ylab = &quot;&quot;, main = &quot;Diff(RW) = WN&quot;) Specify the drift parameter with mean. The drift parameter is the slope of the RW model. rw &lt;- arima.sim(model = list(order = c(0, 1, 0)), n = 100, mean = 1) ts.plot(rw, xlab = &quot;Period&quot;, ylab = &quot;&quot;, main = &quot;RW Model, mean = 1&quot;) Fit a random walk model with drift by first differencing the data, then fitting the WN model to the differenced data. The arima() intercept is the drift variable. wn.mod &lt;- arima(diff(rw), order = c(0, 0, 0)) ts.plot(rw) abline(a = 0, b = wn.mod$coef) rw.mod &lt;- arima(rw, order = c(0, 1, 0)) points(rw - residuals(rw.mod), type = &quot;l&quot;, col = 2, lty = 2) When dealing with time series data, ask first if it stationary because stationary models are much simpler. A stationary process oscillates randomly about a fixed mean, a phenomenon called reversion to the mean. In contrast, nonstationary processes have time trends, periodicity, or lack mean reversion. Even when a process is nonstationary, the changes in the series may be approximately stationary. For example, inflation rates show a pattern over time related to Federal Reserve policy, but the changes in interest rates are stationary. WN processes are stationary, but RW processes (the cumulative sum of the WN process) are not. Here are plots of a WN process and corresponding RW process using the cumsum() of the WN. Only the WN process is stationary. # White noise wn &lt;- arima.sim(model = list(order = c(0, 0, 0)), n = 100) # Random walk from white noise rw &lt;- cumsum(wn) plot.ts(cbind(wn, rw), xlab = &quot;Period&quot;, main = &quot;WN with Zero Mean, and Corresponding RW&quot;) Here is another WN process with corresponding RW process, this time with a drift parameter of 0.4. # White noise with mean &lt;&gt; 0 wn &lt;- arima.sim(model = list(order = c(0, 0, 0)), n = 100, mean = 0.4) # Random walk with drift from white noise rw &lt;- cumsum(wn) plot.ts(cbind(wn, rw), xlab = &quot;Period&quot;, main = &quot;WN with Nonzero Mean, and Corresponding RW&quot;) 9.3.1 Autoregression The autoregressive (AR) model is the most widely used time series model. It shares the familiar interpretation of a simple linear regression, but each observation is regressed on the previous observation. \\[Y_t - \\mu = \\phi(Y_{t-1} - \\mu) + \\epsilon_t\\] where \\(\\epsilon_t \\sim WN(0, \\sigma_\\epsilon^2)\\). The AR model also includes the white noise (WN) and random walk (RW) models as special cases. The arima.sim() function can simulate data from an AR model by setting the model argument equal to list(ar = phi) where phi is a slope parameter in the interval (-1, 1). As phi approaches 1, the plot smooths. With negative phi values, the plot oscillates. # small autocorrelation x &lt;- arima.sim(model = list(ar = 0.5), n = 100) # large autocorrelation y &lt;- arima.sim(model = list(ar = 0.9), n = 100) # negative autocorrelation (oscillation) z &lt;- arima.sim(model = list(ar = -0.75), n = 100) plot.ts(cbind(x, y, z)) The plots generated by the acf() function provide useful information about each lag. Series x (small slope parameter) has positive autocorrelation for the first couple lags, but they quickly decay toward zero. Series y (large slope parameter) has positive autocorrelation for many lags. Series z (negative slope parameter) has an oscillating pattern. par(mfrow = c(2,2)) acf(x) acf(y) acf(z) The stationary AR model has a slope parameter between -1 and 1. The AR model exhibits higher persistence when its slope parameter is closer to 1, but the process reverts to its mean fairly quickly. Its sample ACF also decays to zero at a quick (geometric) rate, meaning values far in the past have little impact on the present value of the process. Below, the AR model with slope parameter 0.98 exhibits greater persistence than with slope parameter 0.90, but both decay to 0. ar90 &lt;- arima.sim(model = list(ar = 0.9), n = 200) ar98 &lt;- arima.sim(model = list(ar = 0.98), n = 200) par(mfrow = c(2,2)) ts.plot(ar90) ts.plot(ar98) acf(ar90) acf(ar98) By contrast, the random walk (RW) model is a special case of the AR model in which the slope parameter is equal to 1. The RW model is nonstationary, and shows considerable persistence and relatively little decay in the ACF. ar100 &lt;- arima.sim(model = list(order = c(0, 1, 0)), n = 200) par(mfrow = c(2,1)) ts.plot(ar100) acf(ar100) Fit the AR(1) model (autoregressive model with one time lag) using the arima() function with order = c(1, 0, 0), meaning 1 time lag, 0 differencing, and 0 order moving average. Below is an AR(1) model fit to the AirPassengers dataset. The output of the arima function shows \\(\\phi\\) as ar1 = 0.9646, \\(\\mu\\) as intercept = 278.4649, and \\(\\hat{\\sigma}_\\epsilon^2\\) as sigma^2 = 1119. ar1 &lt;- arima(AirPassengers, order = c(1, 0, 0)) ar1.fit &lt;- AirPassengers - residuals(ar1) print(ar1) ## ## Call: ## arima(x = AirPassengers, order = c(1, 0, 0)) ## ## Coefficients: ## ar1 intercept ## 0.9646 278.4649 ## s.e. 0.0214 67.1141 ## ## sigma^2 estimated as 1119: log likelihood = -711.09, aic = 1428.18 par(mfrow = c(2, 1)) ts.plot(AirPassengers) points(ar1.fit, type = &quot;l&quot;, col = 2, lty = 2) acf(AirPassengers) Use the ARIMA model to forecast observations with the predict() function. Specify the number of periods beyond the last observation with the n.ahead parameter. Below is a forecast of 10 periods (years) beyond the 1871-1970 annual observations in the Nile dataset. The relatively wide band of confidence (dotted lines) is a result of the low persistence in the data. ar1 &lt;-arima(Nile, order = c(1, 0, 0)) print(ar1) ## ## Call: ## arima(x = Nile, order = c(1, 0, 0)) ## ## Coefficients: ## ar1 intercept ## 0.5063 919.5685 ## s.e. 0.0867 29.1410 ## ## sigma^2 estimated as 21125: log likelihood = -639.95, aic = 1285.9 ts.plot(Nile, xlim = c(1871, 1980)) ar1.fit &lt;- Nile - resid(ar1) ar.pred &lt;- predict(ar1, n.ahead = 10)$pred ar.pred.se &lt;- predict(ar1, n.ahead = 10)$se points(ar.pred, type = &quot;l&quot;, col = 2) points(ar.pred - 2*ar.pred.se, type = &quot;l&quot;, col = 2, lty = 2) points(ar.pred + 2*ar.pred.se, type = &quot;l&quot;, col = 2, lty = 2) 9.3.2 Simple Moving Average The simple moving average (MA) model is \\[Y_t = \\mu + \\epsilon_t + \\theta\\epsilon_{t-1}\\] If the slope parameter \\(\\theta\\) is zero, \\(Y_t\\) is a white noise process, \\(Y_t \\sim (\\mu, \\sigma_\\epsilon^2)\\). Large \\(\\theta\\) indicates large autocorrelation. Negative \\(\\theta\\) indicates an oscillating series. The MA model is used to account for very short-run autocorrelation. Each observation is regressed on the previous innovation, which is not actually observed. Like the AR model, the MA model includes the white noise (WN) model as special case. Simulate the MA model using arima.sim() with parameter list(ma = theta), where theta is a slope parameter from the interval (-1, 1). Here are three MA models. The first has slope paramter 0.5 and the second has slope parameter 0.9. The second plot shows more persistance as a result. THe third plot has a negative slope parameter and oscillates as a result. x &lt;- arima.sim(model = list(ma = 0.5), n = 100) y &lt;- arima.sim(model = list(ma = 0.9), n = 100) z &lt;- arima.sim(model = list(ma = -0.5), n = 100) plot.ts(cbind(x, y, z)) Use the acf() function to estimate the autocorrelation functions. The MA series x with slope = 0.5 has a positive sample autocorrelation at the first lag, but it is approximately zero at other lags. The series y with slope = 0.9 has a larger sample autocorrelation at its first lag, but it is also approximately zero for the others. The series z with slope = -0.5 has a negative sample autocorrelation at the first lag and alternates, but is approximately zero for all higher lags. par(mfrow = c(2, 2)) acf(x) acf(y) acf(z) Fit the MA model using the arima() function with `order = c(0, 0, 1), meaning 0 time lag, 0 differencing, and 1st order moving average. Below is an MA model fit to the Nile dataset. The output of the arima function shows \\(\\theta\\) as ma1 = 0.3783, \\(\\mu\\) as intercept = 919.2433, and \\(\\sigma_\\epsilon^2\\) as sigma^2 = 23272. ma &lt;- arima(Nile, order = c(0, 0, 1)) print(ma) ## ## Call: ## arima(x = Nile, order = c(0, 0, 1)) ## ## Coefficients: ## ma1 intercept ## 0.3783 919.2433 ## s.e. 0.0791 20.9685 ## ## sigma^2 estimated as 23272: log likelihood = -644.72, aic = 1295.44 ts.plot(Nile) ma.fit &lt;- Nile - resid(ma) points(ma.fit, type = &quot;l&quot;, col = 2, lty = 2) Use the ARIMA model to forecast observations with the predict() function. The MA model can only produce a 1-step forecast. Except for the 1-step forecast, all forecasts from the MA model are equal to the estimated mean (intercept). Below is a forecast of 10 periods (years) beyond the 1871-1970 annual observations in the Nile dataset. predict(ma, n.ahead = 10) ## $pred ## Time Series: ## Start = 1971 ## End = 1980 ## Frequency = 1 ## [1] 868.8747 919.2433 919.2433 919.2433 919.2433 919.2433 919.2433 919.2433 ## [9] 919.2433 919.2433 ## ## $se ## Time Series: ## Start = 1971 ## End = 1980 ## Frequency = 1 ## [1] 152.5508 163.1006 163.1006 163.1006 163.1006 163.1006 163.1006 163.1006 ## [9] 163.1006 163.1006 ts.plot(Nile, xlim = c(1871, 1980)) ma.pred &lt;- predict(ma, n.ahead = 10)$pred ma.pred.se &lt;- predict(ma, n.ahead = 10)$se points(ma.pred, type = &quot;l&quot;, col = 2) points(ma.pred - 2*ma.pred.se, type = &quot;l&quot;, col = 2, lty = 2) points(ma.pred + 2*ma.pred.se, type = &quot;l&quot;, col = 2, lty = 2) How do you decide which model provides the best fit? Measure the model fit with the Akaike information criterion (AIC) and/or Bayesian information criterion (BIC). These indicators penalize models with more estimated parameters to avoid overfitting, so smaller indicator values are preferable. Use the AIC() and BIC() functions to estimate the indicators. Below are the AIC and BIC for the AR(1) and MA models. Although the predictions from both models are similar (they have a correlation coeffiicent of 0.94), both the AIC and BIC indicate that the AR model is a slightly better fit for the Nile data. cor(ar1.fit, ma.fit) ## [1] 0.9401765 AIC(ar1) ## [1] 1285.904 AIC(ma) ## [1] 1295.442 BIC(ar1) ## [1] 1293.72 BIC(ma) ## [1] 1303.257 9.4 Toolbox 9.4.1 Error Analysis Evaluate the forecast accuracy with the test dataset. Forecast accuracy can only be determined by how well the model performs on new data. The difference between the forecasted value and the actual observed value \\(e_{T+h} = y_{T+h} - \\hat{y}_{t+h|T}\\) is the forecaset error. What distinguishes forecast errors from model residuals is not just the datasets (train vs test). Forecasted values are (usually) multi-step forecasts - they include prior forecasted values as inputs. To evaluate forecast accuracy, Split the dataset into a training dataset and a test dataset. The test dataset should be about 20% of the original dataset, or at least as long as the anticipated forecast length. Construct training and test dataset with either the window() function or the subset() function. Here is the marathon dataset from the fpp2 package split into a training and test dataset with the naive forecast model fit to the training dataset and forecasting h = 20 periods ahead. # # marathon.train &lt;- subset(marathon, end = length(marathon) - 20) # marathon.train &lt;- window(marathon, end = 1996) # # # marathon.test &lt;- subset(marathon, start = length(marathon) - 19) # marathon.test &lt;- window(marathon, start = 1997) # # marathon.fc &lt;- naive(marathon.train, h = 20) # autoplot(marathon.fc) + # autolayer(marathon.fc, series = &quot;Naive Forecast&quot;) + # autolayer(marathon.test, series = &quot;Test Data&quot;) + # labs(title = &quot;Boston marathon winning times since 1897&quot;, # x = &quot;Year&quot;, # y = &quot;Minutes&quot;) There are a few benchmark metrics to evaluate a fit based on the errors. MAE. Mean absolute error, \\(mean(|e_t|)\\) RMSE. Root mean squared error, \\(\\sqrt{mean(e_t^2)}\\) MAPE. Mean absolute percentage error, \\(mean(|e_t / y_t|) \\times 100\\) MASE. Mean absolute scaled error, \\(MAE/Q\\) where \\(Q\\) is a scaling constant The MAE and RMSE are on the same scale as the data, so they are only useful for comparing models fitted to the same series. The MAPE is unitless, but does not work for \\(y_t = 0\\), and it assumes a meaningful zero (ratio data). The MASE is most useful for comparing datasets of different units. Use function accuracy() to evaluate a model. Here is a comparison of two methods to forecast the marathon dataset. Since both models use the same training set, the RMSE is a good metric to compare. The first model (naive) has a lower RMSE, so it is the better model. # marathon.fc.naive &lt;- naive(marathon.train, h = 24) # accuracy(marathon.fc.naive, oil)[&quot;Test set&quot;, &quot;RMSE&quot;] # # marathon.fc.drift &lt;- rwf(marathon.train, drift = TRUE, h = 25) # accuracy(marathon.fc.drift, oil)[&quot;Test set&quot;, &quot;RMSE&quot;] # # autoplot(marathon.train) + # autolayer(marathon.fc.naive$mean, series = &quot;Naive&quot;) + # autolayer(marathon.fc.drift$mean, series = &quot;Drift&quot;) + # autolayer(marathon.test, series = &quot;Test&quot;) There is a better way of evaluating a model than with a single test set. Time series cross-validation break the dataset into multiple training sets by setting the cutoff at varying points and then setting the test set to be a fixed number of steps ahead of the horizon. Function tsCV() returns the errors using time series cross-validation. Calculate the RMSE by taking the square root of the mean of the squared errors. # e &lt;- tsCV(oil, forecastfunction = naive, h = 1) # sqrt(mean(e^2, na.rm = TRUE)) 9.4.2 Selecting Predictors There are many strategies to choose regression model predictors when there are many to choose from. There are five common measures of predictive accuracy: \\(\\bar{R}^2\\), CV, AIC, AICc, and BIC. They can be calculated using the CV() (cross-validation) function from the forecast package. \\(\\bar{R}^2\\) is common and well-established, but tends to select too many predictor variables, making it less suitable for forecasting. BIC has the feature that if there is a true underlying model, the BIC will select it given enough data. However, there is rarely a true underlying model, and even if there was one, that model would not necessarily produce the best forecasts because the parameter estimates may not be accurate. The AICc, AIC, and CV statistics are usually best because forecasting is their objective. If the value of time series size \\(T\\) is large enough, they all lead to the same model. \\(R^2\\) is not a good measure of predictive ability because it does not measure bias: model that consistently predicts values that are 20% of observed will have \\(R^2 = 1\\). \\[\\bar{R}^2 = 1 - (1 - R^2) \\frac{T - 1}{T - k - 1}\\] where \\(T\\) is the number of observations and \\(k\\) is the number of predictors. Maximizing \\(\\bar{R}^2\\) is equivalent to minimized the regression standard error \\(\\hat{\\sigma}\\). Classical leave-one-out cross-validation (CV) measures the predictive ability of a model. In concept CV is calculated by fitting a model without observation \\(t\\), then measuring the predictive error at observation \\(t\\). The process is repeated for all \\(T\\) observations. \\(CV\\) is the mean squared error. The model with the minimum CV is the best model for forecasting. \\[CV = \\frac{1}{T} \\sum_{t=1}^T [\\frac{e_t}{1 - h_t}]^2\\] where \\(h_t\\) are the diagonal values of the hat-matrix \\(H\\) from \\(\\hat{y} = X\\beta = X(X`X)^{-1}X&#39;y = Hy\\) and \\(e_t\\) is the residual obtained from fitting the model to all \\(T\\) observations. Closely related to CV is Akaikes Information Criterion (AIC), defined as \\[AIC = T \\log(\\frac{SSE}{T}) + 2(k + 2)\\] The measure penalises the models by the number of parameters that need to be estimated. The model with the minimum AIC is the best model for forecasting. For large values of \\(T\\), minimising the AIC is equivalent to minimising the CV. For small values of \\(T\\), the AIC tends to select too many predictors, and so a bias-corrected version of the AIC has been developed, AICc. \\[AIC_c = AIC + \\frac{2(k+2)(k + 3)}{T - k - 3}\\] BIC is similar to AIC, but penalises the number of parameters more heavily than the AIC. For large values of \\(T\\), minimising BIC is similar to leave-v-out cross-validation when \\(v = T[1  1/\\log(T) - 1]\\). \\[BIC = T \\log(\\frac{SSE}{T}) + (k + 2)\\log(T)\\] There are two common methods for using the measures: best subsets regression and stepwise regression. In best subsets regression, fit all possible models, then choose the one with the best metric value (e.g., lowest AIC). If there are simply too many candidate models (e.g., 40 predictors yield \\(2^40\\) models), the use stepwise regression. In backwards stepwise regression, include all candidate predictors initially, then check whether leaving any one predictor out improves the evaulation metric. If any leave-one-out model is better, then choose the best leave-one-out model. Repeat until no leave-one-out model is better. Here is an example from dataset uschange in the fpp2 package. uschange contains quarterly growth rates of personal consumption, income, production, and savings, and the unemployment rate in the US from 1960 to 2016. The 4 predictors of consumption yield \\(2^4 = 16\\) possible models. # inc &lt;- c(1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0) # prd &lt;- c(1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0) # sav &lt;- c(1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0) # emp &lt;- c(1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0) # df &lt;- data.frame(inc, prd, sav, emp) # # mat &lt;- matrix(NA, nrow = 16, ncol = 5) # mat[1,] &lt;- CV(tslm(Consumption ~ Income + Production + Savings + Unemployment, data = uschange)) # mat[2,] &lt;- CV(tslm(Consumption ~ Income + Production + Savings, data = uschange)) # mat[3,] &lt;- CV(tslm(Consumption ~ Income + Production + Unemployment, data = uschange)) # mat[4,] &lt;- CV(tslm(Consumption ~ Income + Production, data = uschange)) # mat[5,] &lt;- CV(tslm(Consumption ~ Income + Savings + Unemployment, data = uschange)) # mat[6,] &lt;- CV(tslm(Consumption ~ Income + Savings, data = uschange)) # mat[7,] &lt;- CV(tslm(Consumption ~ Income + Unemployment, data = uschange)) # mat[8,] &lt;- CV(tslm(Consumption ~ Income, data = uschange)) # mat[9,] &lt;- CV(tslm(Consumption ~ Production + Savings + Unemployment, data = uschange)) # mat[10,] &lt;- CV(tslm(Consumption ~ Production + Savings, data = uschange)) # mat[11,] &lt;- CV(tslm(Consumption ~ Production + Unemployment, data = uschange)) # mat[12,] &lt;- CV(tslm(Consumption ~ Production, data = uschange)) # mat[13,] &lt;- CV(tslm(Consumption ~ Savings + Unemployment, data = uschange)) # mat[14,] &lt;- CV(tslm(Consumption ~ Savings, data = uschange)) # mat[15,] &lt;- CV(tslm(Consumption ~ Unemployment, data = uschange)) # mat[16,] &lt;- CV(tslm(Consumption ~ 1, data = uschange)) # # #df2 &lt;- as.data.frame(mat) # df &lt;- cbind(data.frame(inc, prd, sav, emp), as.data.frame(mat)) # colnames(df) &lt;- c(&quot;Inc&quot;, &quot;Prd&quot;, &quot;Sav&quot;, &quot;Emp&quot;, &quot;CV&quot;, &quot;AIC&quot;, &quot;AICc&quot;, &quot;BIC&quot;, &quot;AdjR2&quot;) # df$CV &lt;- round(df$CV, 3) # df$AIC &lt;- round(df$AIC, 1) # df$AICc &lt;- round(df$AICc, 1) # df$BIC &lt;- round(df$BIC, 1) # df$AdjR2 &lt;- round(df$AdjR2, 3) # # df %&gt;% arrange(AICc) %&gt;% # knitr::kable() The best model contains all four predictors. However, there is clear separation between the models in the first four rows and the ones below, so Income and Savings are more important than Production and Unemployment. Also, the first two rows have almost identical values of CV, AIC and AICc. So you could possibly drop the Production variable and get similar forecasts. 9.5 Judgmental Forecasts Use judgmental forecasting when: (i) there is no available data; (ii) data is available, but the statistical forecasts need to be adjusted using judgement; or (iii) data is available, but independent statistical and judgmental forecasts can be combined. Follow these principals when conducting a judgmental forecast: Set the forecasting task clearly and concisely. Implement a systematic approach. Identify and weight information. Document and justify. This reduces bias. Systematically evaluate forecasts. Check forecast peformance after data arrive. Segregate forecasters and users. The forecaster may be deliver a reality check to the user. The Delphi method relies on the key assumption that forecasts from a group are generally more accurate than those from individuals. The method is assemble a panel of experts which makes initial forecasts with justifications. A summary of the forecasts and reasoning is distributed to the experts. The experts review their forecasts in light of the feedback. This step may be iterated until a satisfactory level of consensus is reached. A similar method called forecasting by analogy constructs the candidate forecasts by relying on analaogous situations. The experts use their judgment and experience to identify the appropriate analagous situations. Another method is to construct forecasts based on possible scenarios that may unfold. 9.6 Time Series Regression A time series regression forecasts one time series as a linear relationshiop with other time series. \\[y_t = \\beta X + \\epsilon_t\\] The linear regression model assumes there is a linear relationship between the forecast variable and the predictor variables. The errors must have mean zero, otherwise the forecasts are biased (the least squares method guarantees this is true). The errors must not be autocorrelated, otherwise the forecasts are inefficient because there is more information in the data that can be exploited. Finally, the errors must not be related to the predictor variables, otherwise there is more information that should be included in the systematic part of the model. It is also useful to have the errors being normally distributed with a constant variance in order to produce prediction intervals. Another important assumption is that each predictor is not a random variable. The tslm() function fits a linear regression model to time series data. It is similar to the lm() function, but also provides additional facilities for handling time series. Here is an example from dataset uschange in the fpp2 package. uschange contains quarterly growth rates of personal consumption, income, and production in the US from 1960 to 2016. It also contains the unemployment rate. One could model consumption as \\[Consumption_t = \\beta_0 + \\beta_1 Income_t + \\beta_2 Production_t + \\beta_3 Savings_t + \\beta_4 Unemployment_t + \\epsilon_t\\] The scatterplot matrix shows positive relationships with income and industrial production, and negative relationships with savings and unemployment. # corrplot::corrplot(cor(as.data.frame(uschange)), # method = &quot;number&quot;, # type = &quot;upper&quot;) Fit the model with tslm(). # uschange.lm &lt;- tslm(Consumption ~ Income + Production + Savings + Unemployment, data = uschange) # summary(uschange.lm) The fitted values follow the actual data fairly closely. The Adjusted \\(R^2\\) is 0.749 and the standard error of the regression is \\(\\hat{\\sigma}_\\epsilon = 0.329\\). The fitted values to actual values plot has a strong linear relationship. # p1 &lt;- autoplot(uschange[,&#39;Consumption&#39;], series=&quot;Data&quot;) + # autolayer(fitted(uschange.lm), series=&quot;Fitted&quot;) + # xlab(&quot;Quarter&quot;) + ylab(&quot;&quot;) + # ggtitle(&quot;Percent change in US consumption expenditure&quot;) + # guides(colour = guide_legend(title=&quot; &quot;)) # # p2 &lt;- cbind(Data = uschange[,&quot;Consumption&quot;], # Fitted = fitted(uschange.lm)) %&gt;% # as.data.frame() %&gt;% # ggplot(aes(x=Data, y=Fitted)) + # geom_point() + # ylab(&quot;Fitted (predicted values)&quot;) + # xlab(&quot;Data (actual values)&quot;) + # ggtitle(&quot;Percent change in US consumption expenditure&quot;) + # geom_abline(intercept=0, slope=1) # # grid.arrange(p1, p2, nrow = 2) Evaluate the regression model with a series of diagnostic tests using checkresiduals() from the forecast package. The residuals vs time plot shows some heteroscedasticity in the variance. Heteroscedasticity can make prediction intervals inaccurate. The histogram shows that the residuals are slightly skewed. Non-normality of the residuals can also make the prediction intervals inaccurate. The autocorrelation plot (ACF) shows a significant spike at lag 7. If autocorrelation exists, forecasts are still unbiased, but the prediction intervals are larger than they need to be. Another test of autocorrelation in the residuals is the Breusch-Godfrey test for serial correlation up to a specified order. A small p-value indicates there is significant autocorrelation remaining in the residuals. The Breusch-Godfrey test is similar to the Ljung-Box test, but it is specifically designed for use with regression models. In this case, the spike at lag 7 is not enough for the Breusch-Godfrey to be significant (p = 0.062). In any case, the autocorrelation is not particularly large, and at lag 7 it is unlikely to have a noticeable impact on the forecasts or the prediction intervals. # checkresiduals(uschange.lm) The residuals should be independent of each of the explanatory variables and independent of candidate variables not used in the model. In this case, the residuals show a random pattern in each of the plots. # df &lt;- as.data.frame(uschange) # df[,&quot;Residuals&quot;] &lt;- as.numeric(residuals(uschange.lm)) # p1 &lt;- ggplot(df, aes(x=Income, y=Residuals)) + # geom_point() # p2 &lt;- ggplot(df, aes(x=Production, y=Residuals)) + # geom_point() # p3 &lt;- ggplot(df, aes(x=Savings, y=Residuals)) + # geom_point() # p4 &lt;- ggplot(df, aes(x=Unemployment, y=Residuals)) + # geom_point() # gridExtra::grid.arrange(p1, p2, p3, p4, nrow=2) A plot of the residuals against the fitted values should also show no pattern. # cbind(Fitted = fitted(uschange.lm), # Residuals = residuals(uschange.lm)) %&gt;% # as.data.frame() %&gt;% # ggplot(aes(x=Fitted, y=Residuals)) + geom_point() Check for outliers and influential observations in the data and take remedial action if the data are spurious. For multiple linear regression models there is no straight-forward visual diagnostic like the simple linear regression scatterplot. The hat matrix \\(H\\) identifies leverage points. Recall that in the linear regression model \\(\\hat{y} = X \\hat{\\beta}\\) the slope coefficients are estimated by \\(\\hat{\\beta} = (X&#39;X)^{-1}X&#39;y\\). Substituting, \\(\\hat{y} = X(X&#39;X)^{-1}X&#39;y\\), or \\(\\hat{y} = Hy\\), where \\[H = X(X&#39;X)^{-1}X&#39;.\\] \\(H\\) is called the hat matrix because \\(H\\) puts the hat on \\(y\\). \\(H\\) is an \\(n \\times n\\) matrix. The diagonal elements \\(H_{ii}\\) are a measure of the distances between each observation \\(i\\)s predictor variables \\(X_i\\) and the average of the entire data set predictor variables \\(\\bar{X}\\). \\(H_{ii}\\) are the leverage that the observed responses \\(y_i\\) exert on the predicted responses \\(\\hat{y}_i\\). Each \\(H_{ii}\\) is in the unit interval [0, 1] and the values sum to the number of regression parameters (including the intercept) \\(\\sum{H_{ii}} = p\\). A common rule is to research any observation whose leverage value is more than 3 times larger than the mean leverage value, which since the sum of the leverage values is \\(p\\), equals \\[H_{ii} &gt; 3 \\frac{p}{n}.\\] There are multiple methods to identify influential points. One is Cooks distance. Cooks distance for observation \\(i\\) is defined \\[D_i = \\frac{(y_i - \\hat{y}_i)^2}{p \\times MSE} \\frac{H_{ii}}{(1 - H_{ii})^2}.\\] \\(D_i\\) directly summarizes how much all of the fitted values change when the ith observation is deleted. A data point with \\(D_i &gt; 1\\) is probably influential. \\(D_i &gt; 0.5\\) is at least worth investigating. # autoplot(cooks.distance(uschange.lm)) + # geom_hline(yintercept = 0.5, linetype=&quot;dashed&quot;) + # labs(title = &quot;Cook&#39;s Distance&quot;, # subtitle = &quot;Investigate distances &gt; 0.5.&quot;, # x = &quot;Observation Number&quot;, # y = &quot;Cook&#39;s&quot;) There are several predictor variables that you may add to a time series regression model. The trend is the slope of \\(y_t = \\beta_0 + \\beta_1 t + \\epsilon_t\\). The season is a factor indicating the season (month, quarter, etc.) based on the frequency of the data. The time series trend and seasaon is calculated on the fly in the tslm() function as variables trend and season. Here is an example using the Autralian beer production dataset ausbeer. This model includes both a trend variable and a seasonal variable. \\[y_t = \\beta_0 + \\beta_1 t + \\beta_2 S2 + \\beta_3 S3 + \\beta_4 S4\\] where \\(S2\\), \\(S3\\), and \\(S4\\) are seasonal (quarterly) dummiess, with season 1 as the reference. The model is concisely formulated in tslm(). There is a downward trend of 0.34 megalitres per quarter. On average, Q2 production is lower than Q1 by 34.7 megalitres, Q3 production is lower than Q1 by 17.8 megalitres, and Q4 production is higher than Q1 by 72.8 megalitres. # y &lt;- window(ausbeer, start = 1992) # summary(ausbeer.lm &lt;- tslm(y ~ trend + season)) # autoplot(y, series = &quot;Data&quot;) + # autolayer(forecast(ausbeer.lm, h=20), series = &quot;Forecast&quot;) + # autolayer(fitted(ausbeer.lm), series = &quot;Fitted&quot;) + # labs( x = &quot;Quarter&quot;, # y = &quot;Megalitres&quot;, # title = &quot;Quarterly Beer Production&quot;, # subtitle = &quot;Linear time series regression with trend and seasonal dummies.&quot;) You can also create dummy variables to flag holidays and outliers. Predict future values with ex-ante forecasts or ex-post forecasts. An ex-ante forecasts are possible when the model is based only on calendar effects. # y &lt;- window(ausbeer, start = 1992) # ausbeer.lm &lt;- tslm(y ~ trend + season) # ausbeer.fcst &lt;- forecast(ausbeer.lm, h = 8) # autoplot(ausbeer.fcst) + # ggtitle(&quot;Time Series Regression with Forecast&quot;) Ex-post forecasts require assumptions (scenarios) about future values. # uschange.lm &lt;- tslm(Consumption ~ Income + Savings + Unemployment, # data = uschange) # newdata = data.frame( # Income = c(1, 1, 1, 1), # Savings = c(0.5, 0.5, 0.5, 0.5), # Unemployment = c(0, 0, 0, 0)) # uschange.fcst &lt;- forecast(uschange.lm, newdata = newdata) # autoplot(uschange.fcst) + # ggtitle(&quot;US Change in Consumption, ex-post Model&quot;) 9.7 Exponential Smoothing Exponential smoothing weights recent observations more heavily than remote observations. Think of exponential smoothing as a family of methods varying by their trend and seasonal components. Additionally, the errors may be additive or multiplicative. There can be no trend (N), or it can be an additive (A) linear trend from the forecast horizon, or it can be a damped additive (Ad) trend leveling off from the forecast horizon. There can be no seasonality (N), or it can be additive (A), or additive damped (Ad). The errors may be constant over time (A), or increase with the level (M). The trend and seaonality combinations produce 3 x 3 = 9 possible exponential smoothing methods. The parameters determining the level, trend, and seasonality of the exponential smoothing model are based on minimization of the sum of square errors (SSE) of the simultaneous equations. The two treatment of errors double the number of possible state space models to 18. State space models include error, trend, and seasonality components and are therefore called ETS models. ETS models do not just extend the exponential smoothing models to account for treatment of the error variance. They also estimate their parameters differently. ETS models use maximum likelihood estimation. For models with additive errors, this is equivalent to minimizing the sum of squared errors (SSE). The great advantage of using ETS models is that you can optimize the parameter settings by minimizing the Akaike Information Criterion (AICc). The sections below describe the basic exponential smoothing models, focusing on the structure and parameters. 9.7.1 Simple Exponential Smoothing Simple exponential smoothing models have no seasonal or trend components. Simple exponential smoothing models are of the form \\(\\hat{y}_{t+h|t} = \\alpha y_t + \\alpha(1-\\alpha)y_{t-1} + \\alpha(1-\\alpha)^2y_{t-2} \\dots\\) where \\(0 &lt; \\alpha &lt; 1\\) is a weighting parameter. Exponential smoothing models are commonly expressed in a component form as a regressive model. The first component, the forecast, is the last value of the estimated level. \\[\\hat{y}_{t+h|t} = l_t\\] The second component, the level, describes how the level changes over time. \\[l_t = \\alpha y_t + (1 - \\alpha)l_{t-1}\\] \\(l_t\\) is the level (or smoothed value) of the series at time \\(t\\). Expressed this way, it is clear there are two parameters to estimate: \\(\\alpha\\) and \\(l_0\\). Simple exponential smoothing estimates the parameters by minimizing the SSE. Unlike regression, which returns exact parameter estimates, the SSE for the exponential equation is minimized with nonlinear optimization. The ses() function performs simple exponential smoothing. Here is simple exponential smoothing applied to the marathon dataset to produce a 10-year forecast. \\(\\alpha = 0.3457\\), and \\(l_0 = 167.1741\\). # marathon.train &lt;- subset(marathon, end = length(marathon) - 10) # marathon.test &lt;- subset(marathon, start = length(marathon) - 9) # marathon.ses &lt;- ses(marathon.train, h = 10) # summary(marathon.ses) Simple exponential smoothing produces a flat line that is exponentially weighted from the prior values, then extended into the forecast period. # autoplot(marathon.ses) + # autolayer(marathon, series=&quot;Actual&quot;) + # autolayer(fitted(marathon.ses), series=&quot;Fitted&quot;) + # autolayer(marathon.ses$mean, series=&quot;Forecast&quot;) + # labs(title = &quot;Boston Marathon Winning Times with 10-year Forecast&quot;, # subtitle = &quot;Method: Simple Exponential Smoothing&quot;, # y = &quot;Minutes&quot;, # x = &quot;Year&quot;) + # guides(colour=guide_legend(title=&quot;Series&quot;), # fill=guide_legend(title=&quot;Prediction interval&quot;)) + # scale_color_manual(values = c(&quot;black&quot;, &quot;red&quot;, &quot;blue&quot;)) Check the model assumptions with checkresiduals(). The residuals plot has constant and independent variance, at least after 1930. The histogram has a normal distribution. The autocorrelation function (ACF) plot shows spikes all within the insignificance band, yet the Ljung-Box test rejects the null hypothesis of no autocorrelation of the residuals (p = 0.0240). The forecast might still be useful even with residuals that dont quite pass the white noise test. # checkresiduals(marathon.ses) 9.7.2 Holt Holts linear trend model expands simple exponential smoothing with a trend component. The forecast contains both a level and a trend. \\[\\hat{y}_{t+h|t} = l_t + hb_t\\] The level is adjusted for the trend too. \\[l_t = \\alpha y_t + (1 - \\alpha)(l_{t-1} + hb_{t-1})\\] A third equation, the trend, describes how the slope changes over time. \\(\\beta^*\\) describes how quickly the slope can change. \\[b_t = \\beta^*(l_t - l_{t-1}) + (1 - \\beta^*)b_{t-1}\\] Now there are four parameter to estimate, \\(\\alpha\\), \\(\\beta^*\\), \\(l_0\\), and \\(b_0\\). Holt estimates the parameters by minimizing the SSE. The holt function performs Holts linear method. A variation of Holts linear trend method is Holts damped trend method. Whereas Holts linear trend stays constant over time, the damped trend levels off to a constant. Add the damped = TRUE parameter to holt() to use the damped trend method. Here is Holts linear trend applied to the austa dataset from the fpp2 package to produce a 10-year forecast. austa contains total annual international visitors to Australia, 1980-2015. # austa.train &lt;- subset(austa, end = length(austa) - 10) # austa.test &lt;- subset(austa, start = length(austa) - 9) # austa.holt.lin &lt;- holt(austa.train, h = 10) # austa.holt.dmp &lt;- holt(austa.train, h = 10, damped = TRUE) # summary(austa.holt.lin) Holts linear trend produces a sloped, but straight line. The forecasted values from the damped trend version are overlaid in green. They just start to even out at the forecast horizon. # autoplot(austa.holt.lin) + # autolayer(austa, series = &quot;Actual&quot;) + # autolayer(fitted(austa.holt.lin), series = &quot;Fitted&quot;) + # autolayer(austa.holt.lin$mean, series = &quot;Forecast&quot;) + # autolayer(austa.holt.dmp$mean, series = &quot;Forecast (damped)&quot;) + # labs(title = &quot;International Visitors to Australia with 10-year Forecast&quot;, # subtitle = &quot;Method: Holt&#39;s Linear Trend&quot;, # y = &quot;Visitors&quot;, # x = &quot;Year&quot;) + # guides(colour=guide_legend(title = &quot;Series&quot;), # fill=guide_legend(title = &quot;Prediction interval&quot;)) + # scale_color_manual(values = c(&quot;black&quot;, &quot;red&quot;, &quot;blue&quot;, &quot;green&quot;)) Check the model assumptions with checkresiduals. The residuals plot looks has constant and independent variance. The histogram does not really show a normal distribution. The autocorrelation function (ACF) plot shows spikes all within the insignificance band, and the Ljung-Box test fails to reject the null hypothesis of no autocorrelation of the residuals (p = 0.3253). # checkresiduals(austa.holt.lin) 9.7.3 Holt-Winters The Holt-Winters method adds a seasonality component. There are two versions of this model, the additive and the multiplicative. The additive method assumes the error variance is constant, and the multiplicative version assumes the error variance scales with the level. Here is the additive version first. The forecast includes a level, trend, and now also a season. \\[\\hat{y}_{t+h|t} = l_t + hb_t + s_{t-m+h_m^+}\\] The level is adjusted for the trend and now the season too. \\[l_t = \\alpha(y_t - s_{t-m}) + (1 - \\alpha)(l_{t-1} + b_{t-1})\\] The trend is not affected by the seasonal component. \\[b_t = \\beta^*(l_t - l_{t-1}) + (1 - \\beta^*)b_{t-1}\\] The seasonal compenent changes over time in relation to the \\(\\gamma\\) parameter. \\(m\\) is the period of seasonality. \\[s_t = \\gamma(y_t - l_{t-1} - b_{t-1}) + (1 - \\gamma)s_{t-m}\\] There are now three smoothing parameters: \\(0 \\le \\alpha \\le 1\\), \\(0 \\le \\beta^* \\le 1\\), and \\(0 \\le \\gamma \\le 1-\\alpha\\). In the additive version, the seasonal component averages to zero. In the multiplicative version, the seasonality averages to one. Use the multiplicative method if the seasonal variation increases with the level. \\[\\hat{y}_{t+h|t} = (l_t + hb_t) s_{t-m+h_m^+}\\] \\[l_t = \\alpha\\frac{y_t}{s_{t-m}} + (1 - \\alpha)(l_{t-1} + b_{t-1})\\] \\[b_t = \\beta^*(l_t - l_{t-1}) + (1-\\beta*)b_{t-1}\\] \\[s_t = \\gamma\\frac{y_t}{(l_{t-1} - b_{t-1})} + (1 - \\gamma)s_{t-m}\\] Here is the Holt-Winters model applied to the a10 dataset from the fpp2 package to produce a 36-month forecast. a10 contains monthly anti-diabetic drug sales in Australia, 1991-2008. The error variance increases with the series level, so the multiplicative method applies. The model estimates the three smoothing parameters, plus initial states, including 12 initial season states - one for each month in the year. # a10.train &lt;- subset(a10, end = length(a10) - 36) # a10.test &lt;- subset(a10, start = length(a10) - 35) # a10.hw &lt;- hw(a10, seasonal = &quot;multiplicative&quot;, h = 36) # summary(a10.hw) # autoplot(a10.hw) + # autolayer(a10, series = &quot;Actual&quot;) + # autolayer(fitted(a10.hw), series = &quot;Fitted&quot;) + # autolayer(a10.hw$mean, series = &quot;Forecast&quot;) + # labs(title = &quot;Anti-Diabetic Drug Sales in Australia with 36-month Forecast&quot;, # subtitle = &quot;Method: Holt-Winters (multiplicative)&quot;, # y = &quot;Scripts&quot;, # x = &quot;Month&quot;) + # guides(colour=guide_legend(title = &quot;Series&quot;), # fill=guide_legend(title = &quot;Prediction interval&quot;)) + # scale_color_manual(values = c(&quot;black&quot;, &quot;red&quot;, &quot;blue&quot;)) Check the model assumptions with checkresiduals. The residuals plot shows some long-term autocorrelation (a long hump), and the variance increases in the latter years. The histogram shows a normal distribution. The autocorrelation function (ACF) plot shows many spikes outside the insignificance band, and the Ljung-Box test rejects the null hypothesis of no autocorrelation of the residuals (p &lt; 0.0001). # checkresiduals(a10.hw) Here is a four-week Holt-Winters forecast of the hyndsight dataset of daily pageviews on the Hyndsight blog for one year starting April 30, 2014. Create a training dataset consisting of all obserations minus the last four weeks. Then forecast those four weeks with Holt-Winters. Use the additive method because the variance is not scaling with page volume. Creae a second forecast with the seasonal naive method as a benchmark. Notice that the Ljung-Box test rejects the null hypothesis of no autocorrelation of the residuals. The forecast might still provide useful information even with residuals that fail the white noise test. # hyndsight.train &lt;- subset(hyndsight, end = length(hyndsight) - 4*7) # # hyndsight.hw &lt;- hw(hyndsight.train, seasonal = &quot;additive&quot;, h = 4*7) # # hyndsight.sn &lt;- snaive(hyndsight.train, h = 4*7) # # checkresiduals(hyndsight.hw) Compare Holt-Winters to the seasonal naive forecast. The RMSE of Holt-Winters (201.7656) is smaller than the RMSE of seasonal naive (202.7610), so it is the more accurate forecast. # accuracy(hyndsight.hw, hyndsight) # accuracy(hyndsight.sn, hyndsight) Here finally is a plot of the forecasted page views. # autoplot(hyndsight) + # autolayer(hyndsight.hw$mean) 9.7.4 ETS The namesake function for finding errors, trend, and seasonality (ETS) provides a completely automatic way of producing forecasts for a wide range of time series. # fets &lt;- function(y, h) {forecast(ets(y), h = h)} # a10.ets &lt;- ets(a10.train) # a10.snaive &lt;- snaive(a10.train) # a10.ets.cv &lt;- tsCV(a10.train, fets, h = 4) # a10.snaive.cv &lt;- tsCV(a10.train, snaive, h = 4) # mean(a10.ets.cv^2, na.rm = TRUE) # mean(a10.snaive.cv^2, na.rm = TRUE) 9.8 ARIMA ARIMA models are another approach to time series forecasting. Exponential smoothing and ARIMA models are the two most widely-used approaches to time series forecasting, and provide complementary approaches to the problem. While exponential smoothing models are based on a description of the trend and seasonality in the data, ARIMA models aim to describe the autocorrelations in the data. An autoregressive (AR) model is a multiple regression with lagged observations as predictors. \\[y_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\dots + \\phi_p y_{t-p} + e_t\\] A moving average (MA) model is a multiple regression with lagged errors as predictors. \\[y_t = c + e_t + \\theta_1 y_{t-1} + \\theta_2 y_{t-2} + \\dots + \\theta_p y_{t-q}\\] An autoregressive moving average (ARMA) model is a multiple regression with lagged observations and lagged errors as predictors. \\[y_t = c + \\phi_1 y_{t-1} + \\dots + \\phi_p y_{t-p} + \\theta_1 y_{t-1} + \\dots + \\theta_p y_{t-q} + e_t\\] An ARMA model with differencing (ARIMA(p,d,q)) model is an ARMA model with d levels of differencing. Whereas ETS models can handle non-constant variance with multiplicative errors and seasonality, ARIMA models require that you explicitly transform the data. There are an infinite number of tranformations. In increasing strength, there is the square root (\\(y_t^{.5}\\)), cube root (\\(y_t^{.333}\\)), log (\\(log(y_t)\\)), and inverse (\\(y_t^{-1}\\)). Notice below how the various transformations dampen the seasonal fluctuations. The square root and log transformations are not quite strong enough to even out the error variance, but the inverse transformation is a little too large. We need a transformation somewhere in between. # x &lt;- cbind(usmelec, # `lambda = .5` = sqrt(usmelec), # `lambda = log` = log(usmelec), # `lambda = -1` = -usmelec^-1) # autoplot(x, facet=TRUE) + # labs(title = &quot;US Net Electricity Generation&quot;, # x = &quot;Month&quot;, y = &quot;kWh&quot;) The Box-Cox transformation (described above) can find the optimal transformation. In this case, the optimal transformation is \\(w_t = (y_t^\\lambda - 1) / \\lambda\\) where \\(\\lambda = -0.5738\\). # (lambda &lt;- BoxCox.lambda(usmelec)) # df &lt;- cbind(Raw = usmelec, # BoxCox = BoxCox(usmelec, lambda = lambda)) # autoplot(df, facet=TRUE) + # xlab(&quot;Month&quot;) + ylab(&quot;kWh&quot;) + # ggtitle(&quot;Electricity Production: Jan 1956 - Aug 1995&quot;) Here is how you would use the transformation in a forecast. (This example is contrived because ets models can handle non-constant variance with multiplicative errors and seasonality). # usmelec %&gt;% # ets(lambda = BoxCox.lambda(usmelec)) %&gt;% # forecast(h = 60) %&gt;% # autoplot() R function auto.arima() from the forecast package chooses the optimal ARIMA model parameters using the Akaike criterion.4 Here is a plot of annual US net electricity generation from the usnetelec dataset in the expsmooth package. You can probably guess straight away that this time series will require one level of differencing (d = 1). # autoplot(usnetelec) + # labs(title = &quot;Annual US Net Electricity Generation&quot;, # y = &quot;billion kWh&quot;, # x = &quot;Year&quot;) The auto.arima() function chooses an ARIMA(2,1,2) with drift. # usnetelec.arima &lt;- auto.arima(usnetelec) # summary(usnetelec.arima) Here is a plot of the forecast. # usnetelec.arima %&gt;% forecast(h = 10) %&gt;% autoplot() Explicitly choose a model with the Arima() function. # usnetelec %&gt;% Arima(order = c(2, 1, 2), include.constant = TRUE) %&gt;% forecast() %&gt;% autoplot() Compare models of different classes with cross-validation. # # Set up forecast functions for ETS and ARIMA models # fets &lt;- function(x, h) { # forecast(ets(x), h = h) # } # farima &lt;- function(x, h) { # forecast(auto.arima(x), h = h) # } # # # Compute CV errors for ETS on austa as e1 # e1 &lt;- tsCV(austa, fets, h = 1) # # # Compute CV errors for ARIMA on austa as e2 # e2 &lt;- tsCV(austa, farima, h = 1) # # # Find MSE of each model class # mean(e1^2, na.rm = TRUE) # mean(e2^2, na.rm = TRUE) # # # Plot 10-year forecasts using the best model class # austa %&gt;% farima(h = 10) %&gt;% autoplot() If the time series includes seasonality, an ARIMA(p,d,q)(P,D,Q)[m] model includes seasonality. # # Check that the logged h02 data have stable variance # h02 %&gt;% log() %&gt;% autoplot() # # # Fit a seasonal ARIMA model to h02 with lambda = 0 # fit &lt;- auto.arima(h02, lambda = 0) # # # Summarize the fitted model # summary(fit) # # # Plot 2-year forecasts # fit %&gt;% forecast(h = 24) %&gt;% autoplot() # # Use 20 years of the qcement data beginning in 1988 # train &lt;- window(qcement, start = 1988, end = c(2007, 4)) # # # Fit an ARIMA and an ETS model to the training data # fit1 &lt;- auto.arima(train) # fit2 &lt;- ets(train) # # # Check that both models have white noise residuals # checkresiduals(fit1) # checkresiduals(fit2) # # # Produce forecasts for each model # fc1 &lt;- forecast(fit1, h = 1 + 4 * (2013 - 2007)) # fc2 &lt;- forecast(fit2, h = 1 + 4 * (2013 - 2007)) # # # Use accuracy() to find better model based on RMSE # accuracy(fc1, qcement) # accuracy(fc2, qcement) # bettermodel &lt;- fit2 9.9 Dynamic Regression Dynamic regression is like ordinary regression with explanatory variables, but the error term is now an ARIMA process instead of white noise. Dataset uschange from the fpp2 package contains growth rates of personal consumption and personal income in the US. You might want to model consumption as a function of income. Add parameter xreq to the auto.arima() function. xreg is a matrix of predictor variables. fpp2::uschange %&gt;% head() ## Consumption Income Production Savings Unemployment ## 1970 Q1 0.6159862 0.9722610 -2.4527003 4.8103115 0.9 ## 1970 Q2 0.4603757 1.1690847 -0.5515251 7.2879923 0.5 ## 1970 Q3 0.8767914 1.5532705 -0.3587079 7.2890131 0.5 ## 1970 Q4 -0.2742451 -0.2552724 -2.1854549 0.9852296 0.7 ## 1971 Q1 1.8973708 1.9871536 1.9097341 3.6577706 -0.1 ## 1971 Q2 0.9119929 1.4473342 0.9015358 6.0513418 -0.1 # (uschange.arima &lt;- auto.arima(uschange[, &quot;Consumption&quot;], # xreg = uschange[, &quot;Income&quot;])) # # Forecast fit as fc # uschange.fc &lt;- forecast(uschange.arima, xreg = rep(10, 6)) # # # Plot fc with x and y labels # autoplot(uschange.fc) + xlab(&quot;Month&quot;) + ylab(&quot;Sales&quot;) # Time plots of demand and temperatures #autoplot(elec[, c(&quot;Demand&quot;, &quot;Temperature&quot;)], facets = TRUE) # Matrix of regressors #xreg &lt;- cbind(MaxTemp = elec[, &quot;Temperature&quot;], # MaxTempSq = elec[, &quot;Temperature&quot;]^2, # Workday = elec[, &quot;Workday&quot;]) # Fit model #fit &lt;- auto.arima(elec[, &quot;Demand&quot;], xreg = xreg) # Forecast fit one day ahead #forecast(fit, xreg = cbind(20, 20^2, 1)) 9.9.1 Dynamic Harmonic Regression Dynamic harmonic regression is based on the principal that a combination of sine and cosine funtions can approximate any periodic function. \\[y_t = \\beta_0 + \\sum_{k=1}^{K}[\\alpha_k s_k(t) + \\gamma_k c_k(t)] + \\epsilon_t\\] where \\(s_k(t) = sin(\\frac{2\\pi k t}{m})\\) and \\(c_k(t) = cos(\\frac{2\\pi k t}{m})\\), \\(m\\) is the seasonal period, \\(\\alpha_k\\) and \\(\\gamma_k\\) are regression coefficients, and \\(\\epsilon_t\\) is modeled as a non-seasonal ARIMA process. The optimal model has the lowest AICc, so start with K=1 and increase until the AICc is no longer decreasing. K cannot be greater than \\(m/2\\). With weekly data, it is difficult to handle seasonality using ETS or ARIMA models as the seasonal length is too large (approximately 52). Instead, you can use harmonic regression which uses sines and cosines to model the seasonality. The fourier() function makes it easy to generate the required harmonics. The higher the order (K), the more wiggly the seasonal pattern is allowed to be. With K=1, it is a simple sine curve. You can select the value of K by minimizing the AICc value. Function fourier() takes in a required time series, required number of Fourier terms to generate, and optional number of rows it needs to forecast. # # Set up harmonic regressors of order 13 # harmonics &lt;- fourier(gasoline, K = 13) # # # Fit a dynamic regression model to fit. Set xreg equal to harmonics and seasonal to FALSE because seasonality is handled by the regressors. # fit &lt;- auto.arima(gasoline, xreg = harmonics, seasonal = FALSE) # # # Forecasts next 3 years # newharmonics &lt;- fourier(gasoline, K = 13, h = 3*52) # fc &lt;- forecast(fit, xreg = newharmonics) # # # Plot forecasts fc # autoplot(fc) Harmonic regressions are also useful when time series have multiple seasonal patterns. For example, taylor contains half-hourly electricity demand in England and Wales over a few months in the year 2000. The seasonal periods are 48 (daily seasonality) and 7 x 48 = 336 (weekly seasonality). There is not enough data to consider annual seasonality. # # Fit a harmonic regression using order 10 for each type of seasonality # fit &lt;- tslm(taylor ~ fourier(taylor, K = c(10, 10))) # # # Forecast 20 working days ahead # fc &lt;- forecast(fit, newdata = data.frame(fourier(taylor, K = c(10, 10), h = 20*48))) # # # Plot the forecasts # autoplot(fc) # # # Check the residuals of fit # checkresiduals(fit) Another time series with multiple seasonal periods is calls, which contains 20 consecutive days of 5-minute call volume data for a large North American bank. There are 169 5-minute periods in a working day, and so the weekly seasonal frequency is 5 x 169 = 845. The weekly seasonality is relatively weak, so here you will just model daily seasonality. The residuals in this case still fail the white noise tests, but their autocorrelations are tiny, even though they are significant. This is because the series is so long. It is often unrealistic to have residuals that pass the tests for such long series. The effect of the remaining correlations on the forecasts will be negligible. # # Plot the calls data # autoplot(calls) # # # Set up the xreg matrix # xreg &lt;- fourier(calls, K = c(10, 0)) # # # Fit a dynamic regression model # fit &lt;- auto.arima(calls, xreg = xreg, seasonal = FALSE, stationary = TRUE) # # # Check the residuals # checkresiduals(fit) # # # Plot forecasts for 10 working days ahead # fc &lt;- forecast(fit, xreg = fourier(calls, c(10, 0), h = 169*8)) # autoplot(fc) 9.9.2 TBATS Model Thte TBATS model (Trigonometric terms for seasonality, Box-Cox transformations for hetergeneity, ARMA errors for short-term dynamics, Trend (possibly damped), and Seasonal (including multiple and non-integer periods)). # gasoline %&gt;% tbats() %&gt;% forecast() %&gt;% autoplot() TBATS is easy to use, but the prediction intervals are often too wide, and it can be quite slow with large time series. TBATS returns output similar to this: TBATS(1, {0,0}, -, {&lt;51.18,14&gt;}), meaning 1=Box-Cox parameter, {0,0} = ARMA error, - = damping parameter, {&lt;51.18,14&gt;} = seasonal period and Fourier terms. References "]]

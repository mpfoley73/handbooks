[["inference.html", "Chapter 1 Inference 1.1 P-Values 1.2 Type I and II Errors 1.3 Statistical Power 1.4 An Exercise in p-values 1.5 Further Reading", " Chapter 1 Inference Statistical inference is the use of a samples distribution to describe the population distribution. Hypothesis tests, confidence intervals, and effect size estimates are all examples of statistical inference. We wary of published study results. Identical studies might produce significant and non-significant results, yet only the significant result is likely to reach publication (publication bias). The researcher may have tortured the data until they found a statistically significant result. The study might suffer from low statistical power. Applying principles of inference can mitigate these problems. There are at least three approaches to establishing statistical inference: frequentist, likelihood, and Bayesian. Think of them philosophically. The frequentist approach is the path of action. It rejects a null hypothesis if the p-value is low because repeated sample analyses are likely to agree. The likelihood apprach is the path of knowledge. It compares the observed summary measure to the likelihoods of the other possible realities. The Bayesian approach is the path of belief. It uses a summary measure to update the prior belief. 1.1 P-Values P-values express how surprising the summary measure is given the null hypothesis (H0). Suppose you hypothesize that IQs have increased from the established mean of \\(\\mu_0\\) = 100. H0 is \\(\\mu\\) = 100 and the alternative hypothesis, H1, is \\(\\mu\\) &gt; 100. Also suppose you are right and the population mean IQ is actually 106. Suppose the population is some very big number - 1,000,000 for convenience. mu_0 &lt;- 100 mu &lt;- 106 sigma &lt;- 15 N &lt;- 1000000 pop_100 &lt;- data.frame(person = seq(1, N), iq = rnorm(N, mu_0, sigma)) pop_106 &lt;- data.frame(person = seq(1, N), iq = rnorm(N, mu, sigma)) Here is what the distribution of IQs might look like. You take a random sample of n = 30 IQs from the population. n &lt;- 30 x &lt;- sample(pop_106$iq, n) (x_bar &lt;- mean(x)) ## [1] 105.5975 You measure \\(\\bar{x}\\) = 105.6, SD = 15.2. How surprising is this result given H0 that \\(\\mu\\) is 100? I.e., what is the probability of observing \\(\\bar{x}\\) this extreme? According to the Central Limit Theorem (CLM) repeated samples of size n from a large population will yield \\(\\bar{x}\\) values that approach a normal distribution centered at \\(\\mu\\) with a standard deviation equal to the population SD (\\(\\sigma\\)) divided by \\(sqrt{n}\\). The standard deviation of the sampling distribution of the mean is commonly referred to as the standard error (SE). For a sample size of n = 30, and a population SD of 15, youd expect repeated samples to converge on a mean of 100 with SE = 2.7. You can verify this empirically. sim &lt;- replicate(1000, mean(sample(pop_100$iq, 30))) Here is the distribution of \\(\\bar{x}\\) measurements from 1,000 random samples from the hypothesized population. So what is the probability of measuring \\(\\bar{x}\\) = 106? It is the probability of measuring a value \\(\\frac{\\bar{x} - \\mu_0}{\\sigma / \\sqrt{n}}\\) = 2.0 standard deviations from 100. You can look up the probability from the normal distribution. q &lt;- (x_bar - mu_0) / (sigma / sqrt(n)) pnorm(q, lower.tail = FALSE) ## [1] 0.02048121 You can get a sense of what this probability means by overlaying the sampling distribution of the mean from the presumed (H0) distribution with the sampling distribution of the mean from the actual population. 1.2 Type I and II Errors Either H0 or H1 is correct, and you must choose to either reject or non reject H0. That means there are four possible states at the end of your study. If your summary measure is extreme enough for you to declare a positive result and reject H0, you are either correct (true positive) or incorrect (false positive). False positives are called Type I errors. Alternatively, if your summary measure is not extreme enough for you to reject H0, you are either correct (true negative) or incorrect (false negative). False negatives are called Type II errors. The probabilities of landing in these four states depend on your chosen significance level, \\(\\alpha\\), and on the statistical power of the study, 1 - \\(\\beta\\). H0 True H1 True Significance test is positive, so you reject H0. False Positive Type I Error Probability = \\(\\alpha\\) True Positive Good Call! Probability = 1 - \\(\\beta\\) Significance test is negative, so you do not reject H0. True Negative Good Call! Probability = (\\(1 - \\alpha\\)) False Negative Type II Error Probability = \\(\\beta\\) \\(\\alpha\\) is the expected rate of Type I errors due to summary measures that are extreme by chance alone. \\(\\beta\\) is the expected rate of Type II errors due to summary measures that are not extreme by chance alone. In the IQ example, if the reality is \\(\\mu\\) = 100, any sample mean over 104.5 would be a false positive under the \\(\\alpha\\) = .05 level of significance. if the reality is \\(\\mu\\) = 106, any sample mean under 104.5 would be a false negative under the \\(1 - \\beta\\) = 0.64 statistical power of the study. 1.3 Statistical Power Type II error rates (\\(\\beta\\)) vary inversely with the power of the study (1 - \\(\\beta\\)). Statistical power is an increasing function of a) sample size, b) effect size, and c) significance level. The positive association with significance level means there is a trade-off between Type I and Type II error rates. A small \\(\\alpha\\) sets a high bar for rejecting H0, but you run the risk of failing to appreciate a real difference. On the other hand, a small \\(\\alpha\\) sets a low bar for rejecting H0, and you run the risk of judging a random difference as a real difference. The 1 - \\(\\beta\\) statistical power threshold is usually set at .80, similar to the \\(\\alpha\\) = .05 level of significance threshold convention. Given a real effect, a study with a statistical power of .80 will only find a positive test result 80% of the time. There may be such a thing as too much power, however. With a large enough sample size, even trivial effect sizes may yield a positive test result. You need to consider both sides of this coin. A power analysis is frequently used to determine the sample size required to detect a threshold effect size given an \\(\\alpha\\) level of significance criteria. A power analysis expresses the relationship among four components. If you know any three, it can return the fourth. The components are a) 1 - \\(\\beta\\) (power), b) sample size, c) \\(\\alpha\\), and d) expected effect size. Suppose you set 1 - \\(\\beta\\) at .8 and \\(\\alpha\\) at .05. You can use the power test to see the relationship between sample size and effect size. Lets do that with the IQ example. data.frame(n = 10:300) %&gt;% mutate(d = map_dbl(n, ~ pwr.t.test(n = ., sig.level = .05, power = .80, type = &quot;one.sample&quot;, alternative = &quot;greater&quot;) %&gt;% pluck(&quot;d&quot;)), delta = d * sigma ) %&gt;% ggplot(aes(x = n, y = delta)) + geom_line() + geom_vline(xintercept = 30, linetype = 2, size = 1, color = &quot;slateblue&quot;) + theme_light() + labs(title = &quot;&quot;) 1.4 An Exercise in p-values What distribution of p-values would you expect if there is a true effect and you repeated the study many times? What if there is not true effect? The answer is completely determined by the statistical power of the study (the probability of observing true positives). Lets run 100,000 simulations of an experiment where you sample 26 individuals and measure their IQ. We simulate the sample by generating 26 random values from the normal distribution centered at 106 with a standard deviation of 15. We will test whether the sample averages differ from the hypothesized population average of 100. library(tidyverse) n_sims &lt;- 1000 #number of simulated experiments run_sim &lt;- function(M = 106, n = 26, SD = 15) { sim &lt;- data.frame(i = 1:n_sims) %&gt;% mutate( x = map(i, ~ rnorm(n = n, mean = M, sd = SD)), z = map(x, ~ t.test(., mu = 100)), p = map_dbl(z, ~ .x$p.value) ) sim } plot_sim &lt;- function(x, M = 106, n = 26, SD = 15, n_breaks = 20, x_lim = c(0, 1), x_labels = waiver(), y_lim = c(0, n_sim)) { test_power = sum(x$p &lt; .05) / n_sims x %&gt;% ggplot(aes(x = p)) + geom_bar(fill = &quot;lightgoldenrod&quot;) + scale_x_binned(n.breaks = n_breaks, limits = x_lim, labels = x_labels) + geom_hline(yintercept = n_sims * .05, color = &quot;firebrick&quot;, linetype = 2) + annotate(&quot;text&quot;, x = .8, y = n_sims * .05, label = &quot;5%&quot;) + theme_light() + scale_y_continuous(labels = scales::comma_format(), limits = c(0, n_sims)) + labs( title = glue( &quot;Measured p-values in {n_sims} simulations of samples of size {n} distributed \\n&quot;, &quot;N({M}, {SD}^2) with H0 = 100. Power of the test is {scales::percent(test_power)}.&quot; ) ) } #With a mean difference of 6, and SD of 15, and a sample size of 26, the test has 50% power) # The power of the study the percent of the times it correctly identifies a # positive result. # study_power &lt;- sum(sim$p &lt; .05) / n_sims # This matches the formal calculation. # study_power_thry &lt;- pwr.t.test( # d = (M - 100) / SD, # n = n, # sig.level = 0.05, # type = &quot;one.sample&quot;, # alternative=&quot;two.sided&quot; # ) Since the statistical power is the probability of observing a statistically significant result, if there is a true effect, we can also see the power in the figure itself. We can calculate the difference between p-values above 0.5 minus the p-values below 0.05, and divide this number by the number of simulations. # sim &lt;- run_sim(M = 106, n = 26, SD = 15) # # sim %&gt;% plot_sim() # # sim %&gt;% filter(p&lt;.05) %&gt;% plot_sim(M = 106, n = 26, SD = 15, n_breaks = 100, x_lim = c(0, .05), waiver()) Change the sample size in line 10 from n&lt;-26 to n&lt;-51. Run the simulation by selecting all lines and pressing CTRL+Enter. What is the power in the simulation now that we have increased the sample size from 26 people to 51 people? sim_51 &lt;- run_sim(M = 106, n = 51, SD = 15) plot_sim(sim_51) The power of the test increased to 79%. The p-values are much steep er than with 50% power. What would happen if there were no real difference in IQ scores? The p-value distribution is essentialy flat. # sim_null &lt;- run_sim(M = 100, n = 26, SD = 15) # # plot_sim(sim_null) The leftmost bar on the plot (p&lt;.05) are the Type I errors (false positives). When there is no true effect, p-values are uniformly distributed under the null. Every p-value is equally likely when the null hypothesis is true, and every bar in the graph will contain 5% of all the p-values (as indicated by the dotted red line). When there is no true effect, p-values are uniformly distributed. When there is a true effect, the p-value distribution depends on the power, and the higher the power, the more p-values fall below 0.05, and the steeper the p-value distribution becomes. Lets take a look at just the p-values below 0.05. Its not the case that all p-values &gt; 0.05 are support for the null-hypothesis, and all p-values below 0.05 are support for the alternative hypothesis. Re-run the simulation (still with M&lt;-100), but focused on just the p values between 0 and .05. # sim_null %&gt;% plot_sim(.001, x_lim = c(-.05, .051)) We see the same uniform distribution, but now every bar contains 1% of the p-values, so the p-value distribution is very flat and almost impossible to see (we will zoom in on the y-axis later this assignment). The red line now clearly gives the frequency for each bar, assuming the null hypothesis is true. 1.5 Further Reading Pritha Bhandari has two nice posts on Type I and Type II errors and Statistical Power. Daniel Lakenss Coursera class Improving your statistical inferences has a great p-value simulation exercise in Week 1 (assignment) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

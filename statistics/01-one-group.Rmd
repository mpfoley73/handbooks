# One-Group Inference Tests

```{r include=FALSE}
library(tidyverse)
library(broom)
library(gtsummary)
library(flextable)
```


*One-group* refers to a single population whose metric you are trying to measure. In the case of a quantitative metric, you are typically measuring its mean value. In the case of a count variable, you are typically measuring its proportion.

## One-Group Inference with Numeric Variable

The tests in this section use the sample mean, $\bar{x}$, as an estimate of the population mean, $\mu$. The tests apply when the data is *independent*. Using the normal or student-t distribution, you can create a confidence interval around the estimate, or evaluate an hypothesized population mean, $\mu_0$. Which test you use depends on what you know about the population distribution (normal vs non-normal) and its variance (known or unknown):

||Normal Distribution|Non-Normal|
|-|-|-|
|Known Variance|*z*-test|Wilcoxon Median|
|Unknown Variance|*t*-test|Wilcoxon Median|

### One Sample z Test for Numeric Var

The *z* test is also called the normal approximation *z* test.  It only applies when the sampling distribution of the population mean is normally distributed with known variance, and there are no significant outliers. The sampling distribution is normally distributed when the underlying population is normally distributed, or when the sample size is large $(n >= 30)$, as follows from the central limit theorem. The *t* test returns similar results, plus it is valid when the variance is unknown, and that is pretty much always. For that reason, you probably will never use this test.

Under the normal approximation method, the measured mean $\bar{x}$ approximates the population mean $\mu$, and the sampling distribution has a normal distribution centered at $\mu$ with standard error $se_\mu = \frac{\sigma}{\sqrt{n}}$ where $\sigma$ is the standard deviation of the underlying population. Define a $(1 - \alpha)\%$ confidence interval as $\bar{x} \pm z_{(1 - \alpha) {/} 2} se_\mu$, or test $H_0: \mu = \mu_0$ with test statistic $Z = \frac{\bar{x} - \mu_0}{se_\mu}$.  

#### Example {-}

```{r include=FALSE}
n <- nrow(mtcars)
x_bar <- mean(mtcars$mpg)
s <- sd(mtcars$mpg)
mu_0 <- 18.0
sigma <- 6
```

The `mtcars` data set is a sample of *n* = `r n` cars. The mean fuel economy is $\bar{x} \pm s$ = `r x_bar %>% format(digits = 1, nsmall=1)` $\pm$ `r s %>% format(digits = 1, nsmall=1)` mpg. The prior measured overall fuel economy for vehicles was $\mu_0 \pm \sigma$ = `r mu_0 %>% format(digits = 1, nsmall=1)` $\pm$ `r sigma %>% format(digits = 1, nsmall=1)` mpg. Has fuel economy improved?

The sample size is $\ge$ 30, so the sampling distribution of the population mean is normally distributed. The population variance is known, so use the *z* test.  

```{r include=FALSE}
se <- sigma / sqrt(n)
Z <- (x_bar - mu_0) / se
p <- pnorm(q = Z, lower.tail = FALSE)
```

$H_0: \mu = 16.0$, and $H_a: \mu > 16.0$ - a right-tail test. The test statistic is $Z = \frac{\bar{x} - \mu_0}{se_\mu}=$ `r Z %>% format(digits = 1, nsmall = 2)` where $se_{\mu_0} = \frac{\mu_0}{\sqrt{n}} =$ `r se %>% format(digits = 1, nsmall = 2)`. $P(z > Z) =$ `r p %>% format(digits = 1, nsmall = 4)`, so reject $H_0$ at the $\alpha =$ 0.05 level of significance.

```{r echo=FALSE, warning=FALSE, fig.height=3.5, fig.width=6.5}
lrr = -Inf  # right-tailed test
urr = mu_0 + qnorm(p = .05, lower.tail = FALSE) * se
data.frame(mu = seq(10, 25, by = .1)) %>%
  mutate(Z = (mu - mu_0) / se,
         prob = dnorm(Z),
         rr = ifelse(mu < lrr | mu > urr, prob, NA_real_)) %>%
  ggplot() +
  geom_line(aes(x = mu, y = prob)) +
  geom_area(aes(x = mu, y = rr), fill = "firebrick", alpha = 0.6) +
  geom_vline(aes(xintercept = mu_0), size = 1) +
  geom_vline(aes(xintercept = x_bar), color = "goldenrod", size = 1, linetype = 2) +
  theme_minimal() +
  theme(legend.position="none",
        axis.text.y = element_blank()) +
  labs(title = bquote("Right-Tail z Test"),
       x = "mu",
       y = "Probability")
```

```{r include=FALSE}
z_crit <- qnorm(p = 1-.05/2, lower.tail = TRUE)
epsilon <- z_crit * se
ci_low <- x_bar - epsilon
ci_high <- x_bar + epsilon
```

The 95% confidence interval for $\mu$ is $\bar{x} \pm z_{(1 - \alpha){/}2} se_\mu$ where $z_{(1 - \alpha){/}2} =$ `r z_crit %>% round(2)`. $\mu =$ `r x_bar %>% format(digits = 1, nsmall = 2)` $\pm$ `r epsilon %>% format(digits = 1, nsmall = 2)` (95% CI `r ci_low %>% format(digits = 1, nsmall = 2)` to `r ci_high %>% format(digits = 1, nsmall = 2)`).

```{r echo=FALSE, warning=FALSE, fig.height=3.5, fig.width=6.5}
data.frame(mu = seq(15, 25, by = .1)) %>%
  mutate(Z = (mu - x_bar) / se,
         prob = dnorm(x = Z), 
         ci = if_else(mu >= ci_low & mu <= ci_high, prob, NA_real_)) %>%
  ggplot() +
  geom_line(aes(x = mu, y = prob), color = "goldenrod") +
  geom_area(aes(x = mu, y = ci), fill = "goldenrod", alpha = 0.3) +
  geom_vline(aes(xintercept = x_bar), color = "goldenrod", size = 1, linetype = 2) +
  theme_minimal() +
  theme(legend.position="none",
        axis.text.y = element_blank()) +
  labs(title = "95% CI",x = "mu",y = "Probability") 
```

### One sample t Test for Numeric Var

The one-sample *t* test applies when the sampling distribution of the population mean is normally distributed and there are no significant outliers. Unlike the *z* test, the population variance can be unknown. The sampling distribution is normally distributed when the underlying population is normally distributed, or when the sample size is large $(n >= 30)$, as follows from the central limit theorem. 

Under the *t* test method, the measured mean, $\bar{x}$, approximates the population mean, $\mu$. The sample standard deviation, $s$, estimates the unknown population standard deviation, $\sigma$. The resulting sampling distribution has a *t* distribution centered at $\mu$ with standard error $se_\bar{x} = \frac{s}{\sqrt{n}}$. Define a $(1 - \alpha)\%$ confidence interval as $\bar{x} \pm t_{(1 - \alpha){/}2} se_\bar{x}$ and/or test $H_0: \mu = \mu_0$ with test statistic $T = \frac{\bar{x} - \mu_0}{se_\bar{x}}$.

```{r include=FALSE}
dep <- foreign::read.spss("./input/one-sample-t-test.sav", to.data.frame = TRUE)

n <- nrow(dep)
mu_0 <- 4.0
x_bar <- mean(dep$dep_score)
s <- sd(dep$dep_score)
```

Here is a case study. A researcher recruits a random sample of *n* = `r n` people to participate in a study about depression intervention. The researcher measures the participants' depression level prior to the study. The mean depression score (`r format(x_bar, digits = 2, nsmall = 2)` $\pm$ `r format(s, digits = 2, nsmall = 2)`) was lower than the population 'normal' depression score of `r format(mu_0, digits = 2, nsmall = 1)`. The null hypothesis is that the sample is representative of the overall population. Should you reject $H_0$?

```{r}
dep %>% gtsummary::tbl_summary(statistic = list(all_continuous() ~ "{mean} ({sd})"))
```

#### Conditions {-}

The one-sample *t* test applies when the variable is continuous and the observations are independent. Additionally, there are two conditions related to the data distribution. If either condition fails, try the suggested work-arounds or use the non-parametric [Wilcoxon 1-Sample Median Test for Numeric Var] instead.

1. **Outliers**. There should be no significant outliers. Outliers exert a large influence on the mean and standard deviation. Test with a box plot. If there are outliers, you might be able to drop them or transform the data.
2. **Normality**.  Values should be *nearly* normally distributed ("nearly" because the *t*-test is robust to the normality assumption). This condition is especially important with small sample sizes. Test with Q-Q plots or the Shapiro-Wilk test for normality. If the data is very non-normal, you might be able to transform the data.

##### Outliers {-}

Assess outliers with a box plot. Box plot whiskers extend up to 1.5\*IQR from the upper and lower hinges and outliers (beyond the whiskers) are are plotted individually. The boxplot shows no outliers.

```{r echo=FALSE, fig.height=2.5}
dep %>%
  ggplot(aes(x = "dep_score", y = dep_score)) +
  geom_boxplot(fill = "snow3", color = "snow4", alpha = 0.6, width = 0.5, 
               outlier.color = "goldenrod", outlier.size = 2) +
  theme_minimal() +
  labs(title = "Boxplot of Depression Score",
       y = "Score", x = NULL)
```

If the outliers might are data entry errors or measurement errors, fix them or discard them. If the outliers are genuine, you have a couple options before reverting to Wilcoxon.

* Transform the variable. Don't do this unless the variable is also non-normal. Transformation also has the downside of making interpretation more difficult.
* Leave it in if it doesn't affect the conclusion (compared to taking it out).

##### Normality {-}

Assume the population is normally distributed if *n* $\ge$ 30. Otherwise, asses a Q-Q plot, skewness and kurtosis values, or a histogram. If you still don't feel confident about normality, run a [Shapiro-Wilk Test].

The data set has *n* = 40 observations, so you can assume normality. Here is a QQ plot anyway. The QQ plot indicates normality.

```{r fig.height=2.5}
dep %>%
  ggplot(aes(sample = dep_score)) +
  stat_qq() +
  stat_qq_line(col = "goldenrod") +
  theme_minimal() +
  labs(title = "Normal Q-Q Plot")
```

Here is the Shapiro-Wilk normality test. It fails to reject the null hypothesis of a normally distributed population.

```{r}
shapiro.test(dep$dep_score)
```

If the data is not normally distributed, you still have a couple options before reverting to Wilcoxon.

* Transform the dependent variable.
* Carry on regardless - the one-sample *t*-test is fairly robust to deviations from normality.

#### Results {-}

Conduct the *t*-test. To get a 95% CI around the *difference* (instead of around the estimate), run the test using the difference, $\mu_0 - \bar{x}$, and leave `mu` at its default of 0.

```{r}
(dep_95ci <- t.test(x = mu_0 - dep$dep_score, alternative = "two.sided", conf.level = .95))
```

The difference is statistically different from 0 at the *p* = .05 level. The effect size, called Cohen's *d*, is defined as $d = |M_D| / s$, where $|M_D| = \bar{x} - \mu_0$, and $s$ is the sample standard deviation. $d <.2$ is considered trivial, $.2 \le d < .5$ small, and $.5 \le d < .8$ large.

```{r}
(d <- rstatix::cohens_d(dep, dep_score ~ 1, mu = 4) %>% pull(effsize) %>% abs())
```

Cohen's *d* is `r d %>% format(digits = 2, nsmall = 2)`, a small effect. 

Make a habit of constructing a plot, just to make sure your head is on straight.

```{r echo=FALSE, warning=FALSE, fig.height=3.5, fig.width=6.5}
se <- s / sqrt(n)
lrr = mu_0 + qt(p = .05, df = n - 1, lower.tail = TRUE) * se
urr = mu_0 + qt(p = .05, df = n - 1, lower.tail = FALSE) * se
data.frame(mu = seq(3.5, 4.5, by = .01)) %>%
  mutate(t = (mu - mu_0) / se,
         prob = dt(x = t, df = n - 1),
         lrr = if_else(mu < lrr, prob, NA_real_),
         urr = if_else(mu > urr, prob, NA_real_)) %>%
  ggplot() +
  geom_line(aes(x = mu, y = prob)) +
  geom_area(aes(x = mu, y = lrr), fill = "firebrick", alpha = 0.6) +
  geom_area(aes(x = mu, y = urr), fill = "firebrick", alpha = 0.6) +
  geom_vline(aes(xintercept = mu_0), size = 1) +
  geom_vline(aes(xintercept = x_bar), color = "goldenrod", size = 1, linetype = 2) +
  theme_minimal() +
  theme(legend.position="none",
        axis.text.y = element_blank()) +
  labs(title = bquote("Two-Tailed t Test"),
       x = "mu",
       y = "Probability")
```

Now you are ready to report the results.

> A one-sample *t*-test was run to determine whether depression score in recruited subjects was different from normal, as defined as a depression score of 4.0. Depression scores were normally distributed, as assessed by Shapiro-Wilk's test (*p* > .05) and there were no outliers in the data, as assessed by inspection of a boxplot. Data are mean $\pm$ standard deviation, unless otherwise stated. Mean depression score (`r format(x_bar, digits = 2, nsmall = 2)` $\pm$ `r format(s, digits = 2, nsmall = 2)`) was lower than the population "normal" depression score of `r format(mu_0, digits = 2, nsmall = 2)`, a statistically significant difference of `r format(dep_95ci$estimate, digits = 2, nsmall = 2)` (95% CI, `r format(dep_95ci$conf.int[1], digits = 1, nsmall = 2)` to `r format(dep_95ci$conf.int[2], digits = 1, nsmall = 2)`), t(`r dep_95ci$parameter`) = `r format(dep_95ci$statistic, digits = 1, nsmall = 2)`, *p* = `r format(dep_95ci$p.value, digits = 1, nsmall = 3)`, *d* = `r d %>% format(digits = 2, nsmall = 2)`.

#### Appendix: Deciding Sample Size {-}

Determine the sample size required for a maximum error $\epsilon$ in the estimate by solving the confidence interval equation, $\bar{x} \pm t_{(1 - \alpha){/}2} \frac{s}{\sqrt{n}}$ for $n=\frac{{t_{\alpha/2,n-1}^2se^2}}{{\epsilon^2}}$ . Unfortunately, $t_{\alpha/2,n-1}^2$ is dependent on $n$, so replace it with $z_{\alpha/2}^2$. What about $s^2$?  Estimate it from the literature, a pilot study, or using the empirical rule that 95% of the range falls within two standard deviations, $s=range / 4$.

For example, if the maximum tolerable error is* $\epsilon$ = 3, and $s$ is approximately 10, what sample size produces an $\alpha$ =0.05 confidence level?

```{r}
ceiling(qnorm(.975)^2 * 10^2 / 3^2)
```

### Wilcoxon 1-Sample Median Test for Numeric Var

This test applies when the variable is not normally distributed.

## One-Group Inference with Categorical Variable

### 1 Sample z Test for Categorical Var

This test applies when you know the population variance.

### 1 sample t Test for Categorical Var

This test applies when you do not know the population variance.

### Wilcoxon 1-Sample Median Test for Categorical Var

This test applies when the variable is not normally distributed.

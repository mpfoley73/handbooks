# Statistical Inference {-}

```{r include=FALSE}
library(tidyverse)
library(glue)
library(pwr)

set.seed(123456)
```

Statistical inference is the use of a sample's distribution to describe the population distribution. Hypothesis tests, confidence intervals, and effect size estimates are all examples of statistical inference. 

We wary of published study results. Identical studies might produce significant and non-significant results, yet only the significant result is likely to reach publication (publication bias). The researcher may have tortured the data until they found a statistically significant result. The study might suffer from low statistical power. Applying principles of inference can mitigate these problems.

There are at least three approaches to establishing statistical inference: frequentist, likelihood, and Bayesian. Think of them philosophically. The frequentist approach is the path of *action*. It rejects a null hypothesis if the *p*-value is low because repeated sample analyses are likely to agree. The likelihood apprach is the path of *knowledge*. It compares the observed summary measure to the likelihoods of the other possible realities. The Bayesian approach is the path of *belief*. It uses a summary measure to update the prior belief.

# Frequentist Statistics

*P*-values express how surprising the summary measure is given the null hypothesis (H0).

Suppose you hypothesize that IQs have increased from the established mean of $\mu_0$ = 100. H0 is $\mu$ = 100 and the alternative hypothesis, H1, is $\mu$ > 100. Also suppose you are *right* and the population mean IQ is actually 106. Finally, assume the "population" is some very big number - 1,000,000 for convenience. 

```{r}
mu_0 <- 100
mu <- 106
sigma <- 15
N <- 1000000

pop_100 <- data.frame(person = seq(1, N), iq = rnorm(N, mu_0, sigma))
pop_106 <- data.frame(person = seq(1, N), iq = rnorm(N, mu, sigma))
```

Here is what the distribution of IQs might look like.

```{r echo=FALSE}
pop_100 %>%
  ggplot(aes(x = iq)) +
  geom_density() +
  theme_light() +
  labs(title = "Presumed population IQs are centered at 100 with SD = 15.")
```

You take a random sample of *n* = 30 IQs from the population. 

```{r collapse=TRUE}
n <- 30

x <- sample(pop_106$iq, n)

(x_bar <- mean(x))
```

You measure $\bar{x}$ = `r scales::number(x_bar, accuracy = 0.1)`, SD = `r scales::number(sd(x), accuracy = 0.1)`. How surprising is this result given H0 that $\mu$ is 100? I.e., what is the *probability* of observing $\bar{x}$ this extreme? According to the Central Limit Theorem (CLM) repeated samples of size *n* from a large population will yield $\bar{x}$ values that approach a normal distribution centered at $\mu$ with a standard deviation equal to the population SD ($\sigma$) divided by $\sqrt{n}$. The standard deviation of the sampling distribution of the mean is commonly referred to as the *standard error* (SE). For a sample size of *n* = 30, and a $\sigma$ of 15, you'd expect repeated samples to converge on a mean of 100 with SE = 2.7. You can verify this empirically. 

```{r}
sim <- replicate(1000, mean(sample(pop_100$iq, 30)))
```

Here is the distribution of $\bar{x}$ measurements from 1,000 random samples from the hypothesized population.

```{r echo=FALSE}
x_quantiles <- quantile(sim, probs = c(.025, .975))

data.frame(iq = sim) %>%
  ggplot(aes(x = iq)) +
  geom_density() +
  geom_vline(xintercept = x_quantiles["2.5%"], linetype = 2, color = "firebrick") +
  geom_vline(xintercept = x_quantiles["97.5%"], linetype = 2, color = "firebrick") +
  theme_light() +
  labs(
    title = glue(
      "Presumed population IQs are centered at {scales::number(x_bar, accuracy = 0.1)} ",
      "with SD = {scales::number(sd(sim), accuracy = 0.1)}"),
    caption = "95% of values captured within dashed lines."
    )
```

So what is the probability of measuring $\bar{x}$ = `r scales::number(x_bar, accuracy = 1)`? It is the probability of measuring a value $\frac{\bar{x} - \mu_0}{\sigma / \sqrt{n}}$ = `r scales::number((x_bar - mu_0) / (sigma / sqrt(n)), accuracy = .1)` standard deviations from 100. You can look up the probability from the normal distribution. 

```{r collapse=TRUE}
q <- (x_bar - mu_0) / (sigma / sqrt(n))

pnorm(q, lower.tail = FALSE)
```

You can get a sense of what this probability means by overlaying the sampling distribution of the mean from the presumed (H0) distribution with the sampling distribution of the mean from the actual population. 

```{r echo=FALSE, warning=FALSE}
iq_range <- seq(90, 120, .01)

tmp <- data.frame(
  iq = iq_range,
  H0 = dnorm(iq_range, mean = mu_0,  sd = sigma / sqrt(n)),
  H1 = dnorm(iq_range, mean = x_bar, sd = sigma / sqrt(n))
)

alpha_x <- qnorm(.95, mu_0, sigma / sqrt(n))
alpha_y <- dnorm(alpha_x, mean = mu_0, sd = sigma / sqrt(n))

plot_alpha_beta <- function(x, x_bar) {
  x %>% 
    pivot_longer(cols = c(H0, H1), values_to = "density") %>%
    mutate(alpha_beta = if_else((name == "H0" & iq >= alpha_x) | 
                                  (name == "H1" & iq <= alpha_x), 
                                density, NA_real_)) %>%
    ggplot(aes(x = iq, color = name, fill = name)) +
    geom_line(aes(y = density), size = 1) +
    geom_area(aes(y = alpha_beta), alpha = .4, show.legend = FALSE) +
    geom_vline(xintercept = x_bar, linetype = 2, size = .5, color = "steelblue") +
    scale_color_manual(values = c("H0" = "lightgoldenrod", "H1" = "lightsteelblue")) +
    scale_fill_manual(values = c("H0" = "lightgoldenrod", "H1" = "lightsteelblue")) +
    annotate("text", x = mu_0, y = .075, label = "1 - alpha", parse = TRUE) +
    annotate("text", x = x_bar, y = .075, label = "1 - beta", parse = TRUE) +
    annotate("text", x = x_bar - 4, y = .0075, label = "beta", parse = TRUE) +
    annotate("text", x = x_bar - .5, y = .0075, label = "alpha", parse = TRUE) +
    theme_light() +
    theme(panel.grid = element_blank(), legend.position = "top") +
    labs(
      title = glue("Measured x-bar of {scales::number(x_bar, accuracy = .1)} is ",
                   "outside than the .05 confidence levl of the H0 \n",
                   "value of {mu_0}."),
      fill = NULL, color = NULL, x = "IQ"
    )
}

tmp %>% plot_alpha_beta(x_bar = x_bar)
```

The sampling distribution of the mean is centered at $\bar{x}$ = `r scales::number(x_bar, accuracy = 1)`, and that is well into the $\alpha$ = .05 level (the yellow-shaded region). The *p*-value is `r scales::number(pnorm(q, lower.tail = FALSE), .001)` -- the probability of measuring a mean IQ of `r scales::number(x_bar, accuracy = 1)` from a sample of size *n* = `r n` when the true population mean is `r mu_0`. Using an $\alpha$ level of significance, you will reject H0, which is a *true positive* given the fact that in this example the true population mean is `r mu`. But you can imagine a stricter threshold level of significance, shrinking the yellow region to the right of the dashed blue line. Then we mistakenly fail to reject H0, a *false negative* (Type II error). 

## Type I and II Errors

Either H0 or H1 is correct, and you must choose to either reject or not reject H0. That means there are four possible states at the end of your study. If your summary measure is extreme enough for you to declare a "positive" result and reject H0, you are either correct (true positive) or incorrect (false positive). False positives are called *Type I errors*. Alternatively, if your summary measure is *not* extreme enough for you to reject H0, you are either correct (true negative) or incorrect (false negative). False negatives are called *Type II errors*.

The probabilities of landing in these four states depend on your chosen significance level, $\alpha$, and on the statistical power of the study, 1 - $\beta$.

|                        |H0 True                       |H1 True                      |
|------------------------|------------------------------|-----------------------------|
|Significance test is positive, so you reject H0.| False Positive <br>Type I Error<br> Probability = $\alpha$    | True Positive <br>Good Call! <br> Probability = 1 - $\beta$ |
|Significance test is negative, so you do *not* reject H0.   | True Negative <br> Good Call!<br> Probability = ($1 - \alpha$)    | False Negative <br>Type II Error <br>Probability = $\beta$ |

$\alpha$ is the expected rate of Type I errors due to summary measures that are extreme by chance alone. $\beta$ is the expected rate of Type II errors due to summary measures that are not extreme by chance alone. In the IQ example, if the reality is $\mu$ = `r mu_0`, any sample mean over `r scales::number(qnorm(.95, mu_0, sigma / sqrt(n)), accuracy = .1)` would be a false positive under the $\alpha$ = .05 level of significance. If the reality is $\mu$ = `r mu`, any sample mean under `r scales::number(qnorm(.95, mu_0, sigma / sqrt(n)), accuracy = .1)` would be a false negative under the $1 - \beta$ = `r scales::number(pwr.t.test(n = n, d = (x_bar - mu_0) / sigma, sig.level = 0.05, type = "one.sample", alternative="greater")$power, accuracy = .01)` statistical power of the study.

## Statistical Power

Type II error rates ($\beta$) vary inversely with the power of the study (1 - $\beta$). Statistical power is an increasing function of a) sample size, b) effect size, and c) significance level. The positive association with significance level means there is a trade-off between Type I and Type II error rates. A small $\alpha$ sets a high bar for rejecting H0, but you run the risk of failing to appreciate a *real* difference. On the other hand, a small $\alpha$ sets a low bar for rejecting H0, and you run the risk of judging a random difference as a real difference.

The 1 - $\beta$ statistical power threshold is usually set at .80, similar to the $\alpha$ = .05 level of significance threshold convention. Given a real effect, a study with a statistical power of .80 will only find a positive test result 80% of the time. There may be such a thing as *too* much power, however. With a large enough sample size, even trivial effect sizes may yield a positive test result. You need to consider both sides of this coin.

A *power analysis* is frequently used to determine the sample size required to detect a threshold effect size given an $\alpha$ level of significance criteria. A power analysis expresses the relationship among four components. If you know any three, it can return the fourth. The components are a) power (1 - $\beta$), b) sample size (n), c) significance ($\alpha$), and d) expected effect size (Cohen's d).

Suppose you set power at .80 and significance at .05. You can use the power test to see the relationship between sample size and effect size. Let's do that with the IQ example. I multiplied Cohen's d $(\bar{x} - \mu_0)/\sigma$ by $\sigma$ to get a non-normalized effect size.

```{r echo=FALSE}
data.frame(n = rep(10:150, 2),
           power = c(rep(.80, 141), rep(.60, 141))) %>%
  mutate(d = map2_dbl(n, power, ~ pwr.t.test(n = .x, 
                                     sig.level = .05, 
                                     power = .y, 
                                     type = "one.sample", 
                                     alternative = "greater") %>% pluck("d")),
         delta = d * sigma
  ) %>%
  ggplot(aes(x = n, y = delta, group = as.factor(power), color = as.factor(power))) +
  geom_line() +
  annotate("segment", x = 30, xend = 30, y = 0, yend = 7, linetype = 2, size = 1, color = "steelblue") +
  annotate("segment", x = 0, xend = 30, y = 7, yend = 7, linetype = 2, size = 1, color = "steelblue") +
  scale_y_continuous(limits = c(0, NA), breaks = 1:100, expand = c(0, 0)) +
  scale_x_continuous(limits = c(0, 155), breaks = seq(0, 150, 10), expand = c(0, 0)) +
  scale_color_manual(values = c("0.6" = "gray80", "0.8" = "steelblue")) +
  theme_light() +
  theme(panel.grid.minor = element_blank(), legend.position = "top") +
  labs(title = paste("A sample size of 30 is required to detect an effect size",
                     "of 7 at a .05 significance \nlevel with 80% probability."),
       subtitle = "Power analysis with power = .80 vs .60 and significance = .05.",
       color = "Power")
```

The plot shows a sample size of 30 is required to detect an effect size of 7 at a .05 significance level with 80% probability. If an effect size of 5 is important, then if you want to detect it at a .05 significance level with 80% probability, you will need a sample size of at least `r ceiling(pwr.t.test(power = 0.80, d = 5 / sigma, sig.level = 0.05, type = "one.sample", alternative="greater")$n)` (always round up).

```{r}
pwr::pwr.t.test(
  d = 5 / sigma,
  sig.level = .05,
  power = 0.80,
  type = "one.sample",
  alternative = "greater"
)
```

## What p-values would you expect?

This section is based on ideas I learned from homework assignment 1 in Daniel Lakens's Coursera class [Improving your statistical inferences](https://www.coursera.org/learn/statistical-inferences/home/welcome).

What distribution of p-values would you expect if there *is* a true effect and you repeated the study many times? What if there is *no* true effect? The answer is completely determined by the statistical power of the study.

To see this, run 100,000 simulations of an experiment measuring the average IQ from a sample of size *n* = 26. The samples will be 26 random values from the normal distribution centered at 106 with a standard deviation of 15. H0 is $\mu$ = 100.

```{r}
# 100,000 random samples of IQ simulations from a normal distribution where
# sigma = 15. True population value is 100, but we'll try other values.
n_sims <- 1E5
mu <- 100
sigma <- 15

run_sim <- function(mu_0 = 106, n = 26) {
  data.frame(i = 1:n_sims) %>%
    mutate(
      x = map(i, ~ rnorm(n = n, mean = mu_0, sd = sigma)),
      z = map(x, ~ t.test(., mu = mu)),
      p = map_dbl(z, ~ .x$p.value),
      x_bar = map_dbl(x, mean)
    ) %>%
    select(x_bar, p)
}
```

```{r echo=FALSE}
plot_sim <- function(x, mu_0 = 106, n = 26, 
                     n_breaks = 20, x_lim = c(0, 1), 
                     p_max = 1.0) {
  test_power = sum(x$p < .05) / n_sims
  x %>% 
    filter(p <= p_max) %>%
    ggplot(aes(x = p)) +
    geom_bar(fill = "lightgoldenrod") + 
    scale_x_binned(n.breaks = n_breaks, limits = x_lim, labels = waiver()) +
    geom_hline(yintercept = n_sims / n_breaks * p_max, color = "firebrick", linetype = 2) +
    annotate("text", x = .8, y = n_sims / n_breaks * p_max, label = "5%") +
    theme_light() +
    scale_y_continuous(labels = scales::comma_format(), limits = c(0, n_sims)) +
    labs(
      title = glue(
        "Measured p-values in {n_sims} simulations of samples of size {n} distributed \n",
        "N({mu_0}, {sigma}^2) with H0 = 100. Power of the test is {scales::percent(test_power)}."
      )
    )
}
```

The null hypothesis is that the average IQ is 100. Our rigged simulation finds an average IQ of 106 - an effect size of 6. 

```{r collapse=TRUE}
sim_106_26 <- run_sim(mu_0 = 106, n = 26)

glimpse(sim_106_26)

mean(sim_106_26$x_bar)
```

The statistical power achieved by the simulations is 50%. That is, the typical simulation detected the effect size of 6 at the .05 significance level about 50% of the time. 

```{r}
pwr.t.test(
  n = 26,
  d = (106 - 100) / 15,
  sig.level = .05,
  type = "one.sample",
  alternative = "two.sided"
)
```

That means that given a population with an average IQ of 106, a two-sided hypothesis test of H0: $\mu$ = 100 from a sample of size 26 will measure an $\bar{x}$ with a *p*-value under .05 only 50% of the time. You can see that in this histogram of *p*-values.

```{r}
sim_106_26 %>% plot_sim()
```

Had there been no effect to observe, you'd expect all *p*-values to be equally likely, so the 20 bins would all have been 5% of the number of simulations -- i.e., *uniformly distributed under the null*. This is called "0 power", although 5% of the *p*-values will still be significant at the .05 level. The 5% of *p*-values < .05 is the Type II error rate - that probability of a positive test result when there is no actual effect to observe.

```{r}
run_sim(mu_0 = 100, n = 26) %>%
  plot_sim(mu_0 = 100)
```

If you want a higher powered study that would detect the effect at least 80% of the time (the normal standard), you'll need a higher sample size. How high? Conduct the power analysis again, but specify the power while leaving out the sample size.

```{r}
pwr.t.test(
  power = 0.80,
  d = (106 - 100) / 15,
  sig.level = .05,
  type = "one.sample",
  alternative = "two.sided"
)
```

You need 51 people (technically, you might want to round up to 52). Here's what that looks like. 80% of *p*-values are below .05 now.

```{r}
run_sim(mu_0 = 106, n = 51) %>%
  plot_sim(mu_0 = 106)
```

So far, we've discovered that when there *is* an effect, the probability that the measure *p*-value is under the $\alpha$ significance level equals the *power of the study*, 1 - $\beta$ - the *true positive* rate, and $\beta$ will be above the $\alpha$ level - the *false negative* rate. We've also discovered that when there is *no* effect, all *p*-values are equally likely, so $\alpha$ of them will be below the $alpha$ level of significance - the *false positive* rate, and 1 - $\alpha$ will be above $\alpha$ - the *true negative* rate.

It's not the case that all p-values below 0.05 are support for the alternative hypothesis. If the statistical power is high enough, a *p*-value just under .05 can be even *less* likely under the null hypothesis.

```{r}
run_sim(mu_0 = 108, n = 51)  %>%
    mutate(bin = case_when(p < .01 ~ "0.00 - 0.01",
                         p < .02 ~ "0.01 - 0.02",
                         p < .03 ~ "0.02 - 0.03",
                         p < .04 ~ "0.03 - 0.04",
                         p < .05 ~ "0.04 - 0.05",
                         TRUE ~ "other")
  ) %>%
  janitor::tabyl(bin)
```

(Recall that under H0, all *p*-values are equally likely, so each of the percentile bins would contain 1% of *p*-values.)

In fact, at best, a *p*-value between .04 and .05 can only be about four times as likely under the alternative hypothesis as the null hypothesis. If your *p*-value is just under .05, it is at best weak support for the alternative hypothesis.

## Further Reading

Pritha Bhandari has two nice posts on [Type I and Type II errors](https://www.scribbr.com/statistics/type-i-and-type-ii-errors/) and [Statistical Power](https://www.scribbr.com/statistics/statistical-power/). Daniel Lakens's Coursera class [Improving your statistical inferences](https://www.coursera.org/learn/statistical-inferences/home/welcome) has a great *p*-value simulation exercise in Week 1 (assignment)
# Association

```{r include=FALSE}
library(tidyverse)
library(gtsummary)
library(foreign)
library(scales)
library(janitor)
library(flextable)
```

Tests of association assess the strength of association between two variables. There are a bunch of variations on this theme.

**Pearson's correlation** assesses the strength of a linear relationship between two continuous variables. Its assumptions include bi-variate normality and no outliers. If either fail, **Spearman's Ranked correlation** and **Kendall's Tau-B are non-parameteric alternatives. Spearman measures the strength and direction of association, not necessarily linear.

If one of the variables is bivariate categorical, use **point-biserial correlation**, a special case of Pearson's correlation. 

Pearson's partial correlation controls for one or more variables - linear regression?

If both variables are ordinal, use Goodman and Kruskal's gamma. Somers' d is an alternative if you want to distinguish between a dependent and independent variable (instead of linear regression?). The Mantel-Haenszel test of trend is used to determine whether there is a linear trend (i.e., a linear relationship/association) between two ordinal variables that are represented in a contingency table. 

The Cochran-Armitage test of trend is used to determine whether there is a linear trend (i.e., a linear relationship/association) between an ordinal independent variable and a dichotomous dependent variable.

The chi-square test for association tests for whether two categorical variables are associated. chi-square test for independence focuses on contingency tables that are greater than 2 x 2, which are often referred to as r x c contingency tables, and tests whether two variables measured at the nominal level are independent (i.e., whether there is an association between the two variables).

**Relative risks** can be calculated from more than one statistical test, but in this guide we will focus on the calculation of relative risk in a 2 x 2 table. **Odds Ratio** can be calculated from more than one statistical test (e.g., a binomial logistic regression, ordinal logistic regression, multinomial logistic regression, etc), but in this guide we will focus on the calculation of an odds ratio from a 2 x 2 contingency table (i.e., a measure of association between two dichotomous variables). 

Goodman and Kruskal's λ (the Greek symbol, λ, is pronounced "lambda") is also referred to as Goodman and Kruskal's lambda. It is a nonparametric measure of the strength of association between two nominal variables where a distinction is made between a dependent and independent variable

The Fisher's exact test can be used to test more than one type of null hypothesis. In this guide we will use Fisher's exact test to determine whether two dichotomous variables are independent (i.e., test the null hypothesis of independence). 

Loglinear analysis is used to understand (and model) associations between two or more categorical variables (i.e., nominal or ordinal variables). However, loglinear analysis is usually employed when dealing with three or more categorical variables, as opposed to two variables, where a chi-square test for association is usually conducted instead. 



Use *association* tests to assess a possible two-way linear association between two continuous (interval or ratio) random variables. Association tests return an estimate between +1 (perfect linear relationship) to -1 (perfect linear inverse relationship).

Use Pearson's product moment if both random variables are normally distributed. If either variable is skewed, ordinal, or has extreme values, use Spearman's rank correlation.

There are three common correlation tests for categorical variables^[See https://www.statology.org/correlation-between-categorical-variables/]: Tetrachoric correlation for binary categorical variables; polychoric correlation for ordinal categorical variables; and Cramer’s V for nominal categorical variables.

## Pearson's Correlation

The Pearson product-moment correlation measures the strength and direction of a linear relationship between two continuous variables, *x* and *y*. The Pearson correlation coefficient, *r*, ranges from -1 (perfect negative linear relationship) to +1 (perfect positive linear relationship). A value of 0 indicates no relationship between two variables.

$$r_{x,y} = \frac{cov(x,y)}{\sigma(x) \sigma(y)}$$

The statistic can be used as an estimate of the population correlation, $\rho$, in a test of statistical significance from 0 (H0: $\rho$ = 0).

$$\rho_{X,Y} = \frac{cov(X,Y)}{\sigma(X) \sigma(Y)}$$

Pearson's correlation applies to two continuous (interval or ratio), paired, linearly related variables. If *x* and *y* have a bivariate normal distribution and there are no outliers, the sampling distribution of a function of $r_{x,y}$, $t$, follows a *t*-distribution with *n* − 2 degrees of freedom. 

$$t = r_{x,y} \sqrt{\frac{n - 2}{1 - r^2_{x,y}}}$$

## Spearman's Ranked Correlation

Spearman's correlation is measure of the strength and direction of a monotonic relationship between two continuous variables, two ordinal variables, or a continuous and an ordinal variable. Unlike Pearson, the relationship need not be linear (it only needs to be monotonic), and the variables need not be continuous (ordinal variables apply too). Spearman's correlation is a non-parametric alternative to Pearson, used when one or more variables are ordinal, or when one of the Pearson's correlation assumptions (bivariate normality and no outliers) is violated.

Spearman's correlation is Pearson's correlation applied to the *ranks* of variables (for ordinal variables, their value already is a rank).

$$\rho = 1 - \frac{6 \sum_i d^2_i}{n(n^2 - 1)}$$

where $d_i$ is the difference in ranks of observation $i$.

## Kendal's Tau

Kendal's Tau, just as with Spearman's correlation, is measure of the strength and direction of a monotonic relationship between two continuous variables, two ordinal variables, or a continuous and an ordinal variable. The relationship need not be linear (it only needs to be monotonic), and the variables need not be continuous (ordinal variables apply too). Kendal's Tau is a non-parametric alternative to Pearson, used when one or more variables are ordinal, or when one of the Pearson's correlation assumptions (bivariate normality and no outliers) is violated.

Kendal's Tau differs from Spearman's correlation in *how* it measures the relationship. Whereas Spearman measures the correlation of the ranks, Kendal's Tau is a function of concordant (C), discordant (D) and tied (Tx and Ty) pairs of observations. Concordant means both X and Y in one observation of a pair are larger than in the other. Discordant means X is larger in one observation than the other while Y is smaller. Tied mean either both observations have the same X or both have the same Y.

$$\mathrm{Kendall's} \space \tau_b = \frac{C - D}{\sqrt{(C + D + T_x \times (C + D + T_Y)}}$$


## Case Study: Numeric Vars

```{r include=FALSE}
# assoc_num <- list()
# 
# assoc_num$pearson_dat <- datasets::trees
# assoc_num$t_n <- with(ind_num$t_dat, by(engagement, gender, length))
# ind_num$t_mean <- with(ind_num$t_dat, by(engagement, gender, mean))
# ind_num$t_sd <- with(ind_num$t_dat, by(engagement, gender, sd))
# 
# assoc_num$spearman_dat <- datasets::trees %>%
#   mutate(
#     Girth = Girth * if_else(row_number() %in% c(10, 20, 30), 1.5, 1),
#     Height = Height * if_else(row_number() %in% c(10, 20, 30), 1.5, 1)
#   )
# ind_num$mw_n <- with(ind_num$mw_dat, by(engagement, gender, length))
# ind_num$mw_median <- with(ind_num$mw_dat, by(engagement, gender, median))
# ind_num$mw_sd <- with(ind_num$mw_dat, by(engagement, gender, sd))
```

The `datasets::trees` data set contains the `Girth` and `Height` of *n* = 31 trees. Is girth linearly associated with height?

I mutated one version of `trees` so that it contains some outliers, forcing us to use Spearman instead of Pearson.

```{r fig.height=3.5, fig.width=7.5, echo=FALSE}
# bind_rows(
#   `Pearson` = assoc_num$pearson_dat, 
#   `Spearman` = assoc_num$spearman_dat, 
#   .id = "set"
# ) %>%
#   ggplot(aes(x = Girth, y = Height)) +
#   geom_point() +
#   facet_wrap(~fct_rev(set)) +
#   labs(title = "Scatterplot of Height and Girth") +
#   theme_light()
```

> There were ` ind_num$t_n["Male"]` male and ` ind_num$t_n["Female"]` female participants. Data are mean $\pm$ standard deviation, unless otherwise stated. The advertisement was more engaging to male viewers, ` gtsummary::inline_text(ind_num$t_gt, engagement, column = "Male")`, than female viewers, ` gtsummary::inline_text(ind_num$t_gt, engagement, column = "Female")`.



## Pearson's Correlation for Categorical Vars

## Spearman's Ranked Correlation for Categorical Vars

